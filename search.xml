<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>视觉注意力机制(上)</title>
      <link href="/2020/12/16/attentions-first/"/>
      <url>/2020/12/16/attentions-first/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，起源于自然语言处理（NLP）领域，后来在计算机视觉中广泛应用。注意力机制本质上与人类对事物的观察机制相似：一般而言，我们在观察事物的时候，首先会倾向于观察事物一些重要的局部信息（如下图所示，我们会首先将注意力集中在目标而不是背景的身上），然后再去关心一些其他部分的信息，最后组合起来形成整体印象。</p><p>注意力机制能够使得深度学习在提取目标的特征时更加具有针对性，使得相关任务的精度有所提升。注意力机制应用于深度学习通常是对输入每个部分赋予不同的权重，抽取出更加关键及重要的信息使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销，这也是注意力机制广泛使用的原因。</p><p><img src="https://i.loli.net/2020/12/16/ynLluegbMsiINTj.png"></p><p>在计算机视觉中，注意力机制主要和卷积神经网络进行结合，按照传统注意力的划分，大部分属于软注意力，实现手段常常是通过掩码（mask）来生成注意力结果。掩码的原理在于通过另一层新的权重，将输入特征图中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。说的更简单一些，网络除了原本的特征图学习之外，还要学会通过特征图提取权重分布，对原本的特征图不同通道或者空间位置加权。因此，按照加权的位置或者维度不同，将注意力分为<strong>空间域、通道域和混合域</strong>。</p><h2 id="典型方法"><a href="#典型方法" class="headerlink" title="典型方法"></a>典型方法</h2><p>卷积神经网络中常用的注意力有两种，即<strong>空间注意力和通道注意力</strong>，当然也有融合两者的混合注意力。</p><p>首先，我们知道，卷积神经网络输出的是维度为$C\times H \times W$的特征图，其中$C$指的是通道数，它等于作用与输入的卷积核数目，每个卷积核代表提取一种特征，所以每个通道代表一种特征构成的矩阵。$H \times W$这两个维度很好理解，这是一个平面，里面的每个值代表一个位置的信息，尽管经过下采样这个位置已经不是原始输入图像的像素位置了，但是依然是一种位置信息。如果，对每个通道的所有位置的值都乘上一个权重值，那么总共需要$C$个值，构成的就是一个$C$维向量，将这个$C$维向量作用于特征图的通道维度，这就叫<strong>通道注意力</strong>。同样的，如果我学习一个$H\times W$的权重矩阵，这个矩阵每一个元素作用于特征图上所有通道的对应位置元素进行乘法，不就相当于对空间位置加权了吗，这就叫做<strong>空间注意力</strong>。</p><p>下面我列举一些常见的使用了注意力机制的卷积神经网络，我在下面一节会详细介绍它们。</p><ol><li>NL(Non-local Neural Networks)</li><li>SENet(Squeeze-and-Excitation Networks)</li><li>BAM(Bottleneck Attention Module)</li><li>CBAM(Convolutional Block Attention Module)</li><li>$A^2$-Nets(Double Attention Networks)</li><li>GSoP-Net(Global Second-order Pooling Convolutional Networks)</li><li>ECA-Net(Efficient Channel Attention for Deep Convolutional Neural Networks)</li><li>GC-Net(Global context network for medical image segmentation)</li><li>SKNet(Selective Kernel Networks)</li><li>CC-Net(Criss-Cross Attention for Semantic Segmentation)</li><li>ResNeSt(ResNeSt: Split-Attention Networks)</li><li>Triplet Attention(Convolutional Triplet Attention Module)</li></ol><h2 id="网络详解"><a href="#网络详解" class="headerlink" title="网络详解"></a>网络详解</h2><h3 id="NL"><a href="#NL" class="headerlink" title="NL"></a><strong>NL</strong></h3><p>Non-local Neural Networks 应该算是引入自注意力机制比较早期的工作，后来的语义分割里各种自注意力机制都可以认为是 Non-local 的特例，这篇文章作者中同样有熟悉的何恺明大神 😂。</p><p>首先聊聊 Non-local 的动机，我们知道，CV 和 NLP 任务都需要捕获长程依赖（远距离信息交互），卷积本身是一种局部算子，CNN 中一般通过堆叠多层卷积层获得更大感受野来捕获这种长程依赖的，这存在一些严重的问题：效率低；深层网络的设计比较困难；较远位置的消息传递，局部操作是很困难的。所以，收到非局部均值滤波的启发，作者设计了一个泛化、简单、可直接嵌入主流网络的 non-local 算子，它可以捕获时间（一维时序数据）、空间（图像）和时空（视频）的长程依赖。</p><p><img src="https://i.loli.net/2020/12/16/pGaDETk4FHqx3oK.png"></p><p>首先，Non-local 操作早在图像处理中已经存在，典型代表就是非局部均值滤波，到了深度学习时代，在计算机视觉中，这种通过关注特征图中所有位置并在嵌入空间中取其加权平均值来计算某位置处的响应的方法，就叫做<strong>自注意力</strong>。</p><p>然后来看看深度神经网络中的 non-local 操作如何定义，也就是下面这个式子，这是一个通式，其中$x$是输入，$y$是输出，$i$和$j$代表输入的某个位置，可以是序列、图像或者视频上的位置，不过因为我比较熟悉图像，所以后文的叙述都以图像为例。因此$x_i$是一个向量，维数和通道一样；$f$是一个计算任意两点的相似性函数，$g$是一个一元函数，用于信息变换；$\mathcal{C}$是归一化函数，保证变换前后整体信息不变。所以，<strong>下面的式子，其实就是为了计算某个位置的值，需要考虑当前这个位置的值和所有位置的值的关系，然后利用这种获得的类似 attention 的关系对所有位置加权求和得到当前位置的值。</strong></p><p>$$<br>\mathbf{y}<em>{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum</em>{\forall j} f\left(\mathbf{x}<em>{i}, \mathbf{x}</em>{j}\right) g\left(\mathbf{x}_{j}\right)<br>$$</p><p>那么，确定了这个通式，在图像上应用，只需要确定$f$、$g$和$\mathcal{C}$即可，首先，由于$g$的输入是一元的，可以简单将$g$设置为 1x1 卷积，代表线性嵌入，算式为$g\left(\mathbf{x}<em>{j}\right)=W</em>{g} \mathbf{x}_{j}$。关于$f$和$\mathcal{C}$需要配对使用，作用其实就是计算两个位置的相关性，可选的函数有很多，具体如下。</p><ul><li><strong>Gaussian</strong><br>高斯函数，两个位置矩阵乘法然后指数映射，放大差异。<br>$$<br>f\left(\mathbf{x}<em>{i}, \mathbf{x}</em>{j}\right)=e^{\mathbf{x}<em>{i}^{T} \mathbf{x}</em>{j}}<br>$$<br>$$<br>\mathcal{C}(x)=\sum_{\forall j} f\left(\mathrm{x}<em>{i}, \mathrm{x}</em>{j}\right)<br>$$</li><li><strong>Embedded Gaussian</strong><br>嵌入空间的高斯高斯形式，$\mathcal{C}(x)$同上。<br>$$<br>\theta\left(\mathbf{x}<em>{i}\right)=W</em>{\theta} \mathbf{x}<em>{i} \text { and } \phi\left(\mathbf{x}</em>{j}\right)=W_{\phi} \mathbf{x}<em>{j}<br>$$<br>$$<br>f\left(\mathbf{x}</em>{i}, \mathbf{x}<em>{j}\right)=e^{\theta\left(\mathbf{x}</em>{i}\right)^{T} \phi\left(\mathbf{x}<em>{j}\right)}<br>$$<br>论文中这里还特别提了一下，如果将$\mathcal{C}(x)$考虑进去，对$\frac{1}{\mathcal{C}(\mathbf{x})} f\left(\mathbf{x}</em>{i}, \mathbf{x}<em>{j}\right)$而言，这其实是一个 softmax 计算，因此有$\mathbf{y}=\operatorname{softmax}\left(\mathbf{x}^{T} W</em>{\theta}^{T} W_{\phi} \mathbf{x}\right) g(\mathbf{x})$，这个其实就是 NLP 中常用的自注意力，因此这里说，自注意力是 Non-local 的特殊形式，Non-local 将自注意力拓展到了图像和视频等高维数据上。但是，softmax 这种注意力形式不是必要的，因此作者设计了下面的两个 non-local 操作。</li><li><strong>Dot product</strong><br>这里去掉了指数函数形式，$\mathcal{C}(x)$的形式也相应改变为$x$上的像素数目。<br>$$<br>f\left(\mathbf{x}<em>{i}, \mathbf{x}</em>{j}\right)=\theta\left(\mathbf{x}<em>{i}\right)^{T} \phi\left(\mathbf{x}</em>{j}\right)<br>$$<br>$$<br>\mathcal{C}(\mathbf{x})=N<br>$$</li><li><strong>Concatenation</strong><br>最后，是一种 concat 的形式，$[\cdot, \cdot]$表示 concat 操作，$\mathcal{C}(x)$同上。<br>$$<br>f\left(\mathbf{x}<em>{i}, \mathbf{x}</em>{j}\right)=\operatorname{ReLU}\left(\mathbf{w}<em>{f}^{T}\left[\theta\left(\mathbf{x}</em>{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)<br>$$</li></ul><p>有了上面定义的 non-local 算子，就可以定义<strong>non-local 模块</strong>了，其定义如下，其中的$+\mathbf{x}_{i}$表示残差连接，这种残差形式可以保证该模块可以嵌入预训练模型中，只要将$W_z$初始化为 0 即可。其实这中间的实现都是通过 1x1 卷积完成的，因此输出和输入可以控制为同等通道数。</p><p>$$<br>\mathbf{z}<em>{i}=W</em>{z} \mathbf{y}<em>{i}+\mathbf{x}</em>{i}<br>$$</p><p>上面是数学上的定义，具体来看，一个时空格式的 non-local 模块如下图，我们从二维图像的角度来看，可以忽略那个$T$，直接将其置为 1 即可。所以，输入是$(h,w,1024)$，经过两个权重变换$W_\theta$和$W_\phi$得到降维后的两个特征图，都为$(h, w, 512)$，它们两者 reshape 后变为$(h\times w, 512)$，之后再其中一个转置后矩阵乘法另一个，得到相似性矩阵$(h\times w, h \times w)$，然后再最后一个维度上进行 softmax 操作。上述这个操作得到一种自注意力图，表示每个像素与其他位置像素的关系。然后将原始输入通过变换$g$得到一个$(h\times w, 512)$的输入矩阵，它和刚刚的注意力图矩阵乘法，输出为$(h\times w, 512)$，这时，每个位置的输出值其实就是其他位置的加权求和的结果。最后，通过 1x1 卷积来升维恢复通道，保证输入输出同维。</p><p><img src="https://i.loli.net/2020/12/16/V5RKiDI7bPEJydG.png"></p><p>这篇文章有很不错的<a href="https://github.com/AlexHex7/Non-local_pytorch">第三方 Pytorch 实现</a>，想了解更多细节的可以去看看源码和论文，其实实现是很简单的。</p><p>最后，简单总结一下这篇文章。提出了 non-local 模块，这是一种自注意力的泛化表达形式，Transformer 的成功已经证明自注意力的长程交互信息捕获能力很强，对网络尤其是处理视频的网络如 I3D 是有参考意义的。当然，其本质还是空间层面的注意力，如果考虑通道注意力或许会获得更好的效果，而且 non-local 模块的矩阵计算开销其实不低，这也制约了 non-local 的广泛应用，略微有点遗憾。</p><h3 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a><strong>SENet</strong></h3><p>SENet 应该算是 Non-local 的同期成果，我在<a href="https://zhouchen.blog.csdn.net/article/details/110826497">之前的文章</a>中专门解读过，这里就大概的说一下。</p><p>卷积操作是卷积神经网络的核心，卷积可以理解为在一个局部感受野范围内将空间维度信息和特征维度信息进行聚合，聚合的方式是加和操作。然而想要提高卷积神经网络的性能其实是很难的，需要克服很多的难点。为了获得更加丰富的空间信息，很多工作被提出，如下图使用多尺度信息聚合的Inception。</p><p><img src="https://i.loli.net/2020/12/16/QXprjOhZCV3ldFc.png"></p><p>那么，自然会想到，能否在通道维度上进行特征融合呢？其实卷积操作默认是有隐式的通道信息融合的，它对所有通道的特征图进行融合得到输出特征图（这就默认每个通道的特征是同权的），这就是为什么一个32通道的输入特征图，要求输出64通道特征图，需要$32\times64$个卷积核（这个32也可以理解为卷积核的深度）。通道方面的注意力也不是没人进行尝试，一些轻量级网络使用分组卷积和深度可分离卷积对通道进行分组操作，但这本质上只是为了减少参数，并没有什么特征融合上的贡献。</p><p>所以，SENet出现了，它从从每个通道的特征图应该不同权的角度出发，更加关注通道之间的关系，让模型学习到不同通道的重要程度从而对其加权，即显式建模不同通道之间的关系。为此，设计了SE（Squeeze-and-Excitation）模块，如下图所示。SE模块的思路很简单，先通过全局平均池化获得每个通道的全局空间表示，再利用这个表示学习到每个通道的重要性权重，这些权重作用与原始特征图的各个通道，得到通道注意力后的特征图。由于轻量简洁，SE模块可以嵌入任何主流的卷积神经网络模型中，因为其可以保证输入输出同维。</p><p><img src="https://i.loli.net/2020/12/16/mBIL5fi3kG8197K.png"></p><h3 id="BAM"><a href="#BAM" class="headerlink" title="BAM"></a><strong>BAM</strong></h3><p>这篇文章和下面的CBAM是同一个团队的成果，非常类似，CBAM收录于ECCV2018，BAM收录于BMVC2018，两篇文章挂到Arxiv上的时间也就差了几分钟而已，这里先简单地说一下区别，BAM其实是通道注意力和空间注意力的并联，CBAM是两者的串联。</p><p>BAM，全名Bottleneck Attention Module，是注意力机制在卷积神经网络中的一次伟大尝试，它提出的BAM模块可以集成到任意的卷积神经网络中，通过channel和spatial两个分支得到注意力图，通道注意力关注语义信息回答what问题，空间注意力关注位置信息，回答where问题，因此结合起来是最好的选择。下图是BAM集成到一个卷积神经网络的示意图，显然，BAM存在于池化层之前，这也是bottleneck的由来，作者说这样的多个BAM模块构建的注意力图层类似人类的感知过程。</p><p><img src="https://i.loli.net/2020/12/16/tcZXoqDmR4MUuBK.png"></p><p><img src="https://i.loli.net/2020/12/16/WYcVdF36JHbTys8.png"></p><p>上图就是核心的BAM模块结构图，我们来一步步看它是如何实现通道空间混合注意力的。整体来看，对于输入特征图$\mathbf{F} \in \mathbb{R}^{C \times H \times W}$，BAM模块最终会得到一个注意力图$\mathbf{M}(\mathbf{F}) \in \mathbb{R}^{C \times H \times W}$，<strong>这里注意到，这是一个和输入同维的张量，此前，通道注意力学习到的是个$C$维向量，空间注意力学到的是个$H\times W$维的矩阵，BAM这种格式表明其混合了通道和空间的注意力信息。</strong> 调整后输出的特征图$\mathbf{F}^{\prime}=\mathbf{F}+\mathbf{F} \otimes \mathbf{M}(\mathbf{F})$，显然，这是一个张量点乘后进行加法的运算，加法是明显的残差结构，点乘发生在学到的注意力图和输入特征图之间，因此输出和输入同样shape。为了计算上高效，通道和空间注意力采用并行的分支结构获得。因此，整体计算上要先获得通道注意力图$\mathbf{M}<em>{\mathbf{c}}(\mathbf{F}) \in \mathbb{R}^{C}$和空间注意力图$\mathbf{M}</em>{\mathbf{s}}(\mathbf{F}) \in \mathbb{R}^{H \times W}$，上面的最终注意力图通过下式计算得到，其中$\sigma$表示Sigmoid激活函数，两个注意力图的加法需要broadcast，得到的就是$C\times H \times W$维度了。</p><p>$$<br>\mathbf{M}(\mathbf{F})=\sigma\left(\mathbf{M}<em>{\mathbf{c}}(\mathbf{F})+\mathbf{M}</em>{\mathbf{s}}(\mathbf{F})\right)<br>$$</p><p>然后，我们再来看看具体的两个分支内发生了什么。首先，看通道注意力分支，首先，对输入特征图$\mathbf{F}$进行全集平均池化得到$\mathbf{F}<em>{\mathbf{c}} \in \mathbb{R}^{C \times 1 \times 1}$，这相当于在每个通道的空间上获得了全局信息。然后，这个向量送入全连接层进行学习，这里进行了一个先降维再升维的操作，所以学到的向量$\mathbf{M}</em>{\mathbf{c}}(\mathbf{F})$依然是$C \times 1 \times 1$维度的，这个就是通道注意力图。</p><p>接着，我们看看空间注意力分支。作者这里先用1x1卷积对输入特征图降维，然后使用两层膨胀卷积以更大的感受野获得更丰富的信息$\mathbf{F_{temp}} \in \mathbb{R}^{C / r \times H \times W}$，最后再用1x1卷积将特征图降维到通道数为1，得到空间注意力图$\mathbf{M}_{\mathbf{s}}(\mathbf{F}) \in \mathbb{R}^{H\times W}$。</p><p><strong>至此，我们理解了BAM模块的结构，它可以嵌入到主流网络中获得一些性能提升，不过后来并没有在各种任务中获得较好的表现，因此不是很广泛，但它的混合注意力思路是值得借鉴的。</strong></p><h3 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a><strong>CBAM</strong></h3><p>CBAM，全名Convolutional Block Attention Module，相对于BAM，在CV中受到了更多的关注，下图就是CBAM的整体结构图，不难发现，它和BAM区别就是通道注意力和空间注意力是串联进行的，实践证明，这样的效果更好一些。</p><p><img src="https://i.loli.net/2020/12/16/xvHuDgG4o2cbhWd.png"></p><p>我们先从上图整体上看看CBAM怎么进行注意力的，首先，输入特征图$\mathbf{F} \in \mathbb{R}^{C \times H \times W}$和通道注意力图$\mathbf{M}<em>{\mathbf{c}} \in \mathbb{R}^{C \times 1 \times 1}$逐通道相乘得到$\mathbf{F’}$，接着，$\mathbf{F’}$会和空间注意力图$\mathbf{M}</em>{\mathbf{s}} \in \mathbb{R}^{1 \times H \times W}$逐位置相乘得到$\mathbf{F’’}$，这就是CBAM的输出，它依然是$C \times H \times W$维度的。</p><p><img src="https://i.loli.net/2020/12/16/JMIqi3S6vuPg7LE.png"></p><p>上图就是两个注意力模块的具体实现，我们先看通道注意力，它很类似于SENet，先是利用全局池化获得每个通道的位置全局信息，不过这里采用全局平均池化和全局最大池化分别得到$\mathbf{F}<em>{\mathrm{avg}}^{\mathrm{c}}$和$\mathbf{F}</em>{\mathrm{max}}^{\mathrm{c}}$，均得到一个$C\times 1\times 1$维的向量，经过共同的全连接层的降维再升维学习两个通道注意力（降维的维度用一个缩放比例控制），加到一起，获得的注意力图仍然是$\mathbf{M}_{\mathbf{c}}(\mathbf{F}) \in \mathbb{R}^{C\times 1 \times 1}$。</p><p>再来看空间注意力，也是采用全局平均池化和全局最大池化，不过是沿着通道进行的，所以得到两个特征图$\mathbf{F}<em>{\text {avg }}^{\mathbf{s}} \in \mathbb{R}^{1 \times H \times W}$ 和 $\mathbf{F}</em>{\max }^{\mathbf{s}} \in \mathbb{R}^{1 \times H \times W}$，然后将它们concat一起后使用一个7x7的卷积进行处理，得到$\mathbf{M}_{\mathbf{s}}(\mathbf{F}) \in \mathbb{1}^{C\times H \times W}$。</p><p>将上述通道注意力图和空间注意力图按照下面的公式先后作用于输入特征图，就得到混合注意力的结果。<strong>至此，我们理解了CBAM模块的运行过程，其中的激活函数和BN等细节我没有提到，可以查看原论文，这里相比于BAM都采用了两个全局池化混合的方式，这在今天的网络中已经很常见了，属于捕获更加丰富信息的手段。</strong></p><p>$$<br>\begin{aligned}<br>\mathbf{F}^{\prime} &amp;=\mathbf{M}<em>{\mathbf{c}}(\mathbf{F}) \otimes \mathbf{F} \<br>\mathbf{F}^{\prime \prime} &amp;=\mathbf{M}</em>{\mathbf{s}}\left(\mathbf{F}^{\prime}\right) \otimes \mathbf{F}^{\prime}<br>\end{aligned}<br>$$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文简单介绍了计算机视觉中几种比较早期的采用注意力机制的卷积神经网络，它们的一些设计理念今天还被活跃用于各类任务中，是很值得了解的。后面的文章会介绍一些相对比较新的成果，欢迎关注。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视觉注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SENet&amp;SKNet解读</title>
      <link href="/2020/12/07/senet-sknet/"/>
      <url>/2020/12/07/senet-sknet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今年有很多基于 ResNet 的新成果诞生，包括由于代码实现错误等问题引起广泛关注却屠榜各个榜单的 ResNeSt，关于 ResNeSt 的好坏这里不多做评论，不过它基于的前人工作 SENet 和 SKNet 其实是很值得回顾的，本文就聊聊这两个卷积神经网络历史上标杆式的作品。</p><ul><li>SENet: <a href="http://arxiv.org/abs/1709.01507">Squeeze-and-Excitation Networks</a>（CVPR2018）</li><li>SKNet: <a href="http://arxiv.org/abs/1903.06586">Selective Kernel Networks</a>（CVPR2019）</li></ul><h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><p>SENet 获得了 ImageNet2017 大赛分类任务的冠军，这也是最后一届 ImageNet 比赛，论文同时获得了 CVPR2018 的 oral。而且，SENet 思路简单，实现方便，计算量小，模块化涉及，可以无缝嵌入主流的网络结构中，实践不断证明其可以使得网络获得更好的任务效果。</p><h3 id="动机和思路"><a href="#动机和思路" class="headerlink" title="动机和思路"></a><strong>动机和思路</strong></h3><p>我们知道，卷积操作是卷积神经网络的核心，卷积可以理解为在一个局部感受野范围内将空间维度信息和特征维度信息进行聚合，聚合的方式是加和（sum）。然后想要提高卷积神经网络的性能其实是很难的，需要克服很多的难点。为了获得更加丰富的空间信息，很多工作被提出，如使用多尺度信息聚合的 Inception（下面左图，图来自官方分享）、考虑空间上下文的 Inside-outside Network（下面右图）以及一些注意力机制。</p><p><img src="https://i.loli.net/2020/12/07/jbxznFHyrITCYlR.jpg"></p><p>那么，自然会想到，能否在通道（channel）维度上进行特征融合呢？首先，其实卷积操作默认是有隐式的通道信息融合的，它对所有通道的特征图进行融合得到输出特征图（这就默认每个通道的特征是同权的），这就是为什么一个 32 通道的输入特征图，要求输出 64 通道特征图，需要 32x64 个卷积核。这方面也不是没人进行尝试，一些轻量级网络使用分组卷积和深度可分离卷积对 channel 进行分组操作，而这本质上只是为了减少参数，并没有什么特征融合上的贡献。</p><p>SENet 则从每个通道的特征图应该不同权角度出发，更加关注 channel 之间的关系，让模型学习到不同 channel 的重要程度从而对其加权，即显式建模不同 channel 之间的关系。为此，设计了 SE（Squeeze-and-Excitation）模块，我这边译作压缩激励模块，不过后文还是采用 SE 进行阐述。</p><h3 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a><strong>结构设计</strong></h3><p>下图就是 SE 模块的结构图，对输入$X$进行一系列卷积变换$\mathbf{F}<em>{t r}$后得到$U$，维度也从$(C’,H’,W’)$变为$(C,H,W)$。这里假定$\mathbf{F}</em>{t r}$就是一个卷积操作，并且卷积核为$\mathbf{V}=\left[\mathbf{v}<em>{1}, \mathbf{v}</em>{2}, \ldots, \mathbf{v}<em>{C}\right]$，$\mathbf{v}</em>{c}$表示第$c$个卷积核的参数，那么输出$\mathbf{U}=\left[\mathbf{u}<em>{1}, \mathbf{u}</em>{2}, \ldots, \mathbf{u}_{C}\right]$，这种表示源于下式，这个不难理解，其中$*$表示卷积运算。</p><p>$$<br>\mathbf{u}<em>{c}=\mathbf{v}</em>{c} * \mathbf{X}=\sum_{s=1}^{C^{\prime}} \mathbf{v}_{c}^{s} * \mathbf{x}^{s}<br>$$</p><p>上面的式子说明了一个什么问题呢？一个卷积核到输入 feature map 的每一个 channel 上进行了操作然后加和在一起，channel 特征和卷积核学到的空间特征混杂纠缠在了一起。这就是说，默认卷积操作建模的通道之间的关系是隐式和局部的。作者希望显示建模通道之间的依赖关系来增强卷积特征的学习。因此，论文希望提供全局信息捕获的途径并且<strong>重标定</strong>卷积结果，所以设计了两个操作<strong>压缩（squeeze）</strong>和<strong>激励（excitation）</strong>。</p><p><img src="https://i.loli.net/2020/12/07/OSTCu8DWXP5wyZm.png"></p><p>上图就是 SE 模块的结构，我们先来看看压缩操作。卷积是在一个局部感受野范围内操作，因此$U$很难获得足够的信息来捕获全局的通道之间的关系。所以这里通过压缩全局空间信息为一个通道特征的操作，论文中采用全局平均池化（GAP）来实现这个目的，具体的$\mathbf{z} \in \mathbb{R}^{C}$通过下式计算，通过在$H\times W$维度上压缩$U$得到。</p><p>$$<br>z_{c}=\mathbf{F}<em>{s q}\left(\mathbf{u}</em>{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)<br>$$</p><p>激励操作需要利用压缩操作得到的全局描述来抓取 channel 之间的关系，所以激励操作必须满足两个特性：灵活，它要能捕获通道间的非线性关系；不互斥，我们希望多个通道都被增强或者多个通道被抑制而不是要得到 one-hot 那样的结果。所以采用下述的简单 gating 机制，其中$W_{1} \in R^{\frac{C}{r} \times C}, W_{2} \in R^{C \times \frac{C}{r}}$。</p><p>$$<br>\mathbf{s}=\mathbf{F}<em>{e x}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}</em>{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)<br>$$</p><p>为了降低模型复杂度以及提升泛化能力，这里采用两个全连接层构成 bottleneck 的结构，其中第一个全连接层起到降维的作用，降维比例$r$是个超参数，然后采用 ReLU 激活，然后第二个全连接层用于恢复特征维度最后 Sigmoid 激活输出，显然输出$\mathbf{s}$是一个$C$维的向量，它表征了各个 channel 的重要性且值在$(0,1)$之间，将这个$\mathbf{s}$逐通道乘上输入$U$即可，这相当于对每个 channel 加权，这种操作类似什么呢？Attention。</p><p>$$<br>\widetilde{\mathbf{x}}<em>{c}=\mathbf{F}</em>{\text {scale}}\left(\mathbf{u}<em>{c}, s</em>{c}\right)=s_{c} \mathbf{u}_{c}<br>$$</p><h3 id="结构应用"><a href="#结构应用" class="headerlink" title="结构应用"></a><strong>结构应用</strong></h3><p>SE 模块可以无缝嵌入到主流的网络结构中，以 Inception 和 ResNet 为例，改造前后的结构如下图，Inception 比较直白，右边的残差网络则将 SE 模块直接嵌入残差单元中。当然，其在 ResNetXt，Inception-ResNet，MobileNet 和 ShuffleNet 等结构基础上也可以嵌入。</p><p>而且，经过分析可以得知，SE 模块算力增加并不大，在 ResNet50 上，虽然增加了约 10%的参数量，但计算量（GFLOPS）却增加不到 1%。</p><p><img src="https://i.loli.net/2020/12/07/Esir4h87akFveRZ.png"></p><p>作者也在主流任务上测试了 SE 模块的适用性，基本上在各个任务上都有涨点，感兴趣的可以查看原论文，不过其能获得 ImageNet2017 的冠军已经说明了 SE 模块的强大。而且其实现也是非常简单的，基本的 SE 模块的 PyTorch 实现如下（参考<a href="https://github.com/moskomule/senet.pytorch">开源链接</a>）。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SELayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SELayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channel<span class="token punctuation">,</span> channel <span class="token operator">//</span> reduction<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channel <span class="token operator">//</span> reduction<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> y<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><h2 id="SKNet"><a href="#SKNet" class="headerlink" title="SKNet"></a>SKNet</h2><p>SENet 设计了 SE 模块来提升模型对 channel 特征的敏感性，CVPR2019 的 SKNet 和 SENet 非常相似，它主要是为了提升模型对感受野的自适应能力，这种自适应能力类似 SENet 对各个通道做类似 attention，只不过是对不同尺度的卷积分支做了这种 attention。</p><h3 id="动机和思路-1"><a href="#动机和思路-1" class="headerlink" title="动机和思路"></a><strong>动机和思路</strong></h3><p>我们设计卷积神经网络的时候参考了动物的视觉机制，但是一个重要的思路并没有在设计卷积神经网络时被过多关注，那就是感受野的尺寸是根据刺激自动调节的，SKNet就从这个思路出发，自适应选择更重要的卷积核尺寸（这个实现其实就是对不同尺度的特征图分别加权，这个权重由网络学习得到，这就是我为什么说和SENet很类似的原因）。</p><p>首先，我们考虑如何获得不同尺寸的感受野信息呢，一个非常直接的想法就是使用不同size的卷积核，然后把他们融合起来就行了，这个思路诞生了下图的Inception网络，不过，Inception将所有分支的多尺度信息线性聚合到了一起，这也许是不合适的，因为不同的尺度的信息有着不同的重要程度，网络应该自己学到这种重要程度（看到这是不是觉得和SENet针对的不同通道信息应该有不同重要程度有类似之处）。</p><p><img src="https://i.loli.net/2020/12/07/hFZLVTjguEdtyqO.png"></p><p>所以，这篇论文，作者设计了一种非线性聚合不同尺度的方法来实现自适应感受野，引入的这种操作称为SK卷积（Selective Kernel Convolution），它包含三个操作：Split、Fuse和Select。Split操作很简单，通过不同的卷积核size产生不同尺度的特征图；Fuse操作则通过聚合不同尺度的信息产生全局的选择权重；最后的Select操作通过这个选择权重来聚合不同的特征图。SK卷积可以嵌入主流的卷积神经网络之中且只会带来很少的计算量，其在大规模的ImageNet和小规模的CIFAR-10上都超越了SOTA方法。</p><h3 id="结构设计-1"><a href="#结构设计-1" class="headerlink" title="结构设计"></a><strong>结构设计</strong></h3><p>在具体了解SKNet之前，必须知道不少的前置知识，比如多分支卷积神经网络（不同size的卷积核特征融合）、分组卷积及深度可分离卷积及膨胀卷积（减少运算量的高效卷积方式）、注意力机制（增强网络的重点关注能力），这些我这边就不多做解释了。</p><p><img src="https://i.loli.net/2020/12/07/gZNdSo9ImtO1HjE.png"><br>上图就是SK卷积的一个基础实现，为了方便描述，作者只采用了两个分支，事实上可以按需增加分支，原理是一样的。可以看到，从左往右分别是三个part：Split、Fuse和Select，下面我就一步步来解释这三个操作是如何获得自适应感受野信息的，解释是完全对照上面这个图来的。</p><p><strong>Split</strong>：对给定的特征图$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$，对其采用两种卷积变换$\widetilde{\mathcal{F}}: \mathbf{X} \rightarrow \tilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$和$\mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$，它们只有卷积核size不同（这里以3和5为例），其余配置一致（卷积采用深度可分离卷积，5x5卷积采用3x3卷进进行膨胀）。这一步，通过两个变换构建了两个感受野的分支，形成了两个特征图$\tilde{\mathbf{U}}$和$\widehat{\mathbf{U}}$，它们的维度都是$H\times W \times C$。</p><p><strong>Fuse</strong>：这一步也就是自适应感受野的核心，这里采用最简单的gates机制控制进入下一层的多尺度信息流。因此，这个gates需要集成来自所有分支的信息，还要有权重的集成。首先，通过逐元素相加获得特征图$\mathbf{U}$（$\mathbf{U}=\tilde{\mathbf{U}}+\widehat{\mathbf{U}}$），然后采用SENet类似的思路，通过GAP生成逐通道的统计信息$\mathbf{s} \in \mathbb{R}^{C}$，计算式如下。</p><p>$$<br>s_{c}=\mathcal{F}<em>{g p}\left(\mathbf{U}</em>{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)<br>$$</p><p>接着，为了更紧凑的表示，通过一个全连接层对$\mathbf{s}$进行降维，获得$\mathbf{z}=\mathcal{F}<em>{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))$，这里先是和$\mathbf{W} \in \mathbb{R}^{d \times C}$相乘然后经过BN和ReLU，$d$作为一个超参使用下降比$r$来控制，不过$d$同时通过下面的式子约束下界（$L$设置为32）。接着，又一个全连接用于升维度，得到分支数个$C$维向量，论文中这里就是$a$和$b$，然后按照通道维度进行soft attention，也就是说$a</em>{c}+b_{c}=1$，这样可以反映不同尺度的特征的重要性，然后用$a$和$b$采用类似SE的方式对原始特征图$\tilde{\mathbf{U}}$和$\widehat{\mathbf{U}}$进行逐通道相乘加权，得到有通道区分度的特征图，再相加到一起得到输出特征图$\mathbf{V}$。<strong>这个特征图，就是自适应不同感受野获得的特征图。</strong></p><p>$$<br>d=\max (C / r, L)<br>$$</p><h3 id="结构应用-1"><a href="#结构应用-1" class="headerlink" title="结构应用"></a><strong>结构应用</strong></h3><p>将SK卷积应用到ResNeXt50中，得到SKNet50，具体配置如下图，在主流任务上都有突破。SK卷积也适用于轻量级网络，因为其不会带来多少算力增加。</p><p><img src="https://i.loli.net/2020/12/07/r9qWGCHwPD5Uueb.png"></p><p>SK卷积的PyTorch实现如下（参考<a href="https://github.com/ResearchingDexter/SKNet_pytorch">开源链接</a>）。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SKConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>M<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>r<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>L<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SKConv<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        d<span class="token operator">=</span>max<span class="token punctuation">(</span>in_channels<span class="token operator">//</span>r<span class="token punctuation">,</span>L<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>M<span class="token operator">=</span>M        self<span class="token punctuation">.</span>out_channels<span class="token operator">=</span>out_channels        self<span class="token punctuation">.</span>conv<span class="token operator">=</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>M<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">1</span><span class="token operator">+</span>i<span class="token punctuation">,</span>dilation<span class="token operator">=</span><span class="token number">1</span><span class="token operator">+</span>i<span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                           nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>                                           nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>global_pool<span class="token operator">=</span>nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span>d<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                               nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">,</span>                               nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2<span class="token operator">=</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>d<span class="token punctuation">,</span>out_channels<span class="token operator">*</span>M<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>softmax<span class="token operator">=</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_size<span class="token operator">=</span>input<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        output<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># the part of split</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span>conv <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">)</span><span class="token punctuation">:</span>            output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># the part of fuse</span>        U<span class="token operator">=</span>reduce<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>x<span class="token operator">+</span>y<span class="token punctuation">,</span>output<span class="token punctuation">)</span>        s<span class="token operator">=</span>self<span class="token punctuation">.</span>global_pool<span class="token punctuation">(</span>U<span class="token punctuation">)</span>        z<span class="token operator">=</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>s<span class="token punctuation">)</span>        a_b<span class="token operator">=</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        a_b<span class="token operator">=</span>a_b<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>self<span class="token punctuation">.</span>M<span class="token punctuation">,</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        a_b<span class="token operator">=</span>self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>a_b<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># the part of select</span>        a_b<span class="token operator">=</span>list<span class="token punctuation">(</span>a_b<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>M<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#split to a and b</span>        a_b<span class="token operator">=</span>list<span class="token punctuation">(</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>a_b<span class="token punctuation">)</span><span class="token punctuation">)</span>        V<span class="token operator">=</span>list<span class="token punctuation">(</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>x<span class="token operator">*</span>y<span class="token punctuation">,</span>output<span class="token punctuation">,</span>a_b<span class="token punctuation">)</span><span class="token punctuation">)</span>        V<span class="token operator">=</span>reduce<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>x<span class="token operator">+</span>y<span class="token punctuation">,</span>V<span class="token punctuation">)</span>        <span class="token keyword">return</span> V</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SENet和SKNet分别从通道信息和感受野自适应角度出发，设计了一个新的网络结构，获得了比较有突破的成果，SKNet是SENet基础上的工作，还集成了近几年卷积神经网络的一些主流技巧，可以说集众家之长，也可以说是人工设计卷积神经网络的集大成者了，SKNet后来的很多效果更好的卷积神经网络或多或少带有NAS技术的影子，不过，自动搜索也是不可阻挡的未来趋势。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SENet </tag>
            
            <tag> SKNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TPAGT解读</title>
      <link href="/2020/12/04/tpagt/"/>
      <url>/2020/12/04/tpagt/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>浙江大学和达摩院前不久提出的一个 MOT 新方法，目前在 MOT Challenge 常用的几个数据集上名列前茅。论文标题 Tracklets Predicting Based Adaptive Graph Tracking 其实已经表明本文最大的两个创新点，基于轨迹预测的特征提取以及基于自适应图网络的特征聚合。大多数现存的多目标跟踪方法将当前帧的检测结果链接到历史轨迹段都是采用基于特征余弦距离和目标边界框 IOU 的线性组合作为度量的，这其实有两个问题：<strong>一是两个不同帧（当前帧和上一帧）上同一个目标提取到的特征往往会出现不一致的问题；二是特征提取只考虑外观而不考虑位置关系、轨迹段信息是不合理的。</strong></p><p>因此，论文提出了一种新的高精度端到端多目标跟踪框架 TPAGT（上一个版本叫 FGAGT，感觉 TPAGT 更加贴合论文的工作），该方法解决了上述的两个问题，在多个数据集上实现了新的 SOTA。</p><ul><li>论文标题<br><br>Tracklets Predicting Based Adaptive Graph Tracking</li><li>论文地址<br><br><a href="http://arxiv.org/abs/2010.09015">http://arxiv.org/abs/2010.09015</a></li><li>论文源码<br><br>暂未开源</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>首先说明的是，TPAGT 按照一般 MOT 的方法划分是一个<strong>二阶段框架</strong>，也就是先完成检测，再按照检测结果到相应的位置提取目标特征，最后利用关联算法得到结果，关联一般采用匈牙利算法。单阶段方法融合了检测和特征提取，是为了速度做出的精度妥协，所以精度相比二阶段有些低。所以，作为一个二阶段方法，TPAGT 的精度应该有所创新，但是相应的速度比较慢，具体推理速度，论文没有提及，只能等源码开放后测试了。</p><p>先来说一说 MOT 现有方法没解决的几个问题。</p><ol><li>特征不一致问题<br><br>这个问题怎么来的呢，其实是因为轨迹段（tracklet）上目标的特征都是来自于之前帧，而不是当前帧（这很容易理解，当前帧只有当前帧的检测结果确定目标位置来提取特征嘛），但是呢，其实在移动过程中，目标的姿态、光强度、视角都可能发生变化，这导致来自不同图像的同一目标的特征即使检测准确也会不太一致，这种不一致对数据关联来说负面影响比较大。</li><li>特征融合问题<br><br>事实上，从 DeepSORT 开始，特征提取器主要关注的就是外观信息，因为这对忽略了运动建模的一些 MOT 方法至关重要，因此特征提取分支也成为 ReID 分支，主要就是因为重识别模型关注的就是外观信息。但是，目标之间的位置关系、tracklet 的历史信息对 MOT 任务也是很重要的。</li><li>样本不平衡问题<br><br>一个 tracklet 只能匹配一个检测框，那这个 tracklet 就是个连续的正例，没有匹配上的 tracklet 就是连续的负例。显然，正例数量是远远少于负例的，而且由于少量的新目标的产生和旧目标的消失，进一步加剧了不同类型的样本的不均衡问题。</li></ol><p>上述的问题 TPAGT 都逐一解决了，其中最主要的一个问题就是 traklets 中的特征和当前帧是不一致的，那么如何解决呢，到当前帧上<strong>重提取</strong>特征就行，但是显然不能直接把上一帧的 bbox（边界框，包含目标的位置区域等信息）用于当前帧，因为目标在图像上不可能静止，使用上一时刻的位置很不合理，所以需要对上一帧进行运动估计得到目标在当前帧预测的 bbox 位置然后提取特征。然后是特征融合的问题，考虑到目标之间的联系近似一个图表示，作者采用了<strong>GNN</strong>（图神经网络）来进行信息的聚合，为了更好获取全局时空信息，GNN 的边权自适应学习。最后，样本不平衡的问题采用了<strong>Balanced MSE Loss</strong>，这是一个加权 MSE，属于常用思路。</p><h2 id="框架设计"><a href="#框架设计" class="headerlink" title="框架设计"></a>框架设计</h2><h3 id="Tracklets-predicting-based-feature-re-extracting"><a href="#Tracklets-predicting-based-feature-re-extracting" class="headerlink" title="Tracklets predicting based feature re-extracting"></a><strong>Tracklets predicting based feature re-extracting</strong></h3><p><img src="https://i.loli.net/2020/12/04/tfmMKJpOq5BgcnR.png"></p><p>上面这个图就是整体框架的设计，我先大体介绍一下网络的 pipeline。首先，网络的输入有当前帧图像、当前帧检测结果、历史帧检测结果；接着，图像被送入 backbone 中获得特征图（这里 backbone 最终采用 ResNet101+FPN 效果最好），然后将 bbox（这里当前帧用的是检测的 bbox，上一帧用的光流预测的 bbox）映射到特征图上通过 RoI Align 获得 region 外观特征继而送入全连接（这个操作类似 Faster R-CNN 的 proposal 提取特征，不理解的可以查阅我的<a href="https://zhouchen.blog.csdn.net/article/details/110404238">博客</a>），然后结合当前帧的位置信息、历史帧信息，让图网络自适应学习进行特征融合从而计算相似度，有了相似度矩阵匈牙利就能计算匹配结果了。</p><p><strong>上面的叙述有个容易误解的地方，它将过去一帧预测的 bbox 和历史帧的非预测的 bbox 都在当前特征图上提取了特征，事实上，不是的，一来实际上，$t-2$帧的特征在处理$t-1$帧的时候已经重提取过了，在当前帧上用当时的 bbox 提取肯定存在严重的不对齐问题；二来，这样会大大加大网络计算的复杂性，完全没有必要。论文这个图画的稍微有些让人误解，等开源后可以再细细研究。</strong></p><p>我们知道，此前的 MOT 方法对运动的建模主要采用卡尔曼滤波为代表的状态估计方法、光流法和位移预测法，这篇论文使用稀疏光流法预测 bbox 的中心点运动，由于目标的运动有时候是高速的，为了应对这种运动模式，必须采用合适的光流方法，文章采用金字塔光流，该方法鲁棒性很强，具体想了解的可以参考<a href="https://blog.csdn.net/gh_home/article/details/51502933">这篇博客</a>，下图是金字塔光流预测的目标当前帧位置（b 图），c 图是 GT 的框，可以看到，预测还是很准的。</p><p><img src="https://i.loli.net/2020/12/04/cg1P6rNB49bGSyH.png"></p><h3 id="Adapted-Graph-Neural-Network"><a href="#Adapted-Graph-Neural-Network" class="headerlink" title="Adapted Graph Neural Network"></a><strong>Adapted Graph Neural Network</strong></h3><p><img src="https://i.loli.net/2020/12/04/NtzZBPnAXgE6l7U.png"></p><p>下面聊一聊这个自适应图神经网络。将 tracklets 和 detections 作为二分图处理不是什么新鲜的事情，但是用来聚合特征 TPAGT 应该是为数不多的工作，<strong>要知道此前我们聚合运动和外观特征只是人工设计的组合，作者这种借助图网络自适应聚合特征是很超前的思路。</strong> 每个检测目标和每个 tracklet 都是节点，如上图所示，detection 之间没有联系，tracklet 之间也没有联系，但是每个 tracklet 和每个 detection 之间都有连接。图网络的学习目的就是每个节点的状态嵌入$\mathbf{h}<em>{v}$，或者说聚合其他信息后的特征向量。最终，这个$\mathbf{h}</em>{v}$包含了邻居节点的信息。</p><p>需要学习的状态嵌入通过下面的公式更新，第一行表示 detections 的节点更新，第二行表示 tracklets 的节点更新，共有$N$个 detection 和$M$个 tracklet。下面讲解第一行的几个符号含义，第二行类似。$f$表示神经网络运算，可以理解为网络拟合函数；$h_{t, c}^{j}$表示第$c$层第$i$个 detection 的状态嵌入。在一开始，$c=0, h_{d, 0}^{i}=f_{d}^{i}, h_{t, 0}^{i}=f_{t}^{j}$，$e_{d, c}^{i, j}$则表示第$i$个检测和第$j$个 tracklet 在第$c$层的图上的边权。本文作者只使用添加自适应的单层 GNN，所以下面具体阐述单层学习的情况。</p><p>$$<br>\begin{aligned}<br>h_{d, c+1}^{i} &amp;=f\left(h_{d, c}^{i},\left{h_{t, c}^{j}, e_{d, c}^{i, j}\right}<em>{j=1}^{N}\right), i=1,2, \cdots, M \<br>h</em>{t, c+1}^{j} &amp;=f\left(h_{t, c}^{j},\left{h_{d, c}^{i}, e_{t, c}^{j, i}\right}_{i=1}^{M}\right) j=1,2, \cdots, N<br>\end{aligned}<br>$$</p><p>首先，边权的初始化不采用随机初始化，而是采用节点的特征和位置先验信息，具体如下，主要是计算每个节点特征向量之间的归一化距离相似度。具体图信息聚合步骤如下。<br></p><ol><li>计算初始相似度<br>$$<br>\begin{array}{c}<br>s_{i, j}=\frac{1}{\left|f_{d}^{i}-f_{t}^{j}\right|<em>{2}+1 \times 10^{-16}} \<br>s</em>{i, j}=\frac{s_{i, j}}{\sqrt{s_{i, 1}^{2}+s_{i, 2}^{2}+\cdots s_{i, j}^{2}+\cdots+s_{i, N}^{2}}}\<br>\mathbf{S}<em>{\mathrm{ft}}=\left[s</em>{i, j}\right]_{M \times N}, i=1, \cdots M, j=1, \cdots N<br>\end{array}<br>$$</li><li>通过 IOU 和上面的初始相似度组成边权（$w$可学习，表示位置和外观信息的相对重要性）</li></ol><p>$$<br>\mathrm{E}=w \times \mathrm{IOU}+(1-w) \times \mathrm{S}_{\mathrm{ft}}<br>$$</p><ol start="3"><li>根据上述的自适应权重聚合节点特征（$\odot$表示点积）</li></ol><p>$$<br>\mathbf{F}<em>{\mathrm{t}}^{\mathrm{ag}}=\mathrm{EF}</em>{t}=\mathrm{E}\left[f_{t}^{1}, f_{t}^{2}, \cdots, f_{t}^{N}\right]^{T}<br>$$</p><p>$$<br>\mathbf{H}<em>{\mathrm{d}}=\sigma\left(\mathbf{F}</em>{d} W_{1}+\operatorname{Sigmoid}\left(\mathbf{F}<em>{d} W</em>{a}\right) \odot \mathbf{F}<em>{\mathrm{t}}^{\mathbf{a g}} W</em>{2}\right)<br>$$</p><p>$$<br>\mathbf{H}<em>{\mathrm{t}}=\sigma\left(\mathbf{F}</em>{t} W_{1}+\operatorname{Sigmoid}\left(\mathbf{F}<em>{t} W</em>{a}\right) \odot \mathbf{F}<em>{\mathrm{d}}^{\mathbf{a g}} W</em>{2}\right)<br>$$</p><p>现有的图跟踪方法需要额外的全连接层降维特征向量，然后通过欧式距离计算相似度。TPAGT 的方法只要标准化来自单隐层图网络的特征，然后矩乘它们即可得到相似度决战，如下式。最终得到的相似度矩阵值介于 0 和 1 之间，越大代表两个目标越相似。学习的目的是使得同一个目标的特征向量尽量接近，不同目标的特征向量尽量垂直，这等价于三元组损失，但是更加简单。</p><p>$h_{d}^{i}=\frac{h_{d}^{i}}{\left|h_{d}^{i}\right|<em>{2}}, h</em>{t}^{j}=\frac{h_{t}^{j}}{\left|h_{t}^{j}\right|<em>{2}}, \mathbf{S}</em>{\mathrm{out}=\mathbf{H}<em>{\mathrm{d}} \mathbf{H}</em>{\mathbf{t}}^{\mathrm{T}}}$</p><h3 id="Blanced-MSE-Loss"><a href="#Blanced-MSE-Loss" class="headerlink" title="Blanced MSE Loss"></a><strong>Blanced MSE Loss</strong></h3><p>得到最终的相似度矩阵就可以进行监督训练了，不过 GT 的标签为相同目标为 1，不同的目标为 0，下图是作者做的可视化，每行代表一个 detection，每列代表一个 tracklet，绿行表示 detection 没有匹配上任何 tracklet，所以是新目标；相对的，红列表示消失的目标。1 表示正例，0 表示负例，显然正负例严重不均衡，所以这里对 MSE 按照目标类型进行了加权（超参），如下式。</p><p><img src="https://i.loli.net/2020/12/04/NGSUCpjo8mQa7xl.png"></p><p>$$<br>\begin{aligned}<br>\mathcal{L} &amp;=\alpha E_{c 0}+\beta E_{c 1}+\gamma E_{n e}+\delta E_{d}+\varepsilon E_{w} \<br>&amp;=\sum_{i=1}^{M} \sum_{j=1}^{N}\left[\begin{array}{c}<br>\alpha\left(\hat{S}<em>{i, j}-S</em>{i, j}\right)^{2} \cdot \mathbb{I}<em>{\text {continue }} \cdot \mathbb{I}</em>{S_{i, j}=0}+\beta\left(\hat{S}<em>{i, j}-S</em>{i, j}\right)^{2} \cdot \mathbb{I}<em>{\text {continue }} \cdot \mathbb{I}</em>{S_{i, j}=1} \<br>+\gamma\left(\hat{S}<em>{i, j}-S</em>{i, j}\right)^{2} \cdot \mathbb{I}<em>{n e w}+\delta\left(\hat{S}</em>{i, j}-S_{i, j}\right)^{2} \cdot \mathbb{I}<em>{\text {disap }}+\varepsilon|W|</em>{2}^{2}<br>\end{array}\right]<br>\end{aligned}<br>$$</p><h2 id="推理设计"><a href="#推理设计" class="headerlink" title="推理设计"></a>推理设计</h2><p>推理时，我们会得到相似度矩阵，那么如何利用这个矩阵呢？假设有$N$个 detection 和$M$个 tracklet，矩阵就是$M\times N$的，此时在后面补充一个$M\times M$的增广矩阵，矩阵中每个值都是一个阈值，如下图，匈牙利算法就成了带筛选的匹配方法，下图由于第 3 行和第 8 行没有高于阈值（0.2）的相似度，所以成为了新目标。</p><p><img src="https://i.loli.net/2020/12/04/ZQHhlGDUcg2o9XC.png"></p><h2 id="实验及分析"><a href="#实验及分析" class="headerlink" title="实验及分析"></a>实验及分析</h2><p>检测部分采用 FairMOT 的检测结果，也就是采用 CenterNet 作为检测器。特征提取部分，文章使用 ResNet101-FPN 作为 backbone，在 COCO 上预训练过，然后在 MOT 数据集上 fine tune 30 轮。其他训练细节可以自行查阅论文，我这里就不多说了，在 Public 和 private 两个赛道进行了测试，结果分别如下，超越了之前的 SOTA 方法如 FairMOT 等，精度突破很大，速度比较慢。</p><p><img src="https://i.loli.net/2020/12/04/cE86gYS5t3oKBDj.png"></p><p><img src="https://i.loli.net/2020/12/04/QbR6ngGePwxSYWh.png"></p><p>此外，作者还进行了丰富的消融实验，证明了 TPAGT 的鲁棒性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>开创性地提出了特征重提取策略，并引入 AGNN 进行特征融合，从而构建了 TPAGT 框架，这是一个端到端的学习框架，可以直接输出相似度矩阵。在 MOT Challenge 两个赛道都获得了 SOTA 表现。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Shan C, Wei C, Deng B, et al. Tracklets Predicting Based Adaptive Graph Tracking[J]. arXiv:2010.09015 [cs], 2020.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TPAGT解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-CNN系列解读</title>
      <link href="/2020/11/30/r-cnns/"/>
      <url>/2020/11/30/r-cnns/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>最近香港大学和加州伯克利开放了一篇新的基于 R-CNN 的工作 Sparse R-CNN，这是一个类似于 DETR 的稀疏端到端目标检测框架，使用少量可学习的 proposal 即可达到 SOTA 性能。这为稀疏端到端目标检测开辟了一条新的路，也将更多研究者的目标吸引了过来。本文从 R-CNN 开始，逐步讲解 R-CNN 系列目标检测算法的发展优化之路。，它们都是沿着 region proposal 这个思路的，只是处理方式大不相同。下面的图是我非常喜欢的目标检测综述《Object detection in 20 years: A survey》中归纳的目标检测里程碑式作品，可以看到，R-CNN 系的三个算法是目标检测发展史上避不开的话题。</p><ul><li><a href="https://arxiv.org/abs/1311.2524">R-CNN</a>（CVPR2014）</li><li><a href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a> （ICCV2015）</li><li><a href="https://arxiv.org/abs/1506.01497">Faster R-CNN</a>（NIPS2015）</li><li><a href="http://arxiv.org/abs/2011.12450">Sparse R-CNN</a>（暂未收录）</li></ul><p><img src="https://i.loli.net/2020/11/30/I6oZrcKxTzlfQ8q.png"></p><h2 id="目标检测思路"><a href="#目标检测思路" class="headerlink" title="目标检测思路"></a>目标检测思路</h2><p>在聊具体的算法之前，我们首先要知道目标检测永不过时的核心思路：<strong>目标定位</strong>+<strong>目标分类</strong>，这是任务本身决定的，后来所谓的 two-stage 方法和 one-stage 方法其实都还是这个思路，只是处理的技巧不同罢了。</p><p>我们知道，<strong>目标分类</strong>这个任务已经基本上被 CNN 所解决，所以只要在 pipeline 中引入卷积分类模型就能确保不错的分类精度。留给目标检测的核心问题其实就是<strong>目标定位</strong>。我们当然会想到很直接很粗暴的想法：遍历图片中所有可能的位置，搜索不同大小、宽高比的所有区域，逐个检测其中是否存在某个目标，以概率较大的结果作为输出。这个就是传统的滑窗方法，这个方法显然是一种密集采样的方式，类似后来的 anchor 策略，R-CNN 系列则采用了一种候选框提名的方式减少滑窗的复杂性形成了 Dense-to-Sparse 方法，Sparse R-CNN 则完全采用 Sparse 策略，获得了低维的 proposal 输入。</p><p><img src="https://i.loli.net/2020/11/30/cQYRqUzH1SvgA5I.png"></p><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><strong>R-CNN</strong></h2><p>R-CNN 是 2014 年出现的一篇目标检测方法，为后来目标检测的研究奠定了基础，R-CNN 结合了传统的滑窗法和边框回归思路，开创性地提出了候选区的概念（Region proposals），先从输入图像中找到一些可能存在对象的候选区，这个过程称为 Selective Search（根据输入图像的特征如颜色等提取出候选区）。在 R-CNN 中这些候选区大概会有 2000 个，对这些区域进行分类和微调即可完成目标检测。候选区的提出大大减少了目标定位的时间，提高了目标检测的效率。然而由于深度学习分类器的存在，非常耗费算力，且 proposal 的生成都是在 CPU 上完成的，因此 R-CNN 处理一张图片大概需要 49 秒，这和实时检测差的还很远。</p><p><img src="https://i.loli.net/2020/11/30/c4A15kyVlfmbi9v.png"></p><p>下面我们来理解一下 R-CNN 的具体训练流程，它的整体思路如上图，具体细节如下。</p><ul><li><strong>Region proposals</strong>：使用 Selective Search 方法生成大约 2000 个候选框（proposals），与 ground truth box（以下简称 GT box）的 IOU（交并比）大于 0.5 则认为这个 proposal 为值得学习的 positive（正例），否则为 negative（负例，即 background）。</li><li><strong>Supervised pre-training and Domain-specific fine-tuning</strong>：在 ImageNet 上预训练 VGG16 这样的视觉深度模型用于提取图像特征，然后在 VOC 数据集上 fine tune 为 21 类（包括背景），这样就得到了一个优质的特征提取器，只要将 proposal 区域 resize 到这个卷积模型的需要尺寸就能提取特征，提取的特征用于目标的分类和边框回归。</li><li><strong>Object category classifiers and Bounding box regression</strong>：分类器使用 SVM 进行分类。分类完成后，边框回归模型用于精调边框的位置，论文对每一类目标使用了简单的线性回归模型。具体训练的细节如优化器等配置这里就不细细展开了。</li></ul><p>测试时（推理时）思路类似，先对图像产生大量的 proposal，然后进行分类和边框回归，然后使用 NMS 算法（非极大值抑制）去除冗余的边框得到最终的检测结果。</p><p><strong>至此，我们理解了 R-CNN 的思路，尽管它不像传统方法那样穷举可能的目标位置，然而 Selective Search 方法提取的候选框多达 2000 个，这 2000 个框都需要 resize 后送入 CNN 和 SVM 中，这就是说，对一幅图像的检测需要对 2000 个图像进行 CNN 特征提取，这在这个算力爆炸的时代都是很费时的，何况在当时，这就导致 R-CNN 平均处理一张图片需要 49 秒。</strong></p><p>那么，有没有办法提高检测的速度呢？其实很容易发现，这 2000 个 proposal 都是图像的一部分，完全可以对图像提取一次特征，然后只需要将 proposal 在原图的位置映射到特征图上，就得到该 proposal 区域的特征了。这样，对一幅图像只需要一次得到卷积特征，然后将每个 proposal 特征送入全连接进行后续任务的端到端学习。</p><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><strong>Fast R-CNN</strong></h2><p>在上面一节的最后，得到一个优化 R-CNN 的思路，SPPNet 沿着这个思路走了下去，其中最关键的一个问题就是 crop/warp 操作使得 proposal 尺寸固定送入了 CNN 中，这是为了全连接层固定输入的要求，但是导致了图像的失真，因此作者设计了金字塔池化来使得任意输入的图像获得同维的特征。有了这个方法，就能对 R-CNN 进行优化了，只对原图进行一次卷积计算，便得到整张图的卷积特征 feature map，然后找到每个候选框在 feature map 上的映射 patch，将此 patch 作为每个候选框的卷积特征输入到 SPP layer（空间金字塔池化层） 和之后的层，完成特征提取工作。如此，对一幅图，SPPNet 只需要提取一次卷积特征，提速 R-CNN 百倍左右。</p><p>Fast R-CNN 吸取了 SPPNet 的思路，对 R-CNN 进行了改进，获得了进一步的性能提升。它采用了单层空间金字塔池化（文中为 RoI Pooling）代替原来的池化操作使得不同的 proposal 在最后一层 feature map 上映射得到的区域上经过池化后得到同维特征。原本的 proposal 位置经过下采样倍率除法就可以得到其在特征图上的位置从而获得特征，这个不难理解，遇到除不尽的就取整即可。<strong>问题是，RoI Pooling 是如何保证任意尺度的特征图都能得到同维的输出特征呢 2？RoI Pooling 采用了一个简单的策略，那就是网格划分。</strong></p><p>假设我有个$w=7,h=5$的 RoI 特征矩阵，我需要得到$W=2,H=2$的输出特征，那么其实只需要将$7\times 5$的矩阵划分为四个部分，每个部分取最大元素值就行，不过，由于非方阵，，难以等分，所以 RoI Pooling 会先进行取整，这样$w$被划分为 3 和 4，$h$被划分为 2 和 3。这样就完成了 RoI 区域的固定维度特征提取，有意思的是，<strong>上述过程其实发生了两次取整，一次计算 proposal 在 feature map 上的坐标，一次计算 feature map 上的 RoI 区域的网格划分，这个过程是有问题的，所以后来提出了 RoI Align。</strong></p><p><img src="https://i.loli.net/2020/11/30/wVenWRkUbPrCfY3.png"></p><p>上图所示的就是 Fast R-CNN 的 pipeline 设计，现在一次卷积特征提取就能得到 2000 个 proposal 的特征了，后续的任务作者也做了改变。彼时，<strong>多任务学习</strong>已经初有起色，原先在 R-CNN 中是先提 proposal，然后 CNN 提取特征，之后用 SVM 分类器，最后再做 bbox regression 的，在 Fast R-CNN 中则将对 proposal 的分类和边框回归同时通过全连接层完成，利用一个上图所示的多任务学习网络。实验也证明，这两个任务能够共享卷积特征，并相互促进。</p><p>其他方面，和 R-CNN 处理思路类似，其实，除了 proposal 的生成，整个 Fast R-CNN 已经是端到端训练的了，Fast-RCNN 很重要的一个贡献是为 Region Proposal + CNN 这种实时检测框架提供了一丝可能性，这也为后来的 Faster R-CNN 埋下伏笔。</p><p>最后提一下，Fast R-CNN 单图平均处理时间缩短到了 2.3 秒，这是一个巨大的突破，不过，对实时检测而言还是有点慢，而下一节的 Faster R-CNN 则将实时检测的可能性化为了现实。</p><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><strong>Faster R-CNN</strong></h2><p>在 Fast R-CNN 中留下了一个遗憾，也是制约网络速度和端到端训练的部件，那就是 Selective Search 求得候选框，这个方法其实非常耗时，Faster R-CNN 就设计了一个 Region Proposal Network（RPN，区域推荐网络）代替 Selective Search，同时也引入了 anchor box 应对目标形状的变化问题（anchor 可以理解为位置和大小固定的 bbox，或者说事先设置好的固定的 proposal）。</p><p><img src="https://i.loli.net/2020/11/30/LUgclEvd8bXsipn.png"></p><p>具体而言，就像上图一样，RPN 网络会在最后一层特征图上对大量的预定义的 anchor 进行正例挑选（这是个分类任务）以及边界框回归（这是个回归任务），通过这两个任务会得到较为准确的 proposal，这就完成了 Fast R-CNN 中 Selective Search 得到的 proposal 了。接着，这些 proposal 会被用于和 Fast R-CNN 中一样的目标分类和边框回归来得到精确的检测结果。</p><p><img src="https://i.loli.net/2020/11/30/KWs7YZC2EbJtmFv.png"></p><p>RPN 的引入使得端到端目标检测框架得以实现，这大大提高了目标检测的速度，它单图检测时间只需要 0.2 秒。直到今天，Faster R-CNN 这种基于候选框的二阶段目标检测框架仍然是目标检测的一个重要研究领域，Faster R-CNN 也成为目标检测领域绕不过的里程碑。</p><h2 id="Sparse-R-CNN"><a href="#Sparse-R-CNN" class="headerlink" title="Sparse R-CNN"></a><strong>Sparse R-CNN</strong></h2><p>距离 Faster R-CNN 的出现已经过去了 5 年的时间，这 5 年是目标检测高速发展的 5 年，在 Faster R-CNN 之后出现了 anchor-based 的单阶段目标跟踪范式，它主张去掉 RPN 直接对 anchor 进行处理，在速度上获得了卓越的表现，如 SSD、RetinaNet 等，它们成为了目标检测的新 SOTA。再后来，受到工业界普遍关注的高效<a href="https://zhouchen.blog.csdn.net/article/details/108032597">anchor-free 方法</a>为目标检测开拓了全新的方向，如 FCOS、CenterNet 等。今年，FaceBook 发布了使用 Transformer 构建的 DETR 框架，它只需要少量的低维输入即可获得 SOTA 检测表现，这为稀疏检测带来了一丝契机。</p><p>最近，沿着目标检测中 Dense 和 Dense-to-Sparse 的框架的设计思路，Sparse R-CNN 建立了一种彻底的稀疏框架，它完全脱离了 anchor、RPN 和 NMS 后处理等设计。</p><p>首先，我们来回顾一下目标检测的范式。最著名的为<strong>Dense范式</strong>，密集检测器的思路从深度学习时代之前沿用至今，这种范式最典型的特征是会设置大量的候选（candidates），无论是anchor、参考点还是其他的形式，最后，网络会对这些候选进行过滤以及分类和边框微调。接着，取得过突出成果的<strong>Dense-to-Sparse范式</strong>最典型的代表就是上文所说的R-CNN系列，这种范式的典型特点是对一组较为稀疏的候选进行分类和回归，这组稀疏的候选其实还是来自密集检测中那种。</p><p><img src="https://i.loli.net/2020/11/30/cQYRqUzH1SvgA5I.png"></p><p>很多人认为目标检测已经是个几乎solved problem，然而Dense范式总有一些问题难以忽略：较慢的NMS后处理、多对一的正负样本分类、candidates的设计等。这些问题可以通过Sparse范式解决，DETR提出了一个解决方案：DETR中，candidates是一组sparse的可学习的object queries，正负样本分配是一对一的二分图匹配，而且，不需要NMS就能获得检测结果。然而，DETR使用的Transformer无形之中使得每个object query与全局特征图进行了注意力交互，这还是Dense的交互。</p><p>Sparse R-CNN作者认为，Sparse的检测框架不仅仅是稀疏的候选，也应该是稀疏的特征交互，因此提出了Sparse R-CNN。</p><p><img src="https://i.loli.net/2020/11/30/bRAIPkMeNhSv7L9.png"></p><p>Sparse R-CNN的出发点是通过少量的（如100个）proposal代替RPN产生的动辄上千的proposal。不妨看一下上面的pipeline设计，Sparse R-CNN的proposal是一组可学习参数，就是上面绿色框中的$N*4$向量，其中$N$表示proposal的数目，一般几百个就够了，4代表物体边框的4个属性，为归一化后的中心点坐标$(x,y)$以及宽高（$w$和$h$），这个向量作为可学习参数和其他参数一起被网络优化，这就是整体的设计。</p><p>但是，如果这也就可以了那么前人早就实现了，这个学习到的proposal可以理解为什么呢？其实就是根据图像信息推理得到的可能出现物体的统计值，这种“肤浅”的proposal确定的RoI特征显然不足以用来精确定位和恶分类目标，事实上作者在Faster R-CNN基础上换成这种可学习的proposal后精度下降了20个点。于是，，作者提出了上图蓝色框中的特征层面的proposal，它和proposal box数目一致，是一个高维特征向量（256维左右）。<strong>proposal box和proposal feature一一对应，proposal feature和proposal box提取出来的RoI feature做一对一的交互，使得RoI特征更利于分类和回归。这种交互如下图所示，类似于注意力机制，这种交互模块可以多个拼接以进一步精炼特征，所以该模块为Dynamic Instance Interactive Module，堆叠多个这样的模块构成了该框架的Dynamic Instance Interactive Head。</strong> 由于交互是一对一的，所以特征的交互和proposal一样是稀疏的。</p><p><img src="https://i.loli.net/2020/11/30/HopeQ7qyCt5wNul.png"></p><p>关于整体框架：backbone采用基于ResNet的FPN，Head是一组Dynamic Instance Interactive Head，上一个head的output features和output boxes作为下一个head的proposal features和proposal boxes。proposal features在与RoI features交互之前做self-attention。整体训练的损失函数是基于二分图匹配的集合预测损失。</p><p><img src="https://i.loli.net/2020/11/30/n7b5JhgqB9PLIjy.png"></p><p>实验结果上看，在COCO数据集达到了SOTA表现。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>R-CNN和Fast R-CNN出现后的一段时期内，目标检测领域的一个重要研究方向是提出更高效的候选框，其中Faster R-CNN开创性提出RPN，产生深远影响。Sparse R-CNN以一组稀疏输入即可获得比肩SOTA的检测性能，为真正的端到端检测开拓了一条路。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R-CNN系列解读 </tag>
            
            <tag> R-CNN解读 </tag>
            
            <tag> Fast R-CNN解读 </tag>
            
            <tag> Faster R-CNN解读 </tag>
            
            <tag> Sparse R-CNN解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientDet 解读</title>
      <link href="/2020/11/22/efficientdet/"/>
      <url>/2020/11/22/efficientdet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇发表于 CVPR2020 的检测论文不同于大火的 anchor-free，还是基于 one-stage 的范式做的设计，是 ICML2019 的 EfficientNet 的拓展，将分类模型引入到了目标检测任务中。近些年目标检测发展迅猛，精度提升的同时也使得模型越来越大、算力需求越来越高，这制约了算法的落地。近些年出现了很多高效的目标检测思路，如 one-stage、anchor-free 以及模型压缩策略，它们基本上都是以牺牲精度为代价获得效率的。EfficientDet 直指当前目标检测的痛点：有没有可能在大量的资源约束前提下，实现高效且高精度的目标检测框架？<strong>这就是 EfficientDet 的由来。</strong></p><ul><li><p>论文标题</p><p>EfficientDet: Scalable and Efficient Object Detection</p></li><li><p>论文地址</p><p><a href="http://arxiv.org/abs/1911.09070">http://arxiv.org/abs/1911.09070</a></p></li><li><p>论文源码</p><p><a href="https://github.com/google/automl/tree/master/efficientdet">https://github.com/google/automl/tree/master/efficientdet</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>之前提到，EfficientDet 是 EfficientNet 的拓展，我们首先来简单聊一聊 EfficientNet，感兴趣的请阅读<a href="https://arxiv.org/abs/1905.11946">原文</a>。在 EfficientNet 中提到了一个很重要的概念 Compound Scaling（符合缩放），这是基于一个公认的事实：调整模型的深度、宽度以及输入的分辨率在一定范围内会对模型性能有影响，但是过大的深度、宽度和分辨率对性能改善不大还会严重影响模型前向效率，所以 EfficientNet 提出复合系数$\phi$统一缩放网络的宽度、深度和分辨率，具体如下。</p><p><img src="https://i.loli.net/2020/11/22/rQsZnAzVWE986He.png"></p><p>这里的$\alpha, \beta, \gamma$都是由一个很小范围的网络搜索得到的常量，直观上来讲，$\phi$是一个特定的系数，可以用来控制资源的使用量，$\alpha, \beta, \gamma$决定了具体是如何分配资源的。值得注意的是，常规卷积操作的计算量是和$d, w^{2}, r^{2}$成正比的，加倍深度会使得 FLOPS 加倍，但是加倍宽度和分辨率会使得 FLOPS 加 4 倍。由于卷积 ops 经常在 CNN 中占据了大部分计算量，使用等式上式缩放卷积网络将会使得整体计算量近似增加$\left(\alpha \cdot \beta^{2} \cdot \gamma^{2}\right)^{\phi}$倍。由于 EfficientNet 对任意$\phi$增加了约束$\alpha \cdot \beta^{2} \cdot \gamma^{2} \approx 2$，整体的计算量近似增加了$2^{\phi}$倍。</p><p><strong>对比 EfficientNet 从 B0 到 B7 的提升，不难知道，这种复合缩放可以较大的提升模型性能，所以 EfficientDet 也将其引入了进来。</strong></p><p>论文首先分析了目前 OD（object detection，目标检测）的两个挑战：<strong>高效多尺度特征融合</strong>和<strong>模型缩放</strong>。</p><p><strong>多尺度特征融合</strong>：FPN 如今被广泛用于多尺度特征融合，最近 PANet、NAS-FPN 等方法研究了更多跨尺度特征融合的结构。不过，这些方法融合不同尺度特征的方式都是简单加和，这默认了不同尺度特征的贡献是同等的，然而往往不是这样的。为了解决这个问题，论文提出了一种简单但是高效的加权双向特征金字塔网络（<strong>BiFPN</strong>），它对不同的输入特征学习权重。</p><p><strong>模型缩放</strong>：之前的方法依赖于更大的 backbone 或者更大分辨率的输入，论文发现放大特征网络和预测网络对精度和速度的考量是很重要的。基于 EfficientNet 的基础，论文为目标检测设计了一种复合缩放（<strong>Compound Scaling</strong>）方法，联合调整 backbone、特征网络、预测网络的深度、宽度和分辨率。</p><p>和 EfficientNet 一样，EfficientDet 指的是一系列网络，如下图包含 D1 到 D7，速度逐渐变慢，精度逐渐提升。在理解 EfficientDet 两个核心工作（<strong>BiFPN</strong>和<strong>Compound Scaling</strong>）之前，可以先看看下图的 SOTA 方法比较，可以看到 EfficientDet-D7 的效果非常惊人，在 FLOPs 仅为 AmoebaNet+NAS-FPN+AA 的十分之一的前提下，COCO2017 验证集上的 AP 到达了 55.1，超越了 SOTA5 个点。而且单尺度训练的 EfficientD7 现在依然霸榜 PaperWithCode 上。</p><p><img src="https://i.loli.net/2020/11/22/NU35fMiRe6Fo2BT.png"></p><p><img src="https://i.loli.net/2020/11/22/LI5h3EBkVtWQzjO.png"></p><p>此外，查看官方仓库提供的模型，其参数量其实是不大的（当然，这不绝对意味着计算量小）。</p><p><img src="https://i.loli.net/2020/11/22/giL6hNfOYMVA2I1.png"></p><h2 id="BiFPN"><a href="#BiFPN" class="headerlink" title="BiFPN"></a>BiFPN</h2><p>CVPR2017 的 FPN 指出了不同层之间特征融合的重要性如下图 a，不过它采用的是自上而下的特征图融合，融合方式也是很简单的高层特征加倍后和低层特征相加的方式。此后，下图 b 所示的 PANet 在 FPN 的基础上又添加了自下而上的信息流。再后来出现了不少其他的融合方式，直到 NAS-FPN 采用了 NAS 策略搜索最佳 FPN 结构，得到的是下图 c 的版本，不过 NAS-FPN 虽然简单高效，但是精度和 PANet 还是有所差距的，并且 NAS-FPN 这种结构是很怪异的，难以理解的。所以，EfficientDet 在 PANet 的基础上进行了优化如下图的 d：移除只有一个输入的节点；同一个 level 的输入和输出节点进行连接，类似 skip connection；PANet 这种一次自上而下再自下而上的特征融合可以作为一个单元重复多次从而获得更加丰富的特征，不过重复多少次是速度和精度的权衡选择，这在后面的复合 22 缩放部分讲到。</p><p><img src="https://i.loli.net/2020/11/22/iuAG38EdxKI9DqF.png"></p><p>上述是 FPN 特征流动的结构，如何数学上组合这些特征也是一个重要的方面。此前的方法都是上一层特征 resize 之后和当前层特征相加。这种方式存在诸多不合理之处，因为这样其实默认融合的两层特征是同权重的，事实上不同尺度的特征对输出特征的贡献是不平等的，应当对每个输入特征加权，这个权重需要网络自己学习。当然，学习到的权重需要归一化到和为 1，采用 softmax 是一个选择，但是 softmax 指数运算开销大，所以作者这里简化为快速标准化融合的方式（Fast normalized fusion），它的计算方法如下，其实就是去掉了 softmax 的指数运算，这种方式在 GPU 上快了很多，精度略微下降，可以接受。</p><p>$$<br>O=\sum_{i} \frac{w_{i}}{\epsilon+\sum_{j} w_{j}} \cdot I_{i}<br>$$</p><h2 id="Compound-Scaling"><a href="#Compound-Scaling" class="headerlink" title="Compound Scaling"></a>Compound Scaling</h2><p>在看复合缩放之前，我们先要知道，有了 BiFPN 又有了 EfficientNet 再加上 head 部分，其实网络框架已经确定了，如下图所示，左边是 backbone（EfficientDet），中间是多层 BiFPN，右边是 prediction head 部分。</p><p><img src="https://i.loli.net/2020/11/22/P28ZjJ4sgwYSXGh.png"></p><p>结合 EfficientNet 的联合调整策略，论文提出目标检测的联合调整策略，用复合系数$\phi$统一调整.调整的内容包括 backbone（EfficientNet 版本，B0 到 B6）、neck 部分的 BiFPN（通道数、layer 数）以及 head 部分（包括层数）还要输入图像分辨率。不过，和 EfficientNet 不同，由于参数太多采用网格搜索计算量很大，论文采用启发式的调整策略。其中 backbone 的选择系数控制，BiFPN 的配置用下面第一个式子计算，head 的层数和输入分辨率是下面 2、3 式的计算方式。</p><p>$$<br>W_{b i f p n}=64 \cdot\left(1.35^{\phi}\right), \quad D_{b i f p n}=3+\phi<br>$$</p><p>$$<br>D_{b o x}=D_{c l a s s}=3+\lfloor\phi / 3\rfloor<br>$$</p><p>$$<br>R_{\text {input}}=512+\phi \cdot 128<br>$$</p><p>最后得到的 8 种结构的配置表如下图。</p><p><img src="https://i.loli.net/2020/11/22/fw7sLOZj8KNtUm2.png"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在简介里我已经提到很多这个检测框架的过人之处了，这里就简单看一下在 COCO 验证集的效果，可以说无论是速度还是精度都是吊打其他 SOTA 方法的，至今依然在 COCO 验证集榜首的位置。</p><p><img src="https://i.loli.net/2020/11/22/KUOb2RJcBCQt5fu.png"></p><p>此外作者也将其拓展到语义分割，潜力也是比较大的。还做了不少消融实验，感兴趣的可以自行查看论文原文。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文最大的亮点在于提出了目标检测网络联合调整复杂度的策略，从而刷新了 SOTA 结果。这个思路来自 EfficientDet，同样 backbone 的高效也源自该网络。文中另一个突出的成果在于设计了 BiFPN 以及堆叠它，可以看到效果还是很显著的。此外，除了官方的 TF 实现外，这里也推荐一个目前公认最好的 PyTorch 实现（由国内大佬完成），<a href="https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch">Github 地址</a>给出，这也是唯一一个达到论文效果的 PyTorch 复现（作者复现时官方还没有开源）。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Tan M, Pang R, Le Q V. EfficientDet: Scalable and Efficient Object Detection[J]. arXiv:1911.09070 [cs, eess], 2020.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EfficientDet解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSTrack 解读</title>
      <link href="/2020/11/20/cstrack/"/>
      <url>/2020/11/20/cstrack/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>自从 FairMOT 的公开以来，MOT 似乎进入了一个高速发展阶段，先是 CenterTrack 紧随其后发布并开源 ，然后是后来的 RetinaTrack、MAT、FGAGT 等 SOTA 方法出现，它们不断刷新着 MOT Challenge 的榜单。最近，CSTrack 这篇文章则在 JDE 范式的基础上进行了改进，获得了相当不错的跟踪表现（基本上暂时稳定在榜单前 5），本文就简单解读一下这篇短文（目前在 Arxiv 上开放的是一个 4 页短文的版本）。</p><ul><li><p>论文标题</p><p>Rethinking the competition between detection and ReID in Multi-Object Tracking</p></li><li><p>论文地址</p><p><a href="http://arxiv.org/abs/2010.12138">http://arxiv.org/abs/2010.12138</a></p></li><li><p>论文源码</p><p><a href="https://github.com/JudasDie/SOTS">https://github.com/JudasDie/SOTS</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>为了追求速度和精度的平衡，联合训练检测模型和 ReID 模型的 JDE 范式（如下图，具体提出参考 JDE 原论文 Towards Real-Time Multi-Object Tracking）受到了学术界和工业界越来越多的关注。这主要是针对之前的 two-stage 方法先是使用现有的检测器检测出行人然后再根据检测框提取对应行人的外观特征进行关联的思路，这种方法在精度上的表现不错，然而由于检测模型不小且 ReID 模型需要在每个检测框上进行推理，计算量非常之大，所以 JDE 这种 one-shot 的思路的诞生是一种必然。</p><p><img src="https://i.loli.net/2020/11/20/XSvleZbIjwDo1Yd.png"></p><p>然而，就像之前 FairMOT 分析的那样，检测和 ReID 模型是<strong>存在不公平的过度竞争</strong>的，这种竞争制约了两个任务（检测任务和 ReID 任务 ）的表示学习，导致 了学习的混淆。具体而言，检测任务需要的是同类的不同目标拥有相似的语义信息（类间距离最大），而 ReID 要求的是同类目标有不同的语义信息（类内距离最大）。此外，<strong>目标较大的尺度变化</strong>依然是 MOT 的痛点。在 ReID 中图像被调整到统一的固定尺寸来进行查询 ，而在 MOT 中，提供在 ReID 网络的特征需要拥有尺度感知能力，这是因为沿着帧目标可能会有巨大的 size 变化。</p><p>为了解决上述的过度竞争问题，论文提出了一种新的互相关网络（CCN）来改进单阶段跟踪框架下 detection 和 ReID 任务之间的协作学习。作者首先将 detection 和 ReID 解耦为两个分支，分别学习。然后两个任务的特征通过自注意力方式获得自注意力权重图和互相关性权重图。自注意力图是促进各自任务的学习，互相关图是为了提高两个任务的协同学习。而且，为了解决上述的尺度问题，设计了尺度感知注意力网络（SAAN）用于 ReID 特征的进一步优化，SAAN 使用了空间和通道注意力，该网络能够获得目标 不同尺度的外观信息，最后 不同尺度外观特征融合输出即可。</p><h2 id="框架设计"><a href="#框架设计" class="headerlink" title="框架设计"></a>框架设计</h2><p>整体的思路还是采用 JDE 的框架，下图的右图是整体 pipeline 的设计，和左侧的 JDE 相比，中间增加了一个 CCN 网络（互相相关网络）用于构建 detection 和 ReID 两个分支不同的特征图（这里解释一下为什么要解耦成两个特征图送入后续的两个任务中，其实原始的 JDE 就是一个特征图送入检测和 ReID 分支中，作者这边认为这会造成后续的混淆，所以在这里分开了，FairMOT 也发现了这个问题，只是用另一种思路解决的而已）。构建的两个特征图分别送入 Detection head 和 SAAN（多尺度+注意力+ReID）中，Detection head 将 JDE 的 YOLO3 换为了更快更准的 YOLO5，其他没什么变动，下文检测这边我就不多提了。检测完成的同时，SAAN 也输出了多尺度融合后的 ReID 特征，至此也就完成了 JDE 联合检测和 ReID 的任务，后续就是关联问题了。</p><p><img src="https://i.loli.net/2020/11/20/KIpS3jV1ezicCk5.png"></p><p>所以，从整个框架来看，CSTrack 对 JDE 的改动主要集中在上图的 CCN 和 SAAN，而这两部分都是在特征优化上做了文章，且主要是基于注意力手段的特征优化（不知道是好是坏呢）。</p><h2 id="CCN"><a href="#CCN" class="headerlink" title="CCN"></a>CCN</h2><p>CCN（Cross-correlation Network）用于提取更适合 detection 和 ReID 任务的一般特征和特定特征。在特定性学习方面，通过学习反映不同特征通道之间相互关系的自联系，增强了每个任务的特征表示。对于一般性学习，可以通过精心设计的相互关系机制来学习两个任务之间的共享信息。</p><p><img src="https://i.loli.net/2020/11/20/XoOUnxREIqfCy3K.png"></p><p>CCN 的结构如上图，我们对着这个图来理解 CCN 的思路。从检测器的 backbone 得到的特征图为$\mathbf{F} \in R^{C \times H \times W}$，首先，这个特征经过平均池化降维获得统计信息（更精炼的特征图）$\mathbf{F}^{\prime} \in R^{C \times H^{\prime} \times W^{\prime}}$。然后，两个不同的卷积层作用于$\mathbf{F}^{\prime}$生成两个特征图$\mathbf{T_1}$和$\mathbf{T_2}$，这两个特征图被 reshape 为特征$\left{\mathbf{M}<em>{\mathbf{1}}, \mathbf{M}</em>{\mathbf{2}}\right} \in R^{C \times N^{\prime}}$（$N^{\prime}=H^{\prime} \times W^{\prime}$）。下面的上下两个分支操作是一致的，先用矩阵$\mathbf{M_1}$或者$\mathbf{M_2}$和自己的转置矩阵相乘获得各自的自注意力图$\left{\mathbf{W}<em>{\mathrm{T}</em>{1}}, \mathbf{W}<em>{\mathrm{T}</em>{2}}\right} \in R^{\mathrm{C} \times \mathrm{C}}$，然后$\mathbf{M_1}$和$\mathbf{M_2}$的转置进行矩阵乘法获得互注意力图$\left{\mathbf{W}<em>{\mathrm{S}</em>{1}}, \mathbf{W}<em>{\mathrm{S}</em>{2}}\right} \in R^{\mathrm{C} \times \mathrm{C}}$（这是$\mathbf{M_1}$的，转置之后 softmax 就是$\mathbf{M_2}$的）。然后，对每个分支，自注意力图和互注意力图相加获得通道级别的注意力图，和原始的输入特征图$\mathbf{F}$相乘再和$\mathbf{F}$相加得到输出特征图$\mathrm{F}<em>{\mathrm{T} 1}$和$\mathrm{F}</em>{\mathrm{T} 2}$。</p><p>上述学到的$\mathrm{F}_{\mathrm{T} 1}$用于 Detection head 的检测处理，后者则用于下面的 SAAN 中 ReID 特征的处理。</p><p><img src="https://i.loli.net/2020/11/20/KhYU9rfR3g7VnFe.png"></p><p>上图就是作者设计的 ReID 分支，用于对 ReID 特征进行多尺度融合，这个设计挺简单的，不同分支采用不同的下采样倍率获得不同尺度的特征图（其中通过空间注意力进行特征优化），然后融合产生的特征通过空间注意力加强，最终输出不同目标的 embedding$\mathbf{E} \in R^{512 \times W \times H}$（特征图每个通道对应不同的 anchor 的 embedding）。</p><p><strong>这样，整个 JDE 框架就完成了。</strong></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在 MOT16 和 MOT17 上实验结果如下图，比较的方法都比较新，MOTA 也是刷到了 70 以上，不过速度稍许有点慢了，总的精度还是很不错的。</p><p><img src="https://i.loli.net/2020/11/20/Tcv3j8x1ruYHsQh.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CSTrack 在 JDE 的基础上使用了更强的检测器也对 ReID 特征进行了优化，获得了相当不错的表现。不过，从结果上看这种暴力解耦还是会对整个跟踪的速度有影响的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Liang C, Zhang Z, Lu Y, et al. Rethinking the competition between detection and ReID in Multi-Object Tracking[J]. arXiv:2010.12138 [cs], 2020.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSTrack解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCN &amp; RepPoints解读</title>
      <link href="/2020/11/13/dcn-reppoints/"/>
      <url>/2020/11/13/dcn-reppoints/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>近几年，Anchor-free的目标检测方法受到了很大的关注，究其原因，该类方法不需要像Anchor-base方法那样受限于anchor的配置（anchor的设置要求开发者对数据很了解）就可以获得不错的检测结果，大大减少了数据分析的复杂过程。Anchor-free方法中有一类方法是基于关键点的，它通过检测目标的边界点（如角点）来配对组合成边界框，RepPoints系列就是其代表之作，这包括了RepPoints、Dense RepPoints和RepPoints v2。不过，回顾更久远的历史，从模型的几何形变建模能力的角度来看，RepPoints其实也是对可变形卷积（Deformable Convolutional Networks，DCN）系列的改进，所以本文会从DCN开始讲起，简单回顾这几个工作对几何建模的贡献，其中，DCN系列包括DCN和DCN v2。</p><ul><li><a href="http://arxiv.org/abs/1703.06211">DCN</a>（ICCV2017）</li><li><a href="http://arxiv.org/abs/1811.11168">DCN v2</a>（CVPR2019）</li><li><a href="http://arxiv.org/abs/1904.11490">RepPoints</a>（ICCV2019）</li><li><a href="http://arxiv.org/abs/1912.11473">Dense RepPoints</a>（ECCV2020）</li><li><a href="http://arxiv.org/abs/2007.08508">RepPoints v2</a>（暂未收录）</li></ul><h2 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h2><p>首先，我们来看DCN v1。在计算机视觉中，同一物体在不同的场景或者视角中未知的几何变化是识别和检测任务的一大挑战。为了解决这类问题，通常可以在数据和算法两个方面做文章。从数据的角度看来，通过充分的<strong>数据增强</strong>来构建各种几何变化的样本来增强模型的尺度变换适应能力；从算法的角度来看，设计一些<strong>几何变换不变的特征</strong>即可，比如SIFT特征。</p><p>上述的两种方法都很难做到，前者是因为样本的限制必然无法构建充分数据以保证模型的泛化能力，后者则是因为手工特征设计对于复杂几何变换是几乎不可能实现的。所以作者设计了Deformable Conv（可变形卷积）和Deformable Pooling（可变形池化）来解决这类问题。</p><h3 id="可变形卷积"><a href="#可变形卷积" class="headerlink" title="可变形卷积"></a><strong>可变形卷积</strong></h3><p>顾名思义，可变形卷积的含义就是进行卷积运算的位置是可变的，不是传统的矩形网格，以原论文里的一个可视化图所示，左边的传统卷积的感受野是固定的，在最上层的特征图上其作用的区域显然不是完整贴合目标，而右边的可变形卷积在顶层特征图上自适应的感受野很好的捕获了目标的信息（这可以直观感受得到）。</p><p><img src="https://i.loli.net/2020/11/13/19Arvci4ldpDfna.png"></p><p>那么可变形卷积是如何实现的呢，其实是通过针对每个卷积采样点的偏移量来实现的。如下图所示，其中淡绿色的表示常规采样点，深蓝色的表示可变卷积的采样点，它其实是在正常的采样坐标的基础上加上了一个偏移量（图中的箭头）。</p><p><img src="https://i.loli.net/2020/11/13/HMiJ4mwNU9fPzjS.png"></p><p>我们先来看普通的卷积的实现。使用常规的网格$\mathcal{R}$在输入特征图$x$上进行采样，采样点的值和权重$w$相乘加和得到输出值。举个例子，一个3x3的卷积核定义的网格$\mathcal{R}$表示如下式，中心点为$(0,0)$，其余为相对位置，共9个点。</p><p>$$<br>\mathcal{R}={(-1,-1),(-1,0), \ldots,(0,1),(1,1)}<br>$$</p><p>那么，对输出特征图$y$上的任意一个位置$p_0$都可以以下式进行计算，其中$\mathbf{p}_n$表示就是网格$\mathcal{R}$中的第$n$个点。</p><p>$$<br>\mathbf{y}\left(\mathbf{p}<em>{0}\right)=\sum</em>{\mathbf{p}<em>{n} \in \mathcal{R}} \mathbf{w}\left(\mathbf{p}</em>{n}\right) \cdot \mathbf{x}\left(\mathbf{p}<em>{0}+\mathbf{p}</em>{n}\right)<br>$$</p><p>而可变形卷积干了啥呢，它对原本的卷积操作加了一个偏移量$\left{\Delta \mathbf{p}_{n} \mid n=1, \ldots, N\right}$，也就是这个偏移量使得卷积可以不规则进行，所以上面的计算式变为了下式。不过要注意的是，这个偏移量可以是小数，所以偏移后的位置特征需要通过双线性插值得到，计算式如下面第二个式子。</p><p>$$<br>\mathbf{y}\left(\mathbf{p}<em>{0}\right)=\sum</em>{\mathbf{p}<em>{n} \in \mathcal{R}} \mathbf{w}\left(\mathbf{p}</em>{n}\right) \cdot \mathbf{x}\left(\mathbf{p}<em>{0}+\mathbf{p}</em>{n}+\Delta \mathbf{p}_{n}\right)<br>$$</p><p>$$<br>\mathbf{x}(\mathbf{p})=\sum_{\mathbf{q}} G(\mathbf{q}, \mathbf{p}) \cdot \mathbf{x}(\mathbf{q})<br>$$</p><p>至此，可变卷积的实现基本上理清楚了，现在的问题就是，这个偏移量如何获得？不妨看一下论文中一个3x3可变卷积的解释图（下图），图中可以发现，上面绿色的分支其实学习了一个和输入特征图同尺寸且通道数为$2N$的特征图（$N$为卷积核数目），这就是偏移量，之所以两倍是因为网格上偏移有x和y两个方向。</p><p><img src="https://i.loli.net/2020/11/13/uemUOJDXLHWwcEi.png"></p><h3 id="可变形RoI池化"><a href="#可变形RoI池化" class="headerlink" title="可变形RoI池化"></a><strong>可变形RoI池化</strong></h3><p><img src="https://i.loli.net/2020/11/13/V1DoF9PZqim2jwd.png"></p><p>理解了可变形卷积，理解可变形RoI就没有太大的难度了。原始的RoI pooling在操作时将输入RoI划分为$k\times k = K$个区域，这些区域叫做bin，偏移就是针对这些bin做的。针对每个bin学习偏移量，这里通过全连接层进行学习，因此deformable RoI pooling的输出如下式（含义参考上面的可变卷积即可）。</p><p>$$<br>\mathbf{y}(i, j)=\sum_{\mathbf{p} \in \operatorname{bin}(i, j)} \mathbf{x}\left(\mathbf{p}<em>{0}+\mathbf{p}+\Delta \mathbf{p}</em>{i j}\right) / n_{i j}<br>$$</p><p><strong>至此，关于DCN的解读就完成了，下图是一个来自原论文对的DCN效果的可视化，可以看到绿点标识的目标基本上被可变形卷积感受野覆盖，且这种覆盖能够针对不同尺度的目标。这说明，可变形卷积确实能够提取出感兴趣目标的完整特征，这对目标检测大有好处。</strong></p><p><img src="https://i.loli.net/2020/11/13/xS8pHOFe72Wf3Jq.png"></p><h2 id="DCN-v2"><a href="#DCN-v2" class="headerlink" title="DCN v2"></a>DCN v2</h2><p>DCNv1尽管获得了不错的效果，然而还是存在着不少的问题，DCNv2中进行了一个可视化，对比了普通卷积、DCNv1和DCNv2的区别，下图中每个图片都有从上到下三个可视化，分别是采样点、有效感受野、有效分割区域。可以看出来，DCNv1虽然能覆盖整个目标，但是这种覆盖不够“精细”，会带来不少的背景信息的干扰。</p><p><img src="https://i.loli.net/2020/11/13/kHfNovIC9XZcwy1.png"></p><p>为此，DCNv2提出了一些改进，总结下来就是：</p><ol><li>使用更多的可变卷积层</li><li>Modulated Deformable Modules（调制变形模块）</li><li>RCNN特征模仿指导训练</li></ol><h3 id="更多的可变卷积层"><a href="#更多的可变卷积层" class="headerlink" title="更多的可变卷积层"></a><strong>更多的可变卷积层</strong></h3><p>DCNv1中，只在ResNet50的conv5中使用了3个可变形卷积，DCNv2认为更多的可变形卷积会有更强的几何变化建模效果，所以将conv3到conv5都换为了可变形卷积。之前之所以没有采用更多的可变形卷积，是因为当时没有在大型检测数据集上进行验证，致使精度提升不高。</p><h3 id="调制可变卷积模块"><a href="#调制可变卷积模块" class="headerlink" title="调制可变卷积模块"></a><strong>调制可变卷积模块</strong></h3><p>这才是本文比较大的突破之一，设计了调制可变卷积来控制采样点权重，这样就可以忽略掉不重要甚至有负效果的背景信息。下式是引入偏移量的可变卷积输出特征图的计算式，这是DCNv1的思路。</p><p>$$<br>\mathbf{y}\left(\mathbf{p}<em>{0}\right)=\sum</em>{\mathbf{p}<em>{n} \in \mathcal{R}} \mathbf{w}\left(\mathbf{p}</em>{n}\right) \cdot \mathbf{x}\left(\mathbf{p}<em>{0}+\mathbf{p}</em>{n}+\Delta \mathbf{p}_{n}\right)<br>$$</p><p>上面我们不是发现DCNv1采样了很多无效区域吗，DCNv2则认为，从输入特征图上不仅仅需要学习偏移量，还需要学习一个权重来表示采样点区域是否感兴趣，不感兴趣的区域，权重为0即可。所以原来的计算式变为下式，其中我们所说的权重$\Delta m_{k} \in [0,1]$称为调制因子。<strong>结构上的实现就是原来学习的offset特征图由2N个通道变为3N个通道，这N个通道的就是调制因子。</strong></p><p>$$<br>y(p)=\sum_{k=1}^{K} w_{k} \cdot x\left(p+p_{k}+\Delta p_{k}\right) \cdot \Delta m_{k}<br>$$</p><p>相应的，可变形池化也引入这样的调制因子，计算式变为下式，结构上的实现类似上面的调制可变卷积，这里就不详细展开了。</p><p>$$<br>y(k)=\sum_{j=1}^{n_{k}} x\left(p_{k j}+\Delta p_{k}\right) \cdot \Delta m_{k} / n_{k}<br>$$</p><h3 id="RCNN特征模仿"><a href="#RCNN特征模仿" class="headerlink" title="RCNN特征模仿"></a><strong>RCNN特征模仿</strong></h3><p>作者发现，RCNN和Faster RCNN的分类score结合起来，模型的表现会有提升。这说明，RCNN学到的关注在物体上的特征可以解决无关上下文的问题。但是将RCNN融入整个网络会降大大降低推理速度，DCNv2这里就采用了类似知识蒸馏的做法，把RCNN当作teacher network，让DCNv2主干的Faster RCNN获得的特征去模拟RCNN的特征。</p><p><img src="https://i.loli.net/2020/11/13/mxRNAUnickufZOt.png"></p><p>整个训练设计如上图，左边的网络为主网络（Faster RCNN），右边的网络为子网络（RCNN）。用主网络训练过程中得到的RoI去裁剪原图，然后将裁剪到的图resize到224×224作为子网络的输入，最后将子网络提取的特征和主网络输出的特征计算feature mimicking loss，用来约束这2个特征的差异（实现上就是余弦相似度）。同时子网络通过一个分类损失（如下式）进行监督学习，因为并不需要回归坐标，所以没有回归损失。推理阶段因为没有子网络，所以速度不会有缺失。</p><p>$$<br>L_{\operatorname{mimic}}=\sum_{b \in \Omega}\left[1-\cos \left(f_{\mathrm{RCNN}}(b), f_{\mathrm{FRCNN}}(b)\right)\right]<br>$$</p><p>很多人不理解RCNN的有效性，其实RCNN这个子网络的输入就是RoI在原输入图像上裁剪出来的图像，这就导致不存在RoI以外区域信息（背景）的干扰，这就使得RCNN这个网络训练得到的分类结果是非常可靠的，以此通过一个损失函数监督主网络Faster RCNN的分类路线训练就能够使网络提取到更多RoI内部特征，而不是自己引入的外部特征。<strong>这是一种很有效的训练思路，但这并不能算创新，所以DCNv2的创新也就集中在调制因子上了。而且，从这个训练指导来看，DCNv2完全侧重于分类信息，对采样点没有监督，因此只能学习分类特征而没有几何特征。</strong></p><p>DCNv2对DCNv1进行了一些粗暴的改进并获得了卓有成效的效果，它能将有效感受野更稳定地聚集在目标有效区域，尽管留下了一些遗憾，不过前人的坑总会被后人解决，那就是RepPoints的故事了。</p><h2 id="RepPoints"><a href="#RepPoints" class="headerlink" title="RepPoints"></a>RepPoints</h2><p>这篇文章发表于ICCV2019，新颖地提出使用点集的方式来表示目标，这种方法在不使用anchor的前提下取得了非常好的效果。很多人认为RepPoints是DCNv3，这是由于RepPoints也采用了可变形卷积提取特征，不过它对采用点有相应的损失函数（追求更高的可解释性），可以对DCN的采样点进行监督，让其学习有效的RoI内部信息。此外，它也弥补了之前DCNv2的遗憾，可以学习到目标的几何特征，，而不仅仅是分类特征。</p><p>总结一下，个人觉得，RepPoints主要针对了之前DCNv2的offset（偏移量）学习过于black box，难以解释，所以采用了定位和分类损失直接监督偏移量的学习，这样定位和识别任务会更加“精致”一些。从这个角度来看（如果仅仅是将RepPoints看作一种新的目标表示法未免太低估其价值了），它其实是对DCNv2的改进，至于作者是不是这个出发点，，可以参考作者的<a href="https://www.zhihu.com/question/322372759/answer/798327725">知乎回答</a>，，总之，其对DCNv2的改进可以总结如下：</p><ol><li>通过定位和分类的损失直接监督可形变卷积的偏移量的学习，使得偏移量具有可解释性；</li><li>通过采样点来直接生成伪框 (pseudo box)，不需要另外学习边界框，这样分类和定位建立起了联系。</li></ol><p>当然，上面这些看法是从DCNv2的角度来看的，下面我们回归作者论文的思路来看看RepPoints是如何实现的。</p><p><img src="https://i.loli.net/2020/11/13/s5pudHOwIBvNKYL.png"></p><p>首先，RepPoints（representative points，表示点）具体是怎样表示目标的呢，其实就像下图这样。显然，这和传统的bbox表示法（bounding box，边界框）不同。在目标检测任务中，bbox作为描述检测器各阶段的目标位置的标准形式。然而，其虽然容易计算，但它们仅提供目标的粗略定位，并不完全拟合目标的形状和姿态（因为bbox是个矩形，而目标往往不可能如此规则，所以bbox中必然存在冗余信息，这会导致特征的质量下降从而降低了检测器性能）。对此，提出了一种更贴合目标的细粒度目标表示RepPoints，其形式是一组通过学习自适应地置于目标之上的点，这种表示既限制了目标的空间位置又捕获了精确的语义信息。</p><p>RepPoints是一种如下的二维表示（我们一般用2D表示和4D表示来区分anchor-free和anchor-base方法），一个目标由$n$个点确定，在该论文中$n$设置为9。</p><p>$$<br>\mathcal{R}=\left{\left(x_{k}, y_{k}\right)\right}_{k=1}^{n}<br>$$</p><p>这种点表示可以通过转换函数$\mathcal{T}$转换为伪框（pseudo box），三种转换函数具体可以参考原论文3.2节。然后就是RepPoints的监督学习了，这里也不难实现，通过pseudo box就可以和GT的bbox进行定位监督了，而分类监督和之前的方法没有太大的区别。</p><p><img src="https://i.loli.net/2020/11/13/d8LgoFJzXxHNGvk.png"></p><p>借此，作者设计了一个anchor-free检测框架RPDet，该框架在检测的各个阶段均使用RepPoints表示。通过下图的Pipeline其实就能理解这个网络架构：首先，通过FPN获得特征图；不同于其他单阶段方法一次分类一次回归得到最终结果，RPDet通过两次回归一次分类得到结果（作者自称1.5阶段），分类和第二次回归均采用可变形卷积，可变形卷积可以很好的和RepPoints结合。因为它是在不规则点上进行的。可形变卷积的偏移量是通过第一次回归得到的，也就意味着偏移量在训练过程中是有监督的，而第一次回归的偏移量通过对角点监督得到。采用这种方式，后续的分类和回归特征均是沿着目标选取的，特征质量更高。</p><p><img src="https://i.loli.net/2020/11/13/QDGWAlZou4HNOCn.png"></p><h2 id="Dense-RepPoints"><a href="#Dense-RepPoints" class="headerlink" title="Dense RepPoints"></a>Dense RepPoints</h2><p>Dense RepPoints是RepPoints之后的一个成果，它将RepPoints拓展至实例分割任务，而方法就是采用更加密集的点集表示目标，如下图。</p><p><img src="https://i.loli.net/2020/11/13/GhBfVzujlgDYObw.png"></p><p>Dense RepPoints在RepPoints的基础上进行了如下拓展，使用了更多的点并赋予了点属性。而在表示物体方面，Dense RepPoints采用上图第四个边缘掩码的方式表示目标，它综合了轮廓表示（表示边缘）和网格掩码（前后景区分，利于学习）的优点。</p><p>$$<br>\mathcal{R}=\left{\left(x_{i}+\Delta x_{i}, y_{i}+\Delta y_{i}, \mathbf{a}<em>{i}\right)\right}</em>{i=1}^{n}<br>$$</p><p><img src="https://i.loli.net/2020/11/13/X7esbqK1hxBDYtQ.png"></p><p>最后pipeline和常规的思路很类似，唯一的问题就是采用点集在目标检测中尚可，用于分割会因为点数太多导致计算量大增，所以设计Group pooling;、Shared offset fields;、Shared attribute map来减少计算量，此外还发现点集损失比点损失更加合适等问题，具体可以查阅论文。</p><h2 id="RepPoints-v2"><a href="#RepPoints-v2" class="headerlink" title="RepPoints v2"></a>RepPoints v2</h2><p>最后，我们来看看最新的RepPoints v2，这是对原本RepPoints目标检测任务上的改进。验证（这里指的是分割任务）和回归是神经网络两类通用的任务。验证更容易学习并且准确，回归通常很高效并且能预测连续变化。因此，采用一定的方式将两者组合起来能充分利用它们的优势。RepPoints v2就是在RepPoints的基础上增加了验证模块，提升了检测器性能。</p><p><img src="https://i.loli.net/2020/11/13/uyBAhK13zkHdGSb.png"></p><p>这篇文章细节不少，这里不展开了，直接对着上面的pipeline讲讲整体的思路。RepPoints V2在RepPoints方法基础上添加了一个验证（分割）分支，该分支主要包含两部分，一部分是角点预测，另一部分是目标前景分割。</p><p>如上图所示，训练阶段得到分割heatmap后，这个分割图和原始特征相加，作为回归特征的补充。不过，在推理阶段，在回归分支获取到目标位置后，利用该分割heatmap来对结果进行进一步修正。</p><p>总的来说，这个分支的添加对任务提升不小：一方面多任务往往会带来更好的效果；另一方面，利用分割图增强特征确实可以增强回归效果。</p><p>RepPoints v2工作有一定的可拓展性，它证明了在基于回归的方法上加上这个模块确实可以提升性能，后人要做的就是权衡精度和速度了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从DCN到RepPoints，本质上其实都是更精细的特征提取演变的过程。点集（RepPoints）方式只是显式的表现了出来而已，不过其确实能在精度和速度上取得非常好的平衡。以RepPoints或者类似的思路如今已经活跃在目标检测和实例分割任务中，推动着计算机视觉基础任务的发展，，这是难能可贵的。而且，跳出bbox的范式也诠释着，有时候跳出固有的范式做研究，会获得意想不到的效果。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DCN解读 </tag>
            
            <tag> RepPoints解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LambdaNetworks 论文解读</title>
      <link href="/2020/11/04/lambdanetworks/"/>
      <url>/2020/11/04/lambdanetworks/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近有不少人和我提到 ViT 以及 DETR 以及商汤提出的 Deformable DETR，仿若看到了 Transformer 在计算机视觉中大放异彩的未来，甚至谷歌对其在自注意力机制上进行了调整并提出 Performer。但是，由于 Transformer 的自注意力机制对内存的需求是输入的平方倍，这在图像任务上计算效率过低，当输入序列很长的时候，自注意力对长程交互建模计算量更是庞大无比。而且，Transformer 是出了名的难训练。所以，想要看到其在视觉任务上有更好的表现，还需要面临不小的挑战，不过，LambdaNetworks倒是提出了一种新的长程交互信息捕获的新范式，而且在视觉任务中效果很不错。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>文章对于捕获输入和结构化上下文之间的长程交互提出了一种新的通用框架，该方法名为Lambda Layer。它通过将可用上下文转化为名为lambdas的线性函数，并将这些函数分别应用于每个输入。Lambda层是通用的，它可以建模全局或者局部的内容和位置上的信息交互。并且，由于其避开了使用“昂贵”的注意力图，使得其可以适用于超长序列或者高分辨率图像。由Lambda构成的LambdaNetworks在计算上是高效的，并且可以通过主流计算库实现。实验证明，LambdaNetworks在图像分类、目标检测、实例分割等任务上达到sota水准且计算更加高效。同时，作者也基于ResNet改进设计了LambdaResNet并且获得和EfficientNet相当的效果，快了4.5倍。</p><ul><li><p>论文地址</p><p>  <a href="https://openreview.net/forum?id=xTJEN-ggl1b">https://openreview.net/forum?id=xTJEN-ggl1b</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/lucidrains/lambda-networks">https://github.com/lucidrains/lambda-networks</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>建模长程信息交互是机器学习领域很重要的课题，注意力机制是当前最主流的范式，然而，自注意力的二次内存占用不利于处理超长序列或者多维输入，比如包含数万像素的图像。论文中这里举了个例子，一批256个64x64的图像使用8head的多头注意力就需要32G的内存。</p><p>考虑到自注意力的局限性，论文提出了Lambda层，该层为捕获输入和结构化的上下文之间的长程信息交互提供了一种新的通用框架。Lambda层捕获信息交互的方式也很简单，它将可用上下文转化为线性函数，并将这些线性函数分别应用于每个输入，这些线性函数就是lambda。Lambda层可以成为注意力机制的替代品，注意力在输入和上下文之间定义了一个相似性核，而Lambda层将上下文信息总结为一个固定size的线性函数，这样就避开了很耗内存的注意力图。他俩的对比，可以通过下面的图看出来（左图是一个包含三个query的局部上下文，它们同处一个全局上下文中；中图是attention机制产生的注意力图；右图则是lambda层线性函数作用于query的结果）。</p><p><img src="https://i.loli.net/2020/11/04/eDh6RuJ7BsMjgZx.png"></p><p>Lambda层用途广泛，可以实现为在全局、局部或masked上下文中对内容和基于位置的交互进行建模。由此产生的神经网络结构LambdaNetworks具有高效的计算能力，并且可以以较小的内存开销建模长程依赖，因此非常适用于超大结构化输入，如高分辨率图像。</p><p>后文也用实验证明，在注意力表现很好的任务中，LambdaNetworks表现相当，且计算更为高效且更快。</p><h2 id="长程信息交互建模"><a href="#长程信息交互建模" class="headerlink" title="长程信息交互建模"></a>长程信息交互建模</h2><p>论文在第二部分主要对一些Lambda的术语进行了定义，引入keys作为捕获queries和它们的上下文之间信息交互的需求，而且，作者也说明，Lambda layer采用了很多自注意力的术语来减少阅读差异，这就是为什么很多人觉得两者在很多名称定义上差异不大的原因。</p><h3 id="queries、contexts和interactions"><a href="#queries、contexts和interactions" class="headerlink" title="queries、contexts和interactions"></a><strong>queries、contexts和interactions</strong></h3><p>$\mathcal{Q}=\left{\left(\boldsymbol{q}<em>{n}, n\right)\right}$和$\mathcal{C}=\left{\left(\boldsymbol{c}</em>{m}, m\right)\right}$分别表示queries和contexts，每个$\left(\boldsymbol{q}<em>{n}, n\right)$都包含内容$\boldsymbol{q}</em>{n} \in \mathbb{R}^{|k|}$和位置$n$，同样的，每个上下文元素$\left(\boldsymbol{c}<em>{m}, m\right)$都包含内容$\boldsymbol{c}</em>{m}$和位置$m$，而$(n, m)$指的是任意结构化元素之间的成对关系。举个例子，这个(n,m)对可以指被固定在二维栅格上的两个像素的相对距离，也可以指图（Graph）上俩node之间的关系。</p><p>下面，作者介绍了Lambda layer的工作过程。先是考虑给定的上下文$\mathcal{C}$的情况下通过函数$\boldsymbol{F}:\left(\left(\boldsymbol{q}<em>{n}, n\right), \mathcal{C}\right) \mapsto \boldsymbol{y}</em>{n}$将query映射到输出向量$\boldsymbol{y}<em>{n}$。显然，如果处理的是结构化输入，那么这个函数可以作为神经网络中的一个层来看待。将$\left(\boldsymbol{q}</em>{n}, \boldsymbol{c}<em>{m}\right)$称为基于内容的交互，$\left(\boldsymbol{q}</em>{n},(n, m)\right)$则为基于位置的交互。此外，若$\boldsymbol{y}<em>{n}$依赖于所有的$\left(\boldsymbol{q}</em>{n}, \boldsymbol{c}<em>{m}\right)$或者$\left(\boldsymbol{q}</em>{n},(n, m)\right)$，则称$\boldsymbol{F}$捕获了全局信息交互，如果只是围绕$n$的一个较小的受限上下文用于映射，则称$\boldsymbol{F}$捕获了局部信息交互。最后，若这些交互包含了上下文中所有$|m|$个元素则称为密集交互（dense interaction），否则为稀疏交互（sparse interaction）。</p><h3 id="引入key来捕获长程信息交互"><a href="#引入key来捕获长程信息交互" class="headerlink" title="引入key来捕获长程信息交互"></a><strong>引入key来捕获长程信息交互</strong></h3><p>在深度学习这种依赖GPU计算的场景下，我们优先考虑快速的线性操作并且通过点积操作来捕获信息交互。这就促使了引入可以和query通过点击进行交互的向量，该向量和query同维。特别是基于内容的交互$\left(\boldsymbol{q}<em>{n}, \boldsymbol{c}</em>{m}\right)$需要一个依赖$\boldsymbol{c}<em>{m}$的$k$维向量，这个向量就是key（键）。相反，基于位置的交互$\left(\boldsymbol{q}</em>{n},(n, m)\right)$则需要位置编码$\boldsymbol{e}<em>{n m} \in \mathbb{R}^{|k|}$，有时也称为相对key。query和key的深度$|k|$以及上下文空间维度$|m|$不在输出$\boldsymbol{y}</em>{n} \in \mathbb{R}^{|v|}$，因此需要将这些维度收缩为layer计算的一部分。因此，捕获长程交互的每一层都可以根据它是收缩查询深度还是首先收缩上下文位置来表征。</p><h3 id="注意力交互"><a href="#注意力交互" class="headerlink" title="注意力交互"></a><strong>注意力交互</strong></h3><p>收缩query的深度首先会在query和上下文元素之间创建一个相似性核，这就是attention操作。随着上下文位置$|m|$的增大而输入输出维度$|k|$和$|v|$不变，考虑到层输出是一个很小维度的向量$|v| \ll|m|$，注意力图（attention map）的计算会变得很浪费资源。</p><h3 id="Lambda交互"><a href="#Lambda交互" class="headerlink" title="Lambda交互"></a><strong>Lambda交互</strong></h3><p>相反，通过一个线性函数$\boldsymbol{\lambda}(\mathcal{C}, n)$获得输出$\boldsymbol{y}<em>{n}=F\left(\left(\boldsymbol{q}</em>{n}, n\right), \mathcal{C}\right)=\boldsymbol{\lambda}(\mathcal{C}, n)\left(\boldsymbol{q}<em>{n}\right)$会更高效地简化映射过程（map）。在这个场景中，上下文被聚合为一个固定size的线性函数$\boldsymbol{\lambda}</em>{n}=\boldsymbol{\lambda}(\mathcal{C}, n)$。每个$\boldsymbol{\lambda}_{n}$作为一个小的线性函数独立于上下文并且被用到相关的query$\boldsymbol{q}_n$后丢弃。这个机制很容易联想到影响比较大的函数式编程和lambda积分，所以称为lambda层。</p><h2 id="Lambda层"><a href="#Lambda层" class="headerlink" title="Lambda层"></a>Lambda层</h2><p>一个lambda层将输入$\boldsymbol{X} \in \mathbb{R}^{|n| \times d_{i n}}$和上下文$\boldsymbol{C} \in \mathbb{R}^{|m| \times d_{c}}$作为输入并产生线性函数lambdas分别作用于query，返回输出$\boldsymbol{Y} \in \mathbb{R}^{|n| \times d_{o u t}}$。显然，在自注意力中，$\boldsymbol{C} = \boldsymbol{X}$。为了不失一般性，我们假定$d_{i n}=d_{c}=d_{o u t}=d$。在接下来的论文里，作者将重点放在了lambda层的一个具体实例上，并且证明lambda层可以获得密集的长程内容和位置的信息交互而不需要构建注意力图。</p><h3 id="将上下文转化为线性函数"><a href="#将上下文转化为线性函数" class="headerlink" title="将上下文转化为线性函数"></a><strong>将上下文转化为线性函数</strong></h3><p>首先，假定上下文只有一个query$\left(\boldsymbol{q}_{n}, n\right)$。我们希望产生一个线性函数lambda$\mathbb{R}^{|k|} \rightarrow \mathbb{R}^{|v|}$，我们将$\mathbb{R}^{|k| \times|v|}$称为函数。下表所示的就是lambda层的超参、参数以及其他相关的配置。</p><p><img src="https://i.loli.net/2020/11/04/rRBmAzS2dYHeniW.png"></p><p><strong>生成上下文lambda函数</strong>：lambda层首先通过线性投影上下文来计算keys和values，并且使用softmax操作跨上下文对keys进行标准化从而得到标准化后的$\bar{K}$。它的实现可以看作是一种函数式消息传递，每个上下文元素贡献一个内容function$\boldsymbol{\mu}<em>{m}^{c}=\overline{\boldsymbol{K}}</em>{m} \boldsymbol{V}<em>{\boldsymbol{m}}^{T}$和位置function$\boldsymbol{\mu}</em>{n m}^{p}=\boldsymbol{E}<em>{n m} \boldsymbol{V}</em>{\boldsymbol{m}}^{T}$，最终的lambda函数其实是两者的和，具体如下，式子中的$\boldsymbol{\lambda}^{c}$为内容lambda，而$\boldsymbol{\lambda}^p_n$为位置lambda。内容$\boldsymbol{\lambda}^{c}$对上下文元素的排列是不变的，在所有的query位置$n$之间共享，并仅基于上下文内容对$\boldsymbol{q}<em>{n}$进行编码转换。不同的是，位置$\lambda</em>{n}^{p}$基于内容$\boldsymbol{c}_{m}$和位置$(n, m)$对查询query进行编码转换，从而支持对结构化输入建模如图像。</p><p>$$<br>\begin{aligned}<br>\boldsymbol{\lambda}^{c} &amp;=\sum_{m} \boldsymbol{\mu}<em>{m}^{c}=\sum</em>{m} \overline{\boldsymbol{K}}<em>{m} \boldsymbol{V}</em>{\boldsymbol{m}}^{T} \<br>\boldsymbol{\lambda}<em>{n}^{p} &amp;=\sum</em>{m} \boldsymbol{\mu}<em>{n m}^{p}=\sum</em>{m} \boldsymbol{E}<em>{n m} \boldsymbol{V}</em>{\boldsymbol{m}}^{T} \<br>\boldsymbol{\lambda}<em>{n} &amp;=\boldsymbol{\lambda}^{c}+\boldsymbol{\lambda}</em>{n}^{p} \in \mathbb{R}^{|k| \times|v|}<br>\end{aligned}<br>$$</p><p><strong>应用lambda到query</strong>：输入被转化为query$\boldsymbol{q}<em>{n}=\boldsymbol{W}</em>{Q} \boldsymbol{x}_{n}$，然后lambda层获得如下输出。</p><p>$$<br>\boldsymbol{y}<em>{n}=\boldsymbol{\lambda}</em>{n} \boldsymbol{q}<em>{n}=\left(\boldsymbol{\lambda}^{c}+\boldsymbol{\lambda}</em>{n}^{p}\right) \boldsymbol{q}_{n} \in \mathbb{R}^{|v|}<br>$$</p><p><strong>Lambda的解释</strong>：$\boldsymbol{\lambda}<em>{n} \in \mathbb{R}^{|k| \times|v|}$矩阵的列可以看作$|k| |v|$维上下文特征的固定size的集合。这些上下文特征从上下文内容和结构聚合而来。应用lambda线性函数动态地分布这些上下文特征来产生输出$\boldsymbol{y}</em>{n}=\sum_{k} q_{n k} \boldsymbol{\lambda}_{n k}$。这个过程捕获密集地内容和位置的长程信息交互，而不需要产生注意力图。</p><p><strong>标准化</strong>： 实验表明，非线性或者标准化操作对计算是有帮助的，作者在计算的query和value之后应用batch normalization发现是有效的。</p><h3 id="对结构化上下文应用Lambda函数"><a href="#对结构化上下文应用Lambda函数" class="headerlink" title="对结构化上下文应用Lambda函数"></a><strong>对结构化上下文应用Lambda函数</strong></h3><p>在这一节，作者主要介绍如何将lambda层应用于结构化上下文。</p><p><strong>Translation equivariance</strong>：在很多机器学习场景中，Translation equivariance是一个很强的归纳偏置。由于基于内容的信息交互是排列等变的，因此本就是translation equivariant。而位置的信息交互获得translation equivariant则通过对任意的translation $t$确保位置编码满足$\boldsymbol{E}<em>{n m}=\boldsymbol{E}</em>{t(n) t(m)}$来做到。实际中，我们定义一个相对位置编码的张量$\boldsymbol{R} \in \mathbb{R}^{|k| \times|r| \times|u|}$，其中$r$索引对所有的$(n,m)$对可能的相对位置，并将其重新索引为$\boldsymbol{E} \in \mathbb{R}^{|k| \times|n| \times|m| \times|u|}$，如$\boldsymbol{E}<em>{n m}=\boldsymbol{R}</em>{r(n, m)}$。</p><p><strong>Lambda 卷积</strong>： 尽管有长程信息交互的好处，局部性在许多任务中仍然是一个强烈的归纳偏置。从计算的角度来看，使用全局上下文可能会产生噪声或过度。因此，将位置交互的范围限制到查询位置$n$周围的一个局部邻域，就像局部自注意和卷积的情况一样，可能是有用的。这可以通过对所需范围之外的上下文位置$m$的位置嵌入进行归零来实现。然而，对于较大的$|m|$值，这种策略仍然代价高昂，因为计算仍然会发生(它们只是被归零)。在上下文被安排在多维网格上时，可以通过常规卷积从局部上下文中生成位置lambdas，将$\boldsymbol{V}$中的$v$维视为额外的空间维度。考虑在一维序列上的大小为$|r|$的局部域上生成位置lambdas。相对位置编码张量$\boldsymbol{R} \in \mathbb{R}^{|r| \times|u| \times|k|}$可以被reshape到$\overline{\boldsymbol{R}} \in \mathbb{R}^{|r| \times 1 \times|u| \times|k|}$，并且被用作二维卷积核来计算需要的位置lambda，算式如下。</p><p>$$<br>\boldsymbol{\lambda}<em>{b n v k}=\operatorname{conv} 2 \mathrm{d}\left(\boldsymbol{V}</em>{b n v u}, \overline{\boldsymbol{R}}_{r 1 u k}\right)<br>$$</p><p>这个操作称为lambda卷积，由于计算被限制在一个局部范围，lambda卷积相对于输入只需要线性时间和内存复杂度的消耗。lambda卷积很容易和其他功能一起使用，如dilation和striding，并且在硬件计算上享受告诉运算。计算效率和局部自注意力形成了鲜明对比，如下表。</p><p><img src="https://i.loli.net/2020/11/04/bmIGXYyWcRNJ23T.png"></p><h3 id="multiquery-lambdas减少复杂性"><a href="#multiquery-lambdas减少复杂性" class="headerlink" title="multiquery lambdas减少复杂性"></a><strong>multiquery lambdas减少复杂性</strong></h3><p>这部分作者主要对计算复杂度进行了分析，设计了多query lambda，计算复杂度对比如下。</p><p><img src="https://i.loli.net/2020/11/04/41S9hjkmUFdxiXb.png"></p><p>提出的multiquery lambdas可以通过einsum高效实现。</p><p>$$<br>\begin{aligned}<br>\boldsymbol{\lambda}<em>{b k v}^{c}=&amp; \operatorname{einsum}\left(\overline{\boldsymbol{K}}</em>{b m k u}, \boldsymbol{V}<em>{b m v u}\right) \<br>\boldsymbol{\lambda}</em>{b n k v}^{p} &amp;=\operatorname{einsum}\left(\boldsymbol{E}<em>{k n m u}, \boldsymbol{V}</em>{b m v u}\right) \<br>\boldsymbol{Y}<em>{b n h v}^{c} &amp;=\operatorname{einsum}\left(\boldsymbol{Q}</em>{b n h k}, \boldsymbol{\lambda}<em>{b k v}^{c}\right) \<br>\boldsymbol{Y}</em>{b n h v}^{p} &amp;=\operatorname{einsum}\left(\boldsymbol{Q}<em>{b n h k}, \boldsymbol{\lambda}</em>{b n k v}^{p}\right) \<br>\boldsymbol{Y}<em>{b n h v} &amp;=\boldsymbol{Y}</em>{b n h v}^{c}+\boldsymbol{Y}_{b n h v}^{p}<br>\end{aligned}<br>$$</p><p>然后，对比了lambda 层和自注意力在resnet50架构上的imagenet分类任务效果。显然，lambda层参数量是很少的，且准确率很高。</p><p><img src="https://i.loli.net/2020/11/04/fehAXKUOJM6Nas2.png"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在大尺度高分辨率计算机视觉任务上进行了充分的实验，和SOTA的EfficientNet相比，可以说无论是速度还是精度都有不小的突破。</p><p><img src="https://i.loli.net/2020/11/04/mWH1QgTY4JGOfdZ.png"></p><p>其长子检测任务上，LambdaResNet也极具优势。</p><p><img src="https://i.loli.net/2020/11/04/1PTjFqlRaEcmJkK.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作者提出了Lambda Layer代替自注意力机制，获得了较好的改进。并借此设计了LambdaNetworks，其在各个任务上都超越了SOTA且速度提高了很多。如果实践证明，Lambda Layer的效果具有足够的鲁棒性，在以后的研究中应该会被广泛使用。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Anonymous. LambdaNetworks: Modeling long-range Interactions without Attention[A]. 2020.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LambdaNetworks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FairMOT实时多目标跟踪系统</title>
      <link href="/2020/10/24/fairmot-realtime/"/>
      <url>/2020/10/24/fairmot-realtime/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>FairMOT是今年很火的一个多目标跟踪算法，前不久也开放了最新版本的论文，并于最近重构了开源代码，我也在实际工程视频上进行了测试，效果是很不错的。不过，官方源码没有提高实时摄像头跟踪的编程接口，我在源码的基础上进行了修改，增加了实时跟踪模块。本文介绍如何进行环境配置和脚本修改，实现摄像头跟踪（<strong>本文均采用Ubuntu16.04进行环境配置，使用Windows在安装DCN等包的时候会有很多问题，不建议使用</strong>）。</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><blockquote><p>下述环境配置需要保证用户已经安装了git和conda，否则配置pytorch和cuda会诸多不便。</p></blockquote><p>首先，通过下面的git命令从Github克隆源码到本地并进入该项目。访问<a href="https://pan.baidu.com/s/1H1Zp8wrTKDk20_DSPAeEkg">链接</a>（提取码uouv）下载训练好的模型，在项目根目录下新建<code>models</code>目录（和已有的<code>assets</code>、<code>src</code>等目录同级），将刚刚下载好的模型文件<code>fairmot_dla34.pth</code>放到这个<code>models</code>目录下。</p><pre class=" language-shell"><code class="language-shell">git clone git@github.com:ifzhang/FairMOT.gitcd FairMOT</code></pre><p>下面，通过conda创建适用于该项目的虚拟环境（环境隔离），国内用户速度慢可以参考<a href="https://zhouchen.blog.csdn.net/article/details/86086919">我conda的文章</a>配置国内源。创建之后通过<code>activate</code>激活环境（该命令出错将<code>conda</code>换为<code>source</code>）。然后在当前虚拟环境下（<strong>后续关于该项目的操作都需要在该虚拟环境下</strong>）安装pytorch和cuda（这里也建议配置国内源后安装<code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch</code>）。最后，通过pip命令安装所需d的Python包（国内建议<a href="https://zhouchen.blog.csdn.net/article/details/106420275">配置清华源</a>），注意先安装cython。</p><pre class=" language-shell"><code class="language-shell">conda create -n fairmot python=3.6conda activate fairmotconda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0pip install cythonpip install -r requirements.txtpip install -U opencv-python==4.1.1.26</code></pre><p>同时，用于项目使用了DCNv2所以需要安装该包，该包只能通过源码安装，依次执行下述命令即可（安装过程报warning是正常情况，不报error就行）。</p><pre class=" language-shell"><code class="language-shell">git clone https://github.com/CharlesShang/DCNv2cd DCNv2./make.shcd ../</code></pre><p>至此，所有的环境配置已经完成，由于这里还需要使用到ffmpeg来生成视频文件，所以系统需要安装ffmpeg（Ubuntu采用apt安装即可），教程很多，不多赘述。</p><p>想要试试项目是否正常工作，可以使用下面的命令在demo视频上进行跟踪测试（初次允许需要下载dla34模型，这个模型国内下载速度还可以，我就直接通过允许代码下载的）。</p><pre class=" language-shell"><code class="language-shell">cd srcpython demo.py mot --input-video ../videos/MOT16-03.mp4 --load_model ../models/fairmot_dla34.pth --conf_thres 0.4</code></pre><p>默认文件输出在项目根目录的<code>demos</code>文件夹下，包括每一帧的检测结果以及组合成的视频。</p><p><img src="https://i.loli.net/2020/10/24/bxVoM7ZyjTBmhLI.png"></p><p><img src="https://i.loli.net/2020/10/24/NF7idqKkQ8CyfG9.png"></p><h2 id="实时跟踪"><a href="#实时跟踪" class="headerlink" title="实时跟踪"></a>实时跟踪</h2><p>实时跟踪主要在两个方面进行修改，一是数据加载器，二是跟踪器。首先，我们在<code>src</code>目录下新建一个类似于<code>demo.py</code>的脚本文件名为<code>camera.py</code>，写入和<code>demo.py</code>类似的内容，不过，我们把视频路径换位摄像机编号（这是考虑到JDE采用opencv进行视频读取，而opencv视频读取和摄像机视频流读取是一个接口）。具体<code>camera.py</code>内容如下。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> _init_paths<span class="token keyword">from</span> opts <span class="token keyword">import</span> opts<span class="token keyword">from</span> tracking_utils<span class="token punctuation">.</span>utils <span class="token keyword">import</span> mkdir_if_missing<span class="token keyword">import</span> datasets<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>jde <span class="token keyword">as</span> datasets<span class="token keyword">from</span> track <span class="token keyword">import</span> eval_seq<span class="token keyword">def</span> <span class="token function">recogniton</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    result_root <span class="token operator">=</span> opt<span class="token punctuation">.</span>output_root <span class="token keyword">if</span> opt<span class="token punctuation">.</span>output_root <span class="token operator">!=</span> <span class="token string">''</span> <span class="token keyword">else</span> <span class="token string">'.'</span>    mkdir_if_missing<span class="token punctuation">(</span>result_root<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"start tracking"</span><span class="token punctuation">)</span>    dataloader <span class="token operator">=</span> datasets<span class="token punctuation">.</span>LoadVideo<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> opt<span class="token punctuation">.</span>img_size<span class="token punctuation">)</span>    result_filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>result_root<span class="token punctuation">,</span> <span class="token string">'results.txt'</span><span class="token punctuation">)</span>    frame_rate <span class="token operator">=</span> dataloader<span class="token punctuation">.</span>frame_rate    frame_dir <span class="token operator">=</span> None <span class="token keyword">if</span> opt<span class="token punctuation">.</span>output_format <span class="token operator">==</span> <span class="token string">'text'</span> <span class="token keyword">else</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>result_root<span class="token punctuation">,</span> <span class="token string">'frame'</span><span class="token punctuation">)</span>    eval_seq<span class="token punctuation">(</span>opt<span class="token punctuation">,</span> dataloader<span class="token punctuation">,</span> <span class="token string">'mot'</span><span class="token punctuation">,</span> result_filename<span class="token punctuation">,</span>             save_dir<span class="token operator">=</span>frame_dir<span class="token punctuation">,</span> show_image<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> frame_rate<span class="token operator">=</span>frame_rate<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0'</span>    opt <span class="token operator">=</span> opts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>init<span class="token punctuation">(</span><span class="token punctuation">)</span>    recogniton<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>接着，原来JDE关于视频加载是针对真正的视频的，对于摄像头这种无限视频流，修改其帧数为无限大（很大很大的整数值即可），也就是将<code>src/lib/datasets/dataset/jde.py</code>中<code>LoadVideo</code>修改如下。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoadVideo</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> path<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1088</span><span class="token punctuation">,</span> <span class="token number">608</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>cap <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoCapture<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>frame_rate <span class="token operator">=</span> int<span class="token punctuation">(</span>round<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FPS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>vw <span class="token operator">=</span> int<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_WIDTH<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>vh <span class="token operator">=</span> int<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_HEIGHT<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> type<span class="token punctuation">(</span>path<span class="token punctuation">)</span> <span class="token operator">==</span> type<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>vn <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token number">32</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>vn <span class="token operator">=</span> int<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_COUNT<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>width <span class="token operator">=</span> img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>height <span class="token operator">=</span> img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>        self<span class="token punctuation">.</span>w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token number">1920</span><span class="token punctuation">,</span> <span class="token number">1080</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Lenth of the video: &amp;#123;:d&amp;#125; frames'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vn<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_size</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vw<span class="token punctuation">,</span> vh<span class="token punctuation">,</span> dw<span class="token punctuation">,</span> dh<span class="token punctuation">)</span><span class="token punctuation">:</span>        wa<span class="token punctuation">,</span> ha <span class="token operator">=</span> float<span class="token punctuation">(</span>dw<span class="token punctuation">)</span> <span class="token operator">/</span> vw<span class="token punctuation">,</span> float<span class="token punctuation">(</span>dh<span class="token punctuation">)</span> <span class="token operator">/</span> vh        a <span class="token operator">=</span> min<span class="token punctuation">(</span>wa<span class="token punctuation">,</span> ha<span class="token punctuation">)</span>        <span class="token keyword">return</span> int<span class="token punctuation">(</span>vw <span class="token operator">*</span> a<span class="token punctuation">)</span><span class="token punctuation">,</span> int<span class="token punctuation">(</span>vh <span class="token operator">*</span> a<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>count <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>count <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> StopIteration        <span class="token comment" spellcheck="true"># Read image</span>        res<span class="token punctuation">,</span> img0 <span class="token operator">=</span> self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># BGR</span>        <span class="token keyword">assert</span> img0 <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">,</span> <span class="token string">'Failed to load frame &amp;#123;:d&amp;#125;'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>count<span class="token punctuation">)</span>        img0 <span class="token operator">=</span> cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>img0<span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Padded resize</span>        img<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> letterbox<span class="token punctuation">(</span>img0<span class="token punctuation">,</span> height<span class="token operator">=</span>self<span class="token punctuation">.</span>height<span class="token punctuation">,</span> width<span class="token operator">=</span>self<span class="token punctuation">.</span>width<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Normalize RGB</span>        img <span class="token operator">=</span> img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        img <span class="token operator">=</span> np<span class="token punctuation">.</span>ascontiguousarray<span class="token punctuation">(</span>img<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>        img <span class="token operator">/=</span> <span class="token number">255.0</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>count<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img0    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>vn  <span class="token comment" spellcheck="true"># number of frames</span></code></pre><p>至此，读取视频流也通过一个粗暴的方式实现了，然后就是窗口显示了，原来项目中跟踪器只会一帧一帧写入跟踪后的结果图像，然后通过<code>ffmpeg</code>将这些图像组合为视频。不过，原项目已经设计了实时显示跟踪结果窗口的接口了，只需要调用<code>track.py</code>中的<code>eval_seq</code>函数时，参数<code>show_image</code>设置为<code>True</code>即可。不过，也许作者并没有测试过这个模块，这里显示会有些问题，务必将<code>eval_seq</code>中下述代码段进行如下修改。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">if</span> show_image<span class="token punctuation">:</span>    cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'online_im'</span><span class="token punctuation">,</span> online_im<span class="token punctuation">)</span>    cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></code></pre><p>调整完成后，输入下面的命令运行跟踪脚本（命令行Ctrl+C停止跟踪，跟踪的每一帧存放在指定的<code>output-root</code>目录下的<code>frame</code>目录中）。</p><pre class=" language-shell"><code class="language-shell">python camera.py mot --load_model ../models/fairmot_dla34.pth --output-root ../results</code></pre><p><img src="https://i.loli.net/2020/10/24/Kp52w4qlb7nyVRQ.png"></p><p>上图是我实际测试得到的运行结果，摄像头分辨率比较低并且我做了一些隐私模糊处理，不过，整个算法的实用性还是非常强的，平均FPS也有18左右（单卡2080Ti）。</p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文对FairMOT源码进行了简单粗暴的修改以实现了一个摄像头视频实时跟踪系统，只是研究FairMOT代码闲暇之余的小demo，具体代码可以在<a href="https://github.com/luanshiyinyang/FairMOT">我的Github</a>找到。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 多目标跟踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FairMOT摄像头实时多目标跟踪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DLA 论文解读</title>
      <link href="/2020/10/22/dla/"/>
      <url>/2020/10/22/dla/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>沿着卷积神经网络在计算机视觉的发展史，可以发现，丰富的表示（representations）是视觉任务最需要的。随着网络的深度增加，单独一层的信息是远远不够的，只有聚合各层信息才能提高网络对 what（识别）和 where（定位）两个问题的推断能力。现有的很多网络设计工作主要致力于设计更深更宽的网络，但是如何更好地组合不同网络层（layer）、不同结构块（block）其实值得更多的关注。尽管跳跃连接（skip connecions）常用来组合多层，不过这样的连接依然是浅层的（因为其只采用了简单的单步运算）。该论文通过更深层的聚合来增强标准网络的能力。DLA 结构能够迭代式层级组合多层特征以提高精度的同时减参。实验证明，DLA 相比现有的分支和合并结构，效果更好。</p><ul><li><p>论文地址</p><p><a href="https://arxiv.org/abs/1707.06484">https://arxiv.org/abs/1707.06484</a></p></li><li><p>论文源码</p><p><a href="https://github.com/ucbdrive/dla">https://github.com/ucbdrive/dla</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>表示学习和迁移学习的发展推动了计算机视觉的发展，可以简单组合的特性催生了很多深度网络。为了满足各种不同的任务，寻找合适的网络结构至关重要。随着网络尺寸的增加，模块化的设计更为重要，所以现在的网络越来越深的同时，更紧密的连接能否带来提升呢？</p><p>更多的非线性、更强的表示能力、更大的感受野一般能够提高网络精度，但是会带来难以优化和计算量大的问题。为了克服这些缺陷，不同的block和modul被集成到一起来平衡和优化这些特点，如使用bottlenecks进行降维、使用residual、gated和concatnative连接特征以及梯度传播算法。这些技术使得网络可以到达100甚至1000层。</p><p>然而，如何连接这些layer和module还需要更多的探索。简单地通过序列化堆叠层来构造网络，如LeNet、AlexNet以及ResNet。经过复杂的分析，更深的网络层能提取到更多语义和全局的特征，但是这并不能表明最后一层就是任务需要的表示。实际上“跳跃连接”已经证明了对于分类、回归以及其他结构化问题的有效性。因此，如何聚合，尤其是深度与宽度上的聚合，对于网络结构的优化是一个非常重要的技术。</p><p><img src="https://i.loli.net/2020/10/22/ELq8voPCiXpZRWk.png"></p><p>论文研究了如何聚合各层特征来融合语义和空间信息以进行识别与定位任务。通过扩展现有的“浅跳跃连接”（单层内部进行连接），论文提出的聚合结构实现了更深的信息共享。文中主要引入两种DLA结构：iterative deep aggregation (IDA，迭代式深度聚合)和hierarchal deep aggregation (HDA，层级深度聚合)。为了更好的兼容现有以及以后的网络结构，IDA和HDA结构通过独立于backbone的结构化框架实现。IDA主要进行跨分辨率和尺度的融合，而HDA主要用于融合各个module和channel的特征。<strong>从上图也可以看出来，DLA集成了密集连接和特征金字塔的优势，IDA根据基础网络结构，逐级提炼分辨率和聚合尺度（语义信息的融合（发生在通道和深度上），类似残差模块），HDA通过自身的树状连接结构，将各个层级聚合为不同等级的表征（空间信息的融合（发生在分辨率和尺度上），类似于FPN）。本文的策略可以通过混合使用IDA与HDA来共同提升效果。</strong> </p><p>DLA通过实验在现有的ResNet和ResNeXt网络结构上采用DLA架构进行拓展，来进行图像分类、细粒度的图像识别、语义分割和边界检测任务。实验表明，DLA的可以在现有的ResNet、ResNeXt、DenseNet等网络结构的基础上提升模型的性能、减少参数数量以及减少显存消耗。DLA达到了目前分类任务中compact models的最佳精度，在分割等任务也达到超越SOTA。DLA是一种通用而有效的深度网络拓展技术。</p><h2 id="DLA"><a href="#DLA" class="headerlink" title="DLA"></a>DLA</h2><p>文中将“聚合”定义为跨越整个网络的多层组合，文中研究的也是那些深度、分辨率、尺度上能有效聚合的一系列网络。由于网络可以包含许多层和连接，模块化的设计可以通过分组和重复来克服复杂度问题。多个layer组合为一个block，多个block再根据分辨率组合为一个stage，DLA则主要探讨block和stage的组合（stage间网络保持一致分辨率，那么空间融合发生在stage间，语义融合发生在stage内）。</p><p><img src="https://i.loli.net/2020/10/22/6cypY4rg9Hkt12h.png"></p><h3 id="IDA"><a href="#IDA" class="headerlink" title="IDA"></a>IDA</h3><p>IDA沿着迭代堆叠的backbone进行，依据分辨率对整个网络分stage，越深的stage含有更多的语义信息但空间信息很少。“跳跃连接”由浅至深融合了不同尺度以及分辨率信息，但是这样的“跳跃连接”都是线性且都融合了最浅层的信息，如上图b中，每个stage都只融合上一步的信息。</p><p>因此，论文提出了IDA结构，从最浅最小的尺度开始，迭代式地融合更深更大尺度地信息，这样可以使得浅层网络信息在后续stage中获得更多地处理从而得到精炼，上图c就是IDA基本结构。</p><p>IDA对应的聚合函数$I$对不同层的特征，随着加深的语义信息表示如下（N表示聚合结点，后文会提到）：</p><p>$$<br>I\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}\right)=\left{\begin{array}{ll}<br>\mathbf{x}<em>{1} &amp; \text { if } n=1 \<br>I\left(N\left(\mathbf{x}</em>{1}, \mathbf{x}<em>{2}\right), \ldots, \mathbf{x}</em>{n}\right) &amp; \text { otherwise }<br>\end{array}\right.<br>$$</p><h3 id="HDA"><a href="#HDA" class="headerlink" title="HDA"></a>HDA</h3><p>HDA以树的形式合并block和stage来保持和组合特征通道，通过HDA，浅层和深层的网络层可以组合到一起，这样的组合信息可以跨越各个层级从而学得更加丰富。尽管IDA可以高效组合stage，但它依然是序列性的，不足以用来融合网络各个block信息。上图d就是HDA的深度分支结构，这是一个明显的树形结构。</p><p>在基础的HDA结构上，可以改进其深度和效率，将一个聚合结点的输出重置为主干网络用于下一棵子树的输入。结构如上图e所示，这样，之前所有block的信息都被送入后续处理中。同时，为了效率，作者将同一深度的聚合结点（指的是父亲和左孩子，也就是相同特征图尺寸的）合并，如上图f。</p><p>HDA的聚合函数$T_n$计算如下，$n$表示深度，$N$依然是聚合结点。</p><p>$$<br>\begin{array}{r}<br>T_{n}(\mathbf{x})=N\left(R_{n-1}^{n}(\mathbf{x}), R_{n-2}^{n}(\mathbf{x}), \ldots,\right.<br>\left.R_{1}^{n}(\mathbf{x}), L_{1}^{n}(\mathbf{x}), L_{2}^{n}(\mathbf{x})\right),<br>\end{array}<br>$$</p><p>上面式子的$R$和$L$定义如下，表示左树和右树，下式$B$表示卷积块。</p><p>$$<br>\begin{aligned}<br>L_{2}^{n}(\mathbf{x}) &amp;=B\left(L_{1}^{n}(\mathbf{x})\right), \quad L_{1}^{n}(\mathbf{x})=B\left(R_{1}^{n}(\mathbf{x})\right) \<br>R_{m}^{n}(\mathbf{x}) &amp;=\left{\begin{array}{ll}<br>T_{m}(\mathbf{x}) &amp; \text { if } m=n-1 \<br>T_{m}\left(R_{m+1}^{n}(\mathbf{x})\right) &amp; \text { otherwise }<br>\end{array}\right.<br>\end{aligned}<br>$$</p><h3 id="结构元素"><a href="#结构元素" class="headerlink" title="结构元素"></a>结构元素</h3><p><strong>聚合结点（Aggregation Nodes ）</strong></p><p>其主要功能是组合压缩输入，它通过学习如何选择和投射重要的信息，以在它们的输出中保持与单个输入相同的维度。论文中，IDA都是二分的（两个输入），HDA则根据树结构深度不同有不定量的参数。</p><p>虽然聚合结点可以采用任意结构，不过为了简单起见，文中采用了conv-BN-激活函数的组合。图像分类中所有聚合结点采用1x1卷积，分割任务中，额外一个IDA用来特征插值，此时采用3x3卷积。</p><p>由于残差连接的有效性，本文聚合结点也采用了残差连接，这能保证梯度不会消失和爆炸。基础聚合函数定义如下，其中$\sigma$是非线性激活函数，$w$和$b$是卷积核参数。</p><p>$$<br>N\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}\right)=\sigma\left(\text { BatchNorm }\left(\sum_{i} W_{i} \mathbf{x}_{i}+\mathbf{b}\right)\right)<br>$$</p><p>包含残差连接的聚合函数变为下式。</p><p>$$<br>N\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}\right)=\sigma\left(\text { Batch } \operatorname{Norm}\left(\sum_{i} W_{i} \mathbf{x}<em>{i}+\mathbf{b}\right)+\mathbf{x}</em>{n}\right)<br>$$</p><p><strong>块和层（Blocks and Stages）</strong></p><p>DLA是一系列适用于各种backbone的结构，它对block和stage内部结构没有要求。论文主要实验了三种残差块，分别如下。</p><ul><li>Basic blocks：将堆叠的卷积层通过一个跳跃连接连接起来；</li><li>Bottleneck blocks：通过1x1卷积对堆叠的卷积层进行降维来进行正则化；</li><li>Split blocks：将不同的通道分组到不同的路径（称为cardinality split）来对特征图进行分散。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者设计了分类网络和密集预测网络，前者基于ResNet等修改设计了如下的DLA网络，下表是设计的一系列网络的配置。</p><p><img src="https://i.loli.net/2020/10/22/kJOICgQwhpxYnKD.png"></p><p><img src="https://i.loli.net/2020/10/22/xNY5ERprfaWTnMm.png"></p><p>DLA网络在Imagenet上对比其他的紧凑网络，结果如下，从精度和参数量来看，性能都是卓越的。</p><p><img src="https://i.loli.net/2020/10/22/U4E2mg5ZM8GITuy.png"></p><p>作者还在检测等任务上也做了实验，这里我就不分析了，感兴趣的可以去阅读原论文。</p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>在很多人还在致力于研究如何设计更深更宽的网络的时候，DLA 想到的问题是如何更好地聚合一个网络中不同层和块的信息，并开创性地提出了 IDA 和 HDA 两种聚合思路。在多类任务上效果都有明显改善，包括分类、分割、细粒度分类等。而且，相比于普通 backbone，DLA 结构的网络参数量更少（并不意味着运算速度快，因为这种跳跃连接结构是很耗内存的），在原始网络上进行 DLA 改进往往能获得更为不错的效果。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DLA论文解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Plotly基础教程</title>
      <link href="/2020/10/09/plotly/"/>
      <url>/2020/10/09/plotly/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Plotly 是一个非常强大的开源数据可视化框架，它通过构建基于 HTML 的交互式图表来显示信息，可创建各种形式的精美图表。本文所说的 Plotly 指的是 Plotly.js 的 Python 封装，<a href="https://plotly.com/">plotly</a>本身是个生态非常复杂的绘图工具，它对很多编程语言提供接口。交互式和美观易用应该是 Plotly 最大的优势，而 Matplotlib 的特点则是可定制化程度高，但语法也相对难学，各有优缺点。</p><h2 id="安装及开发工具"><a href="#安装及开发工具" class="headerlink" title="安装及开发工具"></a>安装及开发工具</h2><p>安装通过 PIP 进行即可。</p><p><code>pip install plotly</code></p><p>Plotly Python 其对应的<a href="https://plotly.com/python/">官网</a>，上面有一些教程和官方API接口的查询。</p><p><img src="https://i.loli.net/2020/10/09/BHfGe7nIWZ49oVt.png"></p><p><strong>上面说了 Plotly 是基于 HTML 显示的，所以这里推荐使用 Jupyter lab（Jupyter notebook 也行）作为开发工具，Jupyter lab 的安装本文不多提及，可以自行查找。尤其注意的是，Plotly 主要维护 Jupyter notebook，所以对 Jupyter lab 支持不是很好，绘图无法显示，最新版 Plotly 需要通过命令<code>conda install nodejs</code>和<code>jupyter labextension install jupyterlab-plotly@4.11.0</code>安装支持插件。</strong></p><h2 id="Plotly-生态"><a href="#Plotly-生态" class="headerlink" title="Plotly 生态"></a>Plotly 生态</h2><ul><li>Plotly 是绘图基础库，它可以深度定制调整绘图，但是 API 复杂学习成本较高。</li><li>Plotly_exprress 则是对 Plotly 的高级封装，上手容易，它对 Plotly 的常用绘图函数进行了封装。缺点是没有 plotly 那样自由度高，个人感觉类似 Seaborn 和 Matplotlib 的关系。<strong>本文不以express为主。</strong></li><li>Dash 用于创建交互式绘图工具，可以方便地用它来探索数据，其绘图基于 Plotly。使用 Dash 需要注册并购买套餐，也就是常说的“在线模式”，一般，我们在 Jupyter 内本地绘图就够用了，这是“离线模式”。</li></ul><h2 id="绘图教程"><a href="#绘图教程" class="headerlink" title="绘图教程"></a>绘图教程</h2><p>下面涉及到的内容均可以在<a href="https://plotly.com/python/">官方文档</a>找到参考，下面的内容也只涉及基础的图形绘制（使用Plotly实现），一些比较基础的图形库知识查看<a href="https://plotly.com/python/plotly-fundamentals/">对应教程</a>。</p><h3 id="基本图表"><a href="#基本图表" class="headerlink" title="基本图表"></a>基本图表</h3><p>在 Plotly 中，预定义了如下的一些基本图表，包括散点图、折线图、柱状图、饼图等，它们的使用方式都是类似的，通过向Figure上添加绘图对象进行绘图，而向绘图对象传递的就是其需要的格式的数据。</p><p><img src="https://i.loli.net/2020/10/09/HDkcCSLxO1Q83eo.png"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npN <span class="token operator">=</span> <span class="token number">1000</span>t <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>t<span class="token punctuation">)</span>fig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span>data<span class="token operator">=</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>t<span class="token punctuation">,</span> y<span class="token operator">=</span>y<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>下面的代码就是简单散点图的绘制，其在Jupyter中的执行结果如下图，不妨来仔细看看这张图，<strong>在绘制的图形右上角有一行菜单栏且这个图形是可交互的</strong>（包括缩放、旋转、裁剪等），右上角的菜单包括图像下载、缩放、裁剪、在dash中编辑等。</p><p><img src="https://i.loli.net/2020/10/09/VrR52bqpQA4yzea.png"></p><p>也可以通过<code>add_trace</code>来逐个添加绘图对象</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token comment" spellcheck="true"># Create random data with numpy</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npnp<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>N <span class="token operator">=</span> <span class="token number">100</span>random_x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span>random_y0 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">5</span>random_y1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">)</span>random_y2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">5</span>fig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Add traces</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>random_x<span class="token punctuation">,</span> y<span class="token operator">=</span>random_y0<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>random_x<span class="token punctuation">,</span> y<span class="token operator">=</span>random_y1<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'lines+markers'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'lines+markers'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>random_x<span class="token punctuation">,</span> y<span class="token operator">=</span>random_y2<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'lines'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'lines'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2020/10/09/2n6AJes5rNKqtiH.png"></p><p>其他基本图表类似，传入指定格式的数据即可。</p><h3 id="统计图表"><a href="#统计图表" class="headerlink" title="统计图表"></a>统计图表</h3><p>很多统计学图表也预先定义在了Plotly中，主要包括下图所示的箱型图、直方图、热力图、等高线图等。</p><p><img src="https://i.loli.net/2020/10/09/GpthdgN5PvaOEyz.png"></p><p>和上面的基本图表类似，绘图方式是固定的，只是绘图对象改变了而已。下面的代码就是直方图绘制</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> gofig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Bar<span class="token punctuation">(</span>    name<span class="token operator">=</span><span class="token string">'Control'</span><span class="token punctuation">,</span>    x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Trial 1'</span><span class="token punctuation">,</span> <span class="token string">'Trial 2'</span><span class="token punctuation">,</span> <span class="token string">'Trial 3'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    error_y<span class="token operator">=</span>dict<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span> array<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Bar<span class="token punctuation">(</span>    name<span class="token operator">=</span><span class="token string">'Experimental'</span><span class="token punctuation">,</span>    x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Trial 1'</span><span class="token punctuation">,</span> <span class="token string">'Trial 2'</span><span class="token punctuation">,</span> <span class="token string">'Trial 3'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    error_y<span class="token operator">=</span>dict<span class="token punctuation">(</span>type<span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span> array<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>update_layout<span class="token punctuation">(</span>barmode<span class="token operator">=</span><span class="token string">'group'</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2020/10/09/rwXYOZnCmokGjaI.png"></p><h3 id="AI图表"><a href="#AI图表" class="headerlink" title="AI图表"></a>AI图表</h3><p>同时，Plotly也支持绘制一些简单的机器学习图表，不过都是依靠上面的基本图表实现的，如下述的线性回归。</p><p><img src="https://i.loli.net/2020/10/09/UIqydzwFvbRK5Q4.png"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> plotly<span class="token punctuation">.</span>express <span class="token keyword">as</span> px<span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegressiondf <span class="token operator">=</span> px<span class="token punctuation">.</span>data<span class="token punctuation">.</span>tips<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> df<span class="token punctuation">.</span>total_bill<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>model <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> df<span class="token punctuation">.</span>tip<span class="token punctuation">)</span>x_range <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>X<span class="token punctuation">.</span>min<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>max<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>y_range <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_range<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig <span class="token operator">=</span> px<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>df<span class="token punctuation">,</span> x<span class="token operator">=</span><span class="token string">'total_bill'</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">'tip'</span><span class="token punctuation">,</span> opacity<span class="token operator">=</span><span class="token number">0.65</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_traces<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>x_range<span class="token punctuation">,</span> y<span class="token operator">=</span>y_range<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Regression Fit'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2020/10/09/Q12kFUBOcmKgqX5.png"></p><h3 id="科学绘图"><a href="#科学绘图" class="headerlink" title="科学绘图"></a>科学绘图</h3><p>下面的这些数据科学领域用的挺多的图也做了封装，例如下面的代码就是绘制heatmap的样例。</p><p><img src="https://i.loli.net/2020/10/09/2tSgERp8NkrxJDi.png"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> gofig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span>data<span class="token operator">=</span>go<span class="token punctuation">.</span>Heatmap<span class="token punctuation">(</span>                   z<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> None<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Monday'</span><span class="token punctuation">,</span> <span class="token string">'Tuesday'</span><span class="token punctuation">,</span> <span class="token string">'Wednesday'</span><span class="token punctuation">,</span> <span class="token string">'Thursday'</span><span class="token punctuation">,</span> <span class="token string">'Friday'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Morning'</span><span class="token punctuation">,</span> <span class="token string">'Afternoon'</span><span class="token punctuation">,</span> <span class="token string">'Evening'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   hoverongaps <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2020/10/09/ElyNPz96JOrmnuF.png"></p><h3 id="三维绘图"><a href="#三维绘图" class="headerlink" title="三维绘图"></a>三维绘图</h3><p>Plotly的三维绘图真的很好看，而且其是可交互的，非常方便，例如下面的3D曲面图。</p><p><img src="https://i.loli.net/2020/10/09/aP16hAGqZrT4C8p.png"></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token comment" spellcheck="true"># Read data from a csv</span>z_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'https://raw.githubusercontent.com/plotly/datasets/master/api_docs/mt_bruno_elevation.csv'</span><span class="token punctuation">)</span>fig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span>data<span class="token operator">=</span><span class="token punctuation">[</span>go<span class="token punctuation">.</span>Surface<span class="token punctuation">(</span>z<span class="token operator">=</span>z_data<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>update_layout<span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">,</span> autosize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                  width<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> height<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>                  margin<span class="token operator">=</span>dict<span class="token punctuation">(</span>l<span class="token operator">=</span><span class="token number">65</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token number">65</span><span class="token punctuation">,</span> t<span class="token operator">=</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2020/10/09/DT2Oz5plMsvkCFJ.png"></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文简单介绍了Plotly的基本绘图方式，其实只要了解了Plotly的生态，使用它并不难，更多的子图、标注等技巧本文没有涉及，还是建议到<a href="https://plotly.com/python/">官网教程</a>查看，非常易读。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Plotly教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BorderDet 论文解读</title>
      <link href="/2020/10/04/borderdet/"/>
      <url>/2020/10/04/borderdet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>目前密集目标检测器很受欢迎，其速度很快且精度不低，不过这种这种基于点的特征虽然使用方便，但会缺少关键的边界信息。旷视于 ECCV2020 发表的这篇 BorderDet，其中的核心就是设计了 Border Align 操作来从边界极限点提取边界特征用于加强点的特征。以此为基础设计了 BorderDet 框架，该框架依据 FCOS 的 baseline 插入 Border Align 构成，其在多个数据集上涨点明显。Border Align 是适用于几乎所有基于点的密集目标检测算法的即插即用模块。</p><ul><li><p>论文地址</p><p><a href="https://arxiv.org/abs/2007.11056">https://arxiv.org/abs/2007.11056</a></p></li><li><p>论文源码</p><p><a href="https://github.com/Megvii-BaseDetection/BorderDet">https://github.com/Megvii-BaseDetection/BorderDet</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>目前大多数 point-based 的目标检测算法（如 SSD、RetinaNet、FCOS 等方法）都使用特征图上的 single-point 进行目标的回归和分类，但是，single-point 特征没有足够的信息表示一个目标实例，主要是因为缺乏边界信息。此前有很多方法来补充 single-point 的表示能力，但是这些方法往往带来较大计算量的同时并没有引入太多有用的信息，反而带来一些无用的背景信息。这篇文章设计了新的特征提取操作 BorderAlign 来直接利用边界特征优化 single-point 特征，以 BorderAlign 为拓展配合作为 baseline 的 FCOS，提出新的检测框架 BorderDet，实现 SOTA。</p><p>本文的贡献文中列了不少，但在我看来，只有一个核心：<strong>分析密集目标检测器的特征表示，发现边界信息对 single-point 特征的重要性，并设计了一个高效的边界特征提取器 BorderAlign。</strong> 其他的贡献都是顺理成章的附属产物。</p><h2 id="BorderAlign"><a href="#BorderAlign" class="headerlink" title="BorderAlign"></a>BorderAlign</h2><p><img src="https://i.loli.net/2020/10/04/BAv3C6PYKR17f4c.png"></p><p>BorderAlign 的提出是基于大量的实验对比的，我这边就按照作者的思路来进行阐述。首先，采用如上图不同的特征增强方式在 FCOS 的基础上评估效果，结果如下表，根据效果最好的二四两行，发现，只使用边界上中心点做增强效果媲美 region-based 的方法。因此，得出结论，<strong>point-based 方法做目标检测确实缺乏完整的目标特征，但从完整的边界框中密集提取特征是没必要且冗余的，高效的边界特征提取策略可以获得更好的特征增强效果。</strong></p><p><img src="https://i.loli.net/2020/10/04/cLVCn1SkRBZXlTK.png"></p><p>针对上述结论，一种高效显式自适应提取边界特征的方法，BorderAlign 被提出。如下图所示，一个$5C$的 border-sensitive 特征图作为输入，其中$4C$维度对应边界框的四条边，另外$C$维度对应原始 anchor 点的特征。对于一个 anchor 点预测的边界框，对其四个边界在特征图上的特征做池化操作，由于框的位置是小数，所以采用双线性插值取边界特征。</p><p>这里具体的实现如下：假设输入的 5 个通道表示(single point, left border, top border, right border, bottom border)，那么对 anchor 点$(i, j)$对应的 bbox 各边均匀采样$N$个点，$N$默认是 5，如下图所示。采样点的值采用上面所说的双线性插值，然后通过逐通道最大池化得到输出，每个边只会输出值最大的采样点，那么每个 anchor 点最后采用 5 个点的特征作为输出，所以输出也是$5C$维度的。</p><p><img src="https://i.loli.net/2020/10/04/GvkmthPsnaNMyWg.png"></p><p>输出特征图相对输入特征图，各通道计算式如下，$(x_0, y_0, x_!, y_1)$为 anchor 点预测的 bbox。</p><p><img src="https://i.loli.net/2020/10/04/UqBdCvYag6FJi7R.png"></p><p>显然，BorderAlign 是一种自适应的通过边界极限点得到边界特征的方法。文章中对其进行了一些可视化工作，下图所示的边上的小圆圈是边界极限点，大圆圈是不同 channel 上预测的边界极限点。</p><p><img src="https://i.loli.net/2020/10/04/JwlLQHDmBPtZvsE.png"></p><h2 id="BAM-Border-Alignment-Module"><a href="#BAM-Border-Alignment-Module" class="headerlink" title="BAM(Border Alignment Module)"></a>BAM(Border Alignment Module)</h2><p><img src="https://i.loli.net/2020/10/04/d1p4NU2JvhKrEVZ.png"></p><p>该模块用于修正粗糙的 detection 结果，因而必须保证输入输出是同维张量，而其中的 BorderAlign 需求的是 5 个通道，所以必然要经历<strong>降维、特征增强、升维</strong>的过程，为了验证 border feature 的效果，BAM 采用 1x1 卷积实现维度变换。</p><h2 id="BorderDet"><a href="#BorderDet" class="headerlink" title="BorderDet"></a>BorderDet</h2><p><img src="https://i.loli.net/2020/10/04/8QERAPL2jxT3zYm.png"></p><p>上图的框架采用 FCOS 作为 baseline，上面是分类分支，下面是回归分支，coarse cls score 和 coarse box reg 表示 FCOS 的输出。在四个卷积层后引出一个分支做 BorderAlign 操作，也就是进入 BAM 模块，该模块需要 bbox 位置信息，所以看到 coarse box reg 送入两个 BAM 中。最终这两个 BAM 预测得到 border cls score 和 border box reg，和检测器原始输出组合变为最终输出。</p><p>最后补充一点，BorderDet 在推理时对两种分类结果进行直接的相乘输出，而对于 bbox 定位则使用 border 定位预测对初步定位的 bbox 进行原论文中公式(2)的反向转换，对所有的结果进行 NMS 输出（IOU 阈值设置为 0.6）。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文进行了非常丰富的消融实验以对比 BorderAlign 的效果。</p><h3 id="各分支效果"><a href="#各分支效果" class="headerlink" title="各分支效果"></a>各分支效果</h3><p>对比两个 BAM 模块有无的实际效果，发现两个分支效果提升差不多，都是 1.1 左右。<br><img src="https://i.loli.net/2020/10/04/bGoiQs1RwuaUyFS.png"></p><h3 id="相比其他特征增强效果"><a href="#相比其他特征增强效果" class="headerlink" title="相比其他特征增强效果"></a>相比其他特征增强效果</h3><p>和其他经典的特征增强手段相比，BorderAlign 在速度（使用 CUDA 实现了 BorderAlign）和精度上都有突破。<br><img src="https://i.loli.net/2020/10/04/PKk3SOw6Np2IoM8.png"></p><h3 id="集成到检测器涨点效果"><a href="#集成到检测器涨点效果" class="headerlink" title="集成到检测器涨点效果"></a>集成到检测器涨点效果</h3><p>有比较明显的改进。<br><img src="https://i.loli.net/2020/10/04/tZvEcdHCjyYMqBw.png"></p><h3 id="和主流检测器对比"><a href="#和主流检测器对比" class="headerlink" title="和主流检测器对比"></a>和主流检测器对比</h3><p>可以看到，即使不使用多尺度策略，BorderDet 和当前 SOTA 相比效果也是不遑多让的。<br><img src="https://i.loli.net/2020/10/04/DmlrKSEqVpw4nhI.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>边界信息对于 OD 问题十分重要，BorderDet 的核心思想 BorderAlign 高效地将边界特征融入到目标预测中，而且能够 PnP 融入到各种 point-based 目标检测算法中以带来较大的性能提升。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Qiu H, Ma Y, Li Z, et al. BorderDet: Border Feature for Dense Object Detection[J]. arXiv preprint arXiv:2007.11056, 2020.</p><p>[2]<a href="https://zhuanlan.zhihu.com/p/163044323">https://zhuanlan.zhihu.com/p/163044323</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BorderDet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matplotlib绘制动图</title>
      <link href="/2020/09/21/dynamic-picture/"/>
      <url>/2020/09/21/dynamic-picture/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Matplotlib是非常著名的Python绘图库，支持非常复杂的底层定制化操作。本文通过Matplotlib中的动画绘制工具来讲解如何绘制动态图，首先讲解通过交互模式如何显示动态图，继而讲解通过两个动画类来实现动图地保存（GIF格式）。</p><h2 id="显示动态图"><a href="#显示动态图" class="headerlink" title="显示动态图"></a>显示动态图</h2><p>首先，需要明确，Matplotlib绘图有两种显示模式，分别为<strong>阻塞模式</strong>和<strong>交互模式</strong>，他们具体的说明如下。</p><ol><li>阻塞模式，该模式下绘制地图地显示必须使用<code>plt.show()</code>进行展示（默认会弹出一个窗口），代码会运行到该行阻塞不继续执行，直到关闭这个展示（默认是关闭弹出的显示窗口，Pycharm等集成开发环境会自动捕获图片然后跳过阻塞）。</li><li>交互模式，该模式下任何绘图相关的操作如<code>plt.plot()</code>会立即显示绘制的图形然后迅速关闭，继续代码的运行，不发生阻塞。</li></ol><p>默认情况下，Matplotlib使用阻塞模式，要想打开交互模式需要通过下面的几个函数来做操作，下面直接列出要用的核心函数。</p><pre class=" language-python"><code class="language-python">plt<span class="token punctuation">.</span>ion<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打开交互模式</span>plt<span class="token punctuation">.</span>ioff<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 关闭交互模式</span>plt<span class="token punctuation">.</span>clf<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 清除当前的Figure对象</span>plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 暂停GUI功能多少秒</span></code></pre><p>然后就是要清楚，所谓的动图或者视频是怎么做到的，其实它们本质上就是很多静态图以较快的速度连续播放从而给人一种动感，利用Matplotlib绘制动图的原理也是一样的，遵循<code>画布绘图</code>-&gt;<code>清理画布</code>-&gt;<code>画布绘图</code>的循环就行了，不过这里注意，由于交互模式下绘图都是一闪而过，因此<strong>通过<code>plt.pause(n)</code>暂停GUI显示n秒才能得到连续有显示的图像</strong>。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">io_test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 生成画布</span>    plt<span class="token punctuation">.</span>ion<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打开交互模式</span>    <span class="token keyword">for</span> index <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        fig<span class="token punctuation">.</span>clf<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 清空当前Figure对象</span>        fig<span class="token punctuation">.</span>suptitle<span class="token punctuation">(</span><span class="token string">"3d io pic"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 生成数据</span>        point_count <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment" spellcheck="true"># 随机生成100个点</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        z <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        color <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        scale <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span>        ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">,</span> projection<span class="token operator">=</span><span class="token string">"3d"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 绘图</span>        ax<span class="token punctuation">.</span>scatter3D<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> s<span class="token operator">=</span>scale<span class="token punctuation">,</span> c<span class="token operator">=</span>color<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">"o"</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"X"</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Y"</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>set_zlabel<span class="token punctuation">(</span><span class="token string">"Z"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 暂停</span>        plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 关闭交互模式</span>    plt<span class="token punctuation">.</span>ioff<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    io_test<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>上述代码演示了三维空间如何动态显示100个随机点的变化，使用录制软件得到的动图如下，其本质就是不停显示不同的图像而已。</p><p><img src="https://i.loli.net/2020/09/21/lyQHWBXOGv6KSZ3.gif"></p><h2 id="动图保存"><a href="#动图保存" class="headerlink" title="动图保存"></a>动图保存</h2><p>很多时候我们的需求并不是在窗口中动态显示图像，还需要保存到本地GIF图像，显然使用录制工具是一个比较低效的用法，Matplotlib的<code>animation</code>模块提供了两个动画绘制接口，分别是<strong>FuncAnimation</strong>和<strong>ArtistAnimation</strong>，它们都是继承自<code>TimedAnimation</code>的类，因而也具有<code>Animation</code>对象的通用方法，如<code>Animation.save()</code>和<code>Animation.to_html5_video()</code>两个方法实例化一个<code>Animation</code>对象后均可调用，前者表示将动画保存为一个图像，后者表示将动画表示为一个HTML视频。</p><ul><li>FuncAnimation: 通过反复调用同一更新函数来制作动画。</li><li>ArtistAnimation: 通过调用一个固定的Artist对象来制作动画，例如给定的图片序列或者Matplotlib的绘图对象。</li></ul><p>下面给出上述两个类的构造函数所需参数，它们的主要参数也是类似的，都是一个Figure对象作为画布，然后一个对象作为更新的实现方式（前者需要一个反复绘图的更新函数，后者则为一个图像列表或者绘图对象列表）。</p><pre class=" language-python"><code class="language-python">ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>FuncAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> func<span class="token punctuation">,</span> frames<span class="token operator">=</span>None<span class="token punctuation">,</span> init_func<span class="token operator">=</span>None<span class="token punctuation">,</span> fargs<span class="token operator">=</span>None<span class="token punctuation">,</span> save_count<span class="token operator">=</span>None<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> cache_frame_data<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>ArtistAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> artists<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span></code></pre><p>相比较而言，我更喜欢使用<code>FuncAnimation</code>，它的使用要求简洁且定制化程度较高。但是如果想将很多图片合并为一个动图，那么<code>ArtistAnimation</code>是最合适的选择。</p><p>下面的代码演示了如何保存一个动态变化渲染的柱状图，<code>ArtistAnimation</code>传入了一个图像序列，序列中每个元素为绘制的柱状图。然后通过使用<code>Animation</code>的<code>save</code>方法保存了动态图，**需要注意的是，这里有个动画写入器（writer）可以选择，默认不是pillow，我个人觉得pillow安装简单一些。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>animation <span class="token keyword">as</span> animationfig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> tmp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>    y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    temp <span class="token operator">=</span> ax<span class="token punctuation">.</span>bar<span class="token punctuation">(</span>x<span class="token punctuation">,</span> height<span class="token operator">=</span>y<span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>    tmp<span class="token punctuation">.</span>append<span class="token punctuation">(</span>temp<span class="token punctuation">)</span>ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>ArtistAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> tmp<span class="token punctuation">,</span> interval<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> repeat_delay<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>ani<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"bar.gif"</span><span class="token punctuation">,</span> writer<span class="token operator">=</span><span class="token string">'pillow'</span><span class="token punctuation">)</span></code></pre><p>上面代码的执行结果如下图。</p><p><img src="https://i.loli.net/2020/09/21/LHoTe14VU2tqlX9.gif"></p><p>接着，演示使用范围更广的<code>FuncAnimation</code>如何使用。下面的代码中，动态展示了梯度下降在三维图上的优化过程，其中最为核心的代码如下。用于构造<code>Animation</code>对象的除了画布就是一个更新函数，在这个更新函数内部多次绘制散点图从而形成动态效果， <code>frames</code>是帧数，如果设置了这个帧数，那么<code>update</code>函数第一个参数必须有一个<code>num</code>占位，这个<code>num</code>由<code>Animation</code>对象维护，每次内部执行<code>update</code>会自动递增，后面的参数列表<code>fargs</code>只需要传入除了<code>num</code>后面的参数即可。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> ax<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> z<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span>    ax<span class="token punctuation">.</span>scatter3D<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> axani <span class="token operator">=</span> animation<span class="token punctuation">.</span>FuncAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> update<span class="token punctuation">,</span> frames<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> fargs<span class="token operator">=</span><span class="token punctuation">(</span>x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list<span class="token punctuation">,</span> ax3d<span class="token punctuation">)</span><span class="token punctuation">,</span> interval<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> blit<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></code></pre><p>上面的代码演示效果如下图，完整的代码附在文末补充说明中。</p><p><img src="https://i.loli.net/2020/09/21/Kp6qMkuLIt3rFwo.gif"></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文介绍了如何使用Matplotlib绘制动态图，主要通过交互模式和<code>animation</code>模块进行，如果觉得有所帮助，欢迎点赞。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>animation <span class="token keyword">as</span> animation<span class="token keyword">def</span> <span class="token function">GD</span><span class="token punctuation">(</span>x0<span class="token punctuation">,</span> y0<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    f <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">-</span> y <span class="token operator">**</span> <span class="token number">2</span>    g_x <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token number">2</span> <span class="token operator">*</span> x    x<span class="token punctuation">,</span> y <span class="token operator">=</span> x0<span class="token punctuation">,</span> y0    x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>        x_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        z_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>f<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1.01</span><span class="token punctuation">)</span>        grad_x<span class="token punctuation">,</span> grad_y <span class="token operator">=</span> g_x<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> g_x<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        x <span class="token operator">-=</span> lr <span class="token operator">*</span> grad_x        y <span class="token operator">-=</span> lr <span class="token operator">*</span> grad_y        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch&amp;#123;&amp;#125;: grad=&amp;#123;&amp;#125; &amp;#123;&amp;#125;, x=&amp;#123;&amp;#125;"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>i<span class="token punctuation">,</span> grad_x<span class="token punctuation">,</span> grad_y<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> abs<span class="token punctuation">(</span>grad_x<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span> <span class="token operator">and</span> abs<span class="token punctuation">(</span>grad_y<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span>            <span class="token keyword">break</span>    <span class="token keyword">return</span> x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list<span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> ax<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> z<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span>    ax<span class="token punctuation">.</span>scatter3D<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> ax<span class="token keyword">def</span> <span class="token function">draw_gd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    z <span class="token operator">=</span> x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">-</span> y <span class="token operator">**</span> <span class="token number">2</span>    ax3d <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span>projection<span class="token operator">=</span><span class="token string">'3d'</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"X"</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Y"</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>set_zlabel<span class="token punctuation">(</span><span class="token string">"Z"</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>tick_params<span class="token punctuation">(</span>labelsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>plot_surface<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> cstride<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> rstride<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"jet"</span><span class="token punctuation">)</span>    x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list <span class="token operator">=</span> GD<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>    x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x_list<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y_list<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>z_list<span class="token punctuation">)</span>    ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>FuncAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> update<span class="token punctuation">,</span> frames<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> fargs<span class="token operator">=</span><span class="token punctuation">(</span>x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list<span class="token punctuation">,</span> ax3d<span class="token punctuation">)</span><span class="token punctuation">,</span> interval<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> blit<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    ani<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'test.gif'</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    draw_gd<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matplotlib动态图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DY-ReLU论文解读</title>
      <link href="/2020/09/15/dy-relu/"/>
      <url>/2020/09/15/dy-relu/</url>
      
        <content type="html"><![CDATA[<h1 id="Dynamic-ReLU"><a href="#Dynamic-ReLU" class="headerlink" title="Dynamic ReLU"></a>Dynamic ReLU</h1><blockquote><p>其实一直在做论文阅读心得方面的工作，只是一直没有分享出来，这篇文章可以说是这个前沿论文解读系列的第一篇文章，希望能坚持下来。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>论文提出了动态线性修正单元（Dynamic Relu，下文简称 DY-ReLU），它能够依据输入动态调整对应分段函数，与 ReLU 及其静态变种相比，仅仅需要增加一些可以忽略不计的参数就可以带来大幅的性能提升，它可以无缝嵌入已有的主流模型中，在轻量级模型（如 MobileNetV2）上效果更加明显。</p><ul><li><p>论文地址</p><p><a href="http://arxiv.org/abs/2003.10027">http://arxiv.org/abs/2003.10027</a></p></li><li><p>论文源码</p><p><a href="https://github.com/Islanna/DynamicReLU">https://github.com/Islanna/DynamicReLU</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>ReLU 在深度学习的发展中地位举足轻重，它简单而且高效，极大地提高了深度网络的性能，被很多 CV 任务的经典网络使用。不过 ReLU 及其变种（无参数的 leaky ReLU 和有参数的 PReLU）都是静态的，也就是说他们最终的参数都是固定的。<strong>那么自然会引发一个问题，能否根据输入的数据动态调整 ReLU 的参数呢？</strong></p><p><img src="https://i.loli.net/2020/09/15/NHuT5iLaxc4Drbo.png"></p><p>针对上述问题，论文提出了 DY-ReLU，它是一个分段函数$f_{\boldsymbol{\theta}(\boldsymbol{x})}(\boldsymbol{x})$，其参数由超函数$\boldsymbol{\theta {(x)}}$根据$x$计算得到。如上图所示，输入$x$在进入激活函数前分成两个流分别输入$\boldsymbol{\theta {(x)}}$和$f_{\boldsymbol{\theta}(\boldsymbol{x})}(\boldsymbol{x})$，前者用于获得激活函数的参数，后者用于获得激活函数的输出值。超函数$\boldsymbol{\theta {(x)}}$能够编码输入$x$的各个维度（对卷积网络而言，这里指的就是通道，所以原文采用 c 来标记）的全局上下文信息来自适应激活函数$f_{\boldsymbol{\theta}(\boldsymbol{x})}(\boldsymbol{x})$。</p><p>该设计能够在引入极少量的参数的情况下大大增强网络的表示能力，本文对于空间和通道上不同的共享机制设计了三种 DY-ReLU，分别是 DY-ReLU-A、DY-ReLU-B 以及 DY-ReLU-C。</p><h2 id="Dynamic-ReLU-1"><a href="#Dynamic-ReLU-1" class="headerlink" title="Dynamic ReLU"></a>Dynamic ReLU</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><img src="https://i.loli.net/2020/09/15/MKfwy57uRIHxFs3.png"></p><p>原始的 ReLU 为$\boldsymbol{y}=\max {\boldsymbol{x}, 0}$，这是一个非常简单的分段函数。对于输入向量$x$的第$c$个通道的输入$x_c$，对应的激活函数可以记为$y_{c}=\max \left{x_{c}, 0\right}$。进而，ReLU 可以统一表示为带参分段线性函数$y_{c}=\max <em>{k}\left{a</em>{c}^{k} x_{c}+b_{c}^{k}\right}$，基于此提出下式动态 ReLU 来针对$\boldsymbol{x}=\left{x_{c}\right}$自适应$a_c^k$和$b_c^k$。</p><p>$$y_{c}=f_{\boldsymbol{\theta}(\boldsymbol{x})}\left(x_{c}\right)=\max <em>{1 \leq k \leq K}\left{a</em>{c}^{k}(\boldsymbol{x}) x_{c}+b_{c}^{k}(\boldsymbol{x})\right}$$</p><p>系数$\left(a_{c}^{k}, b_{c}^{k}\right)$由超函数$\boldsymbol{\theta (x)}$计算得到，具体如下，其中$K$为函数的数目，$C$为通道数目。且参数$\left(a_{c}^{k}, b_{c}^{k}\right)$不仅仅与$x_c$有关，还和$x_{j \neq c}$有关。</p><p>$\left[a_{1}^{1}, \ldots, a_{C}^{1}, \ldots, a_{1}^{K}, \ldots, a_{C}^{K}, b_{1}^{1}, \ldots, b_{C}^{1}, \ldots, b_{1}^{K}, \ldots, b_{C}^{K}\right]^{T}=\boldsymbol{\theta}(\boldsymbol{x})$</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>DY-ReLU 的核心超函数$\boldsymbol{\theta {(x)}}$的实现采用 SE 模块（SENet 提出的）实现，对于维度为$C \times H \times W$的张量输入，首先通过一个全局池化层压缩空间信息，然后经过两个中间夹着一个 ReLU 的全连接层，最后一个标准化层用于标准化输出的范围在$(-1,1)$之间（采用 sigmoid<br>函数）。该模块最终输出$2KC$个元素，分别是$a_{1: C}^{1: K}$和$b_{1: C}^{1: K}$的残差，记为$\Delta a_{1: C}^{1: K}$和$\Delta b_{1: C}^{1: K}$，最后的输出为初始值和残差的加权和，计算式如下。</p><p>$$a_{c}^{k}(\boldsymbol{x})=\alpha^{k}+\lambda_{a} \Delta a_{c}^{k}(\boldsymbol{x}), b_{c}^{k}(\boldsymbol{x})=\beta^{k}+\lambda_{b} \Delta b_{c}^{k}(\boldsymbol{x})$$</p><p>其中，$\alpha^k$和$\beta^k$分别为$a_c^k$和$b_c^k$的初始值，$\lambda_a$和$\lambda_b$为残差范围控制标量，也就是加的权。$\alpha^k$和$\beta^k$以及$\lambda_a$、$\lambda_b$都是超参数。若$K=2$，有$\alpha^{1}=1, \alpha^{2}=\beta^{1}=\beta^{2}=0$，这就是原始 ReLU。默认的$\lambda_a$和$\lambda_b$分别为 1.0 和 0.5。</p><p><img src="https://i.loli.net/2020/09/15/Z9FVGdh1jJiavCy.png"></p><p>对于学习到不同的参数，DY-ReLU 会有不同的形式，它可以等价于 ReLU、Leaky ReLU 和 PReLU，也可以等价于 SE 模块或者 Maxout 算子，至于具体的形式依据输入而改变，是一种非常灵活的动态激活函数。</p><h3 id="变种设计"><a href="#变种设计" class="headerlink" title="变种设计"></a>变种设计</h3><p>主要提出三种不同的 DY-ReLU 设计，分别是 DY-ReLU-A、DY-ReLU-B 以及 DY-ReLU-C。DY-ReLU-A 空间和通道均共享，只会输出$2K$个参数，计算简单，表达能力较弱；DY-ReLU-B 仅空间上共享，输出$2KC$个参数；DY-ReLU-C 空间和通道均不共享，参数量极大。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>经过对比实验得出 DY-ReLU-B 更适合图像分类，DY-ReLU-C 更适合关键点检测任务，在几个典型网络上改用论文提出的 DY-ReLU，效果如下图，不难发现，在轻量级网络上突破较大。</p><p><img src="https://i.loli.net/2020/09/15/LaHep6XgyjnOFkZ.png"></p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>下面是 DY-ReLU-B 的 Pytorch 实现。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">DyReLU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> conv_type<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>DyReLU<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels        self<span class="token punctuation">.</span>k <span class="token operator">=</span> k        self<span class="token punctuation">.</span>conv_type <span class="token operator">=</span> conv_type        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>conv_type <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'1d'</span><span class="token punctuation">,</span> <span class="token string">'2d'</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels <span class="token operator">//</span> reduction<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channels <span class="token operator">//</span> reduction<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>k<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'lambdas'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token operator">*</span>k <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token operator">*</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'init_v'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>k <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_relu_coefs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv_type <span class="token operator">==</span> <span class="token string">'2d'</span><span class="token punctuation">:</span>            theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>theta<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>        theta <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>theta<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>        <span class="token keyword">return</span> theta    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> NotImplementedError<span class="token keyword">class</span> <span class="token class-name">DyReLUB</span><span class="token punctuation">(</span>DyReLU<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> conv_type<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>DyReLUB<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> reduction<span class="token punctuation">,</span> k<span class="token punctuation">,</span> conv_type<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channels <span class="token operator">//</span> reduction<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>k<span class="token operator">*</span>channels<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>channels        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>get_relu_coefs<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        relu_coefs <span class="token operator">=</span> theta<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>self<span class="token punctuation">.</span>k<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>lambdas <span class="token operator">+</span> self<span class="token punctuation">.</span>init_v        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv_type <span class="token operator">==</span> <span class="token string">'1d'</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># BxCxL -> LxBxCx1</span>            x_perm <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            output <span class="token operator">=</span> x_perm <span class="token operator">*</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k<span class="token punctuation">:</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># LxBxCx2 -> BxCxL</span>            result <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conv_type <span class="token operator">==</span> <span class="token string">'2d'</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># BxCxHxW -> HxWxBxCx1</span>            x_perm <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            output <span class="token operator">=</span> x_perm <span class="token operator">*</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k<span class="token punctuation">:</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># HxWxBxCx2 -> BxCxHxW</span>            result <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> result</code></pre><p>这个结构和上文我所说的 SE 模块是大体对应的，目前支持一维和二维卷积，要想使用只需要像下面这样替换激活层即可（DY-ReLU 需要指定输入通道数目和卷积类型）。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> dyrelu <span class="token keyword">import</span> DyReluB<span class="token keyword">class</span> <span class="token class-name">Model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Model<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> DyReLUB<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> conv_type<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x</code></pre><blockquote><p>有空的话我会在 MobileNet 和 ResNet 上具体实验，看看实际效果是否如论文所述。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文提出了 DY-ReLU，能够根据输入动态地调整激活函数，与 ReLU 及其变种对比，仅需额外的少量计算即可带来大幅的性能提升，能无缝嵌入到当前的主流模型中，是一个涨点利器。本质上，DY-ReLU 就是各种 ReLU 的数学归纳和拓展，这对后来激活函数的研究有指导意义。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Chen Y, Dai X, Liu M, et al. Dynamic ReLU[J]. arXiv:2003.10027 [cs], 2020.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dynamic ReLU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸表情识别</title>
      <link href="/2020/08/25/facial-expression-recognition/"/>
      <url>/2020/08/25/facial-expression-recognition/</url>
      
        <content type="html"><![CDATA[<h1 id="人脸表情识别"><a href="#人脸表情识别" class="headerlink" title="人脸表情识别"></a>人脸表情识别</h1><blockquote><p>2020.8.22，重构了整个仓库代码，改用Tensorflow2中的keras api实现整个系统。考虑到很多反映jupyter notebook写的train使用起来不太方便，这里改成了py脚本实现。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>使用卷积神经网络构建整个系统，在尝试了Gabor、LBP等传统人脸特征提取方式基础上，深度模型效果显著。在FER2013、JAFFE和CK+三个表情识别数据集上进行模型评估。</p><h2 id="环境部署"><a href="#环境部署" class="headerlink" title="环境部署"></a>环境部署</h2><p>基于Python3和Keras2（TensorFlow后端），具体依赖安装如下(推荐使用conda虚拟环境)。</p><pre class=" language-shell"><code class="language-shell">git clone https://github.com/luanshiyinyang/FacialExpressionRecognition.gitcd FacialExpressionRecognitionconda create -n FER python=3.6source activate FERconda install cudatoolkit=10.1conda install cudnn=7.6.5pip install -r requirements.txt</code></pre><p>如果你是Linux用户，直接执行根目录下的<code>env.sh</code>即可一键配置环境，执行命令为<code>bash env.sh</code>。</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>数据集和预训练模型均已经上传到百度网盘，<a href="https://pan.baidu.com/s/1LFu52XTMBdsTSQjMIPYWnw">链接</a>给出，提取密码为2pmd。下载后将<code>model .zip</code>移动到根目录下的<code>models</code>文件夹下并解压得到一个<code>*.h5</code>的模型参数文件，将<code>data.zip</code>移动到根目录下的<code>dataset</code>文件夹下并解压得到包含多个数据集压缩文件，均解压即可得到包含图像的数据集（<strong>其中rar后缀的为原始jaffe数据集，这里建议使用我处理好的</strong>）。</p><h2 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a><strong>传统方法</strong></h3><ul><li>数据预处理<ul><li>图片降噪</li><li>人脸检测（HAAR分类器检测（opencv））</li></ul></li><li>特征工程<ul><li>人脸特征提取<ul><li>LBP</li><li>Gabor</li></ul></li></ul></li><li>分类器<ul><li>SVM<h3 id="深度方法"><a href="#深度方法" class="headerlink" title="深度方法"></a><strong>深度方法</strong></h3></li></ul></li><li>人脸检测<ul><li>HAAR分类器</li><li>MTCNN（效果更好）</li></ul></li><li>卷积神经网络<ul><li>用于特征提取+分类</li></ul></li></ul><h2 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h2><p>使用经典的卷积神经网络，模型的构建主要参考2018年CVPR几篇论文以及谷歌的Going Deeper设计如下网络结构，输入层后加入(1,1)卷积层增加非线性表示且模型层次较浅，参数较少（大量参数集中在全连接层）。<br><img src="https://i.loli.net/2020/08/25/TtcFkPSm3vgZbME.png"></p><p><img src="https://i.loli.net/2020/08/25/faKs6yzbLciUvxm.png"></p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>主要在FER2013、JAFFE、CK+上进行训练，JAFFE给出的是半身图因此做了人脸检测。最后在FER2013上Pub Test和Pri Test均达到67%左右准确率（该数据集爬虫采集存在标签错误、水印、动画图片等问题），JAFFE和CK+5折交叉验证均达到99%左右准确率（这两个数据集为实验室采集，较为准确标准）。</p><p>执行下面的命令将在指定的数据集（fer2013或jaffe或ck+）上按照指定的batch_size训练指定的轮次。训练会生成对应的可视化训练过程，下图为在三个数据集上训练过程的共同绘图。</p><pre class=" language-shell"><code class="language-shell">python src/train.py --dataset fer2013 --epochs 300 --batch_size 32</code></pre><p><img src="https://i.loli.net/2020/08/25/urIbfVFaGiz5EdW.png"></p><h2 id="模型应用"><a href="#模型应用" class="headerlink" title="模型应用"></a>模型应用</h2><p>与传统方法相比，卷积神经网络表现更好，使用该模型构建识别系统，提供<strong>GUI界面和摄像头实时检测</strong>（摄像必须保证补光足够）。预测时对一张图片进行水平翻转、偏转15度、平移等增广得到多个概率分布，将这些概率分布加权求和得到最后的概率分布，此时概率最大的作为标签（也就是使用了推理数据增强）。</p><h3 id="GUI界面"><a href="#GUI界面" class="headerlink" title="GUI界面"></a><strong>GUI界面</strong></h3><p>注意，<strong>GUI界面预测只显示最可能是人脸的那个脸表情，但是对所有检测到的人脸都会框定预测结果并在图片上标记，标记后的图片在output目录下。</strong></p><p>执行下面的命令即可打开GUI程序，该程序依赖PyQT设计，在一个测试图片（来源于网络）上进行测试效果如下图。</p><pre class=" language-shell"><code class="language-shell">python src/gui.py</code></pre><p><img src="https://i.loli.net/2020/08/25/f9VxPwk1dGJ8NKC.png"></p><p>上图的GUI反馈的同时，会对图片上每个人脸进行检测并表情识别，处理后如下图。</p><p><img src="https://i.loli.net/2020/08/25/BmxSpOt1X4RVEUe.png"></p><h3 id="实时检测"><a href="#实时检测" class="headerlink" title="实时检测"></a><strong>实时检测</strong></h3><p>实时检测基于Opencv进行设计，旨在用摄像头对实时视频流进行预测，同时考虑到有些人的反馈，当没有摄像头想通过视频进行测试则修改命令行参数即可。</p><p>使用下面的命令会打开摄像头进行实时检测（ESC键退出），若要指定视频进行进行检测，则使用下面的第二个命令。</p><pre class=" language-shell"><code class="language-shell">python src/recognition_camera.py</code></pre><pre class=" language-shell"><code class="language-shell">python src/recognition_camera.py --source 1 --video_path 视频绝对路径或者相对于该项目的根目录的相对路径</code></pre><p>下图是动态演示的在Youtube上<a href="https://www.youtube.com/watch?v=r5Z741PC9_c">某个视频</a>上的识别结果。</p><p><img src="https://i.loli.net/2020/08/25/sCyxpwNifDJLEd9.gif"></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>具体项目代码、数据集、模型已经开源于<a href="https://github.com/luanshiyinyang/FacialExpressionRecognition.git">我的Github</a>，欢迎Star或者Fork。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 深度学习项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸表情识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv5自定义训练</title>
      <link href="/2020/08/20/yolo5-train/"/>
      <url>/2020/08/20/yolo5-train/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLOv5自定义数据集训练"><a href="#YOLOv5自定义数据集训练" class="headerlink" title="YOLOv5自定义数据集训练"></a>YOLOv5自定义数据集训练</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文介绍如何在自己的VOC格式数据集上训练YOLO5目标检测模型。</p><h2 id="VOC数据集格式"><a href="#VOC数据集格式" class="headerlink" title="VOC数据集格式"></a>VOC数据集格式</h2><p>首先，先来了解一下<a href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC数据集</a>的格式，该数据集油5个部分组成，文件组织结构如下，目前主要的是VOC2007和VOC2012.</p><pre><code>- VOC    - JPEGImages        - 1.jpg        - 2.jpg        - ...    - Annotations        - 1.xml        - 2.xml        - ...    - ImageSets        - Main            - train.txt            - val.txt            - test.txt            - trainval.txt        - ...    - SegmentationClass    - SegmentationObject</code></pre><p>第一个文件夹<strong>JPEGImages</strong>为所有的图像，也就是说，训练集、验证集和测试集需要自己划分；<strong>Annotations</strong>为JPEGImages文件夹中每个图片对应的标注，xml格式文件，文件名与对应图像相同；<strong>ImageSets</strong>主要的子文件夹为Main，其中有四个文本文件，为训练集、验证集、测试集和训练验证集的图片文件名；<strong>SegmentationClass</strong>和<strong>SegmentationObject</strong>文件夹存放分割的结果图，前者为语义分割，后者为实例分割。</p><p>上述xml标注文件，格式如下。对其具体标注解释。</p><pre class=" language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>folder</span><span class="token punctuation">></span></span>down<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>folder</span><span class="token punctuation">></span></span> # 图片所处文件夹  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filename</span><span class="token punctuation">></span></span>1.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filename</span><span class="token punctuation">></span></span> # 图片文件名及后缀  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>path</span><span class="token punctuation">></span></span>./savePicture/train_29635.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>path</span><span class="token punctuation">></span></span> # 存放路径  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">></span></span>  #图源信息    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>database</span><span class="token punctuation">></span></span>Unknown<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>database</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>size</span><span class="token punctuation">></span></span> # 图片尺寸和通道    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>width</span><span class="token punctuation">></span></span>640<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>width</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>height</span><span class="token punctuation">></span></span>480<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>height</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>depth</span><span class="token punctuation">></span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>depth</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>size</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>segmented</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>segmented</span><span class="token punctuation">></span></span>  #是否有分割label，0无1有  # 图像中包含的所有目标，一个目标一个object标签  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>car<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>  # 目标类别    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">></span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">></span></span>  # 目标的姿态    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">></span></span>  # 目标是否被部分遮挡（>15%）    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">></span></span>  # 是否为难以辨识的目标， 需要结合背景才能判断出类别的物体    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">></span></span>  # 目标边界框信息      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">></span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">></span></span>156<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">></span></span>111<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">></span></span>259<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>multi_signs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>editType</span> <span class="token punctuation">/></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">></span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">></span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">></span></span>81<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">></span></span>98<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">></span></span>154<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">></span></span>243<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">></span></span></code></pre><p><strong>也就是说，遇到这种文件格式的数据（主要特点为图像全放在一个文件夹，标注格式如上等），将其作为VOC格式的数据集，将自己的数据集重构为VOC格式以便开源项目的处理。</strong></p><h2 id="自定义训练"><a href="#自定义训练" class="headerlink" title="自定义训练"></a>自定义训练</h2><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a><strong>下载源码</strong></h3><p>通过<code>git clone git@github.com:ultralytics/yolov5.git</code>将YOLOv5源码下载到本地，本文后面的内容也可以参考<a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">官方的自定义数据集训练教程</a>，不同于我的教程，该教程全面包含了VOC格式和COCO格式数据集的处理方法。</p><p>此时创建虚拟环境，并通过<code>pip install -r requirements.txt</code>安装依赖包，我这里测试过，最新的项目是兼容Pytorch 1.6的，1.6之前的Pytorch会有一些问题。</p><h3 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a><strong>数据集处理</strong></h3><p>一般，符合VOC格式的数据集至少包含图像和标注两个文件夹，结构如下。我这里假定测试集是独立的，该数据集实际上为训练集，只需要划分出训练集和验证集即可。<strong>这里建议将文件夹重命名如下，否则后续可能出现数据集加载失败的情况。</strong></p><pre><code>- 根目录    - images    - Annotations</code></pre><p>下面，编写脚本划分数据集，<code>split_train_val.py</code>脚本内容如下（参考Github上的开源脚本），只需要执行<code>python split_train_val.py --xml_path dataset_root/Annotations/ --txt_path dataset_root/anno_txt/</code>就得到了划分结果的文件列表，如训练集对应的<code>train.txt</code>如下图，里面与训练图片所有的文件名。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> random<span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--xml_path'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'input xml label path'</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--txt_path'</span><span class="token punctuation">,</span> type<span class="token operator">=</span>str<span class="token punctuation">,</span> help<span class="token operator">=</span><span class="token string">'output txt label path'</span><span class="token punctuation">)</span>opt <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>trainval_percent <span class="token operator">=</span> <span class="token number">1.0</span>train_percent <span class="token operator">=</span> <span class="token number">0.8</span>xmlfilepath <span class="token operator">=</span> opt<span class="token punctuation">.</span>xml_pathtxtsavepath <span class="token operator">=</span> opt<span class="token punctuation">.</span>txt_pathtotal_xml <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>xmlfilepath<span class="token punctuation">)</span><span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>txtsavepath<span class="token punctuation">)</span><span class="token punctuation">:</span>    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>txtsavepath<span class="token punctuation">)</span>num <span class="token operator">=</span> len<span class="token punctuation">(</span>total_xml<span class="token punctuation">)</span>list_index <span class="token operator">=</span> range<span class="token punctuation">(</span>num<span class="token punctuation">)</span>tv <span class="token operator">=</span> int<span class="token punctuation">(</span>num <span class="token operator">*</span> trainval_percent<span class="token punctuation">)</span>tr <span class="token operator">=</span> int<span class="token punctuation">(</span>tv <span class="token operator">*</span> train_percent<span class="token punctuation">)</span>trainval <span class="token operator">=</span> random<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>list_index<span class="token punctuation">,</span> tv<span class="token punctuation">)</span>train <span class="token operator">=</span> random<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>trainval<span class="token punctuation">,</span> tr<span class="token punctuation">)</span>file_trainval <span class="token operator">=</span> open<span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/trainval.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>file_test <span class="token operator">=</span> open<span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/test.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>file_train <span class="token operator">=</span> open<span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/train.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>file_val <span class="token operator">=</span> open<span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/val.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> list_index<span class="token punctuation">:</span>    name <span class="token operator">=</span> total_xml<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'\n'</span>    <span class="token keyword">if</span> i <span class="token keyword">in</span> trainval<span class="token punctuation">:</span>        file_trainval<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token keyword">in</span> train<span class="token punctuation">:</span>            file_train<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            file_val<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        file_test<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>file_trainval<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>file_train<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>file_val<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>file_test<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2020/08/20/HY5yUEdALgmVthw.png"></p><p>接下来，我们要做的就是每个xml标注提取bbox信息为txt格式（这个称为yolo_txt格式），每个图像对应一个txt文件，文件每一行为一个目标的信息，包括<code>类别 xmin xmax ymin ymax</code>。使用的脚本<code>voc_label.py</code>内容如下（<strong>注意，类别要替换为当前数据集的类别列表</strong>），<strong>在数据集根目录（此时包含Annotations、anno_txt以及images三个文件夹的目录）下执行该脚本</strong>，如<code>python ../../utils/voc_label.py</code>。</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># -*- coding: utf-8 -*-</span><span class="token keyword">import</span> xml<span class="token punctuation">.</span>etree<span class="token punctuation">.</span>ElementTree <span class="token keyword">as</span> ET<span class="token keyword">import</span> os<span class="token keyword">from</span> os <span class="token keyword">import</span> getcwdsets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span>classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'window_shielding'</span><span class="token punctuation">,</span> <span class="token string">'multi_signs'</span><span class="token punctuation">,</span> <span class="token string">'non_traffic_signs'</span><span class="token punctuation">]</span>abs_path <span class="token operator">=</span> os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">convert</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> box<span class="token punctuation">)</span><span class="token punctuation">:</span>    dw <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> <span class="token punctuation">(</span>size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    dh <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> <span class="token punctuation">(</span>size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span> <span class="token operator">-</span> <span class="token number">1</span>    y <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span> <span class="token operator">-</span> <span class="token number">1</span>    w <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    h <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>    x <span class="token operator">=</span> x <span class="token operator">*</span> dw    w <span class="token operator">=</span> w <span class="token operator">*</span> dw    y <span class="token operator">=</span> y <span class="token operator">*</span> dh    h <span class="token operator">=</span> h <span class="token operator">*</span> dh    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> w<span class="token punctuation">,</span> h<span class="token keyword">def</span> <span class="token function">convert_annotation</span><span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    in_file <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'Annotations/%s.xml'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">)</span>    out_file <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'labels/%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    tree <span class="token operator">=</span> ET<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>in_file<span class="token punctuation">)</span>    root <span class="token operator">=</span> tree<span class="token punctuation">.</span>getroot<span class="token punctuation">(</span><span class="token punctuation">)</span>    size <span class="token operator">=</span> root<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'size'</span><span class="token punctuation">)</span>    w <span class="token operator">=</span> int<span class="token punctuation">(</span>size<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'width'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>    h <span class="token operator">=</span> int<span class="token punctuation">(</span>size<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'height'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>    <span class="token keyword">for</span> obj <span class="token keyword">in</span> root<span class="token punctuation">.</span>iter<span class="token punctuation">(</span><span class="token string">'object'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        difficult <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'difficult'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        cls <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        <span class="token keyword">if</span> cls <span class="token operator">not</span> <span class="token keyword">in</span> classes <span class="token operator">or</span> int<span class="token punctuation">(</span>difficult<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">continue</span>        cls_id <span class="token operator">=</span> classes<span class="token punctuation">.</span>index<span class="token punctuation">(</span>cls<span class="token punctuation">)</span>        xmlbox <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'bndbox'</span><span class="token punctuation">)</span>        b <span class="token operator">=</span> <span class="token punctuation">(</span>float<span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'xmin'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> float<span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'xmax'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> float<span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'ymin'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span>             float<span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'ymax'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>        b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4 <span class="token operator">=</span> b        <span class="token comment" spellcheck="true"># 标注越界修正</span>        <span class="token keyword">if</span> b2 <span class="token operator">></span> w<span class="token punctuation">:</span>            b2 <span class="token operator">=</span> w        <span class="token keyword">if</span> b4 <span class="token operator">></span> h<span class="token punctuation">:</span>            b4 <span class="token operator">=</span> h        b <span class="token operator">=</span> <span class="token punctuation">(</span>b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4<span class="token punctuation">)</span>        bb <span class="token operator">=</span> convert<span class="token punctuation">(</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span>        out_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>str<span class="token punctuation">(</span>cls_id<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>str<span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token keyword">for</span> a <span class="token keyword">in</span> bb<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>wd <span class="token operator">=</span> getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> image_set <span class="token keyword">in</span> sets<span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">'labels/'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token string">'labels/'</span><span class="token punctuation">)</span>    image_ids <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'anno_txt/%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_set<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    list_file <span class="token operator">=</span> open<span class="token punctuation">(</span><span class="token string">'%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_set<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> image_id <span class="token keyword">in</span> image_ids<span class="token punctuation">:</span>        list_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>abs_path <span class="token operator">+</span> <span class="token string">'/images/%s.jpg\n'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">)</span>        convert_annotation<span class="token punctuation">(</span>image_id<span class="token punctuation">)</span>    list_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>这时候，我们的目标检测数据集就构建完成了，其内容如下，其中labels中为不同图像的标注文件，<code>train.txt</code>等几个根目录下的txt文件为划分后图像所在位置的绝对路径，如<code>train.txt</code>就含有所有训练集图像的绝对路径。</p><p><img src="https://i.loli.net/2020/08/20/NPwkyM3o4T2cn9m.png"></p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>下面需要两个配置文件用于模型的训练，一个用于数据集的配置，一个用于模型的配置。</p><p>首先是数据集的配置，在根目录下的data目录下新建一个yaml文件，内容如下，首先是训练集和验证集的划分文件，这个文件在上面一节最后生成得到了，然后是目标的类别数目和具体类别列表，这个列表务必和上一节最后<code>voc_label.py</code>中的一致。</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">train</span><span class="token punctuation">:</span> dataset/train/train.txt<span class="token key atrule">val</span><span class="token punctuation">:</span> dataset/train/val.txt<span class="token comment" spellcheck="true"># number of classes</span><span class="token key atrule">nc</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token comment" spellcheck="true"># class names</span><span class="token key atrule">names</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'window_shielding'</span><span class="token punctuation">,</span> <span class="token string">'multi_signs'</span><span class="token punctuation">,</span> <span class="token string">'non_traffic_sign'</span><span class="token punctuation">]</span></code></pre><p>然后，编辑模型的配置文件，此时需要先在项目根目录下的weights目录下执行其中的download_weights.sh这个shell脚本来下载四种模型的权重。然后，选择一个模型，编辑项目根目录下models目录中选择的模型的配置文件，将第一个参数nc改为自己的数据集类别数即可，例如我使用yolov5x模型，则修改yolov5x.yaml文件。<strong>这里weights的下载可能因为网络而难以进行，我也将其上传到了百度网盘，<a href="%E9%93%BE%E6%8E%A5%EF%BC%9Ahttps://pan.baidu.com/s/1UQX6URxaJP0ZqALvWpDWkA">地址</a>给出，提取码为vjlx。</strong></p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>此时，可以使用下面的命令进行模型的训练，训练日志默认保存在<code>./runs/</code>下，包括模型参数、Tensorboard记录等。此时TensorBoard以已经默认打开，浏览器访问效果如下图（由于数据量很小，很快过拟合）。</p><pre class=" language-shell"><code class="language-shell">python train.py --img 640 --batch 8 --epoch 300 --data ./data/ads.yaml --cfg ./models/yolov5x.yaml --weights weights/yolov5x.pt --device '0'</code></pre><p><img src="https://i.loli.net/2020/08/20/GrLI9OTtZD3fJFH.png"></p><h3 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h3><p>接着，就是在有标注的测试集或者验证集上进行模型效果的评估，在目标检测中最常使用的指标为mAP。通过下面的命令进行模型测试，由于这是个比赛，测试集没有标注，这里使用验证集作为测试用数据，下述命令只需要指定数据集配置文件和训练结果模型即可。</p><pre class=" language-shell"><code class="language-shell">python test.py  --data ./data/ads.yaml --weights ./runs/exp0/weights/best.pt --augment</code></pre><p>不进行测试时数据增强和进行测试时数据增强（TTA）在验证集上的表现分别如下。</p><pre><code>Class  Images  Targets   P       R      mAP@.5    mAP@.5:.95all    400      970    0.376    0.441     0.35       0.235</code></pre><pre><code>Class  Images  Targets    P      R      mAP@.5    mAP@.5:.95all     400     970     0.272   0.532   0.366        0.24</code></pre><h3 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h3><p>最后，模型在没有标注的数据上进行推理，使用下面的命令（该命令中<code>save-txt</code>选项用于生成结果的txt标注文件，不指定则只会生成结果图像）。其中，weights使用最满意的实验即可，source则提供一个包含所有测试图片的文件夹即可。</p><pre class=" language-shell"><code class="language-shell"> python detect.py --weights runs/exp0/weights/best.pt --source ./dataset/test/ --device 0 --save-txt</code></pre><p>这样，对每个测试图片会在默认的<code>inference/output</code>文件夹中生成一个同名的txt文件，按照我的需求修改了<code>detect.py</code>文件后，每个txt会生成一行一个目标的信息，信息包括<code>类别序号 置信度 xcenter ycenter w h</code>，后面四个为bbox位置，均未归一化。如下图。</p><p><img src="https://i.loli.net/2020/08/20/l86zj2dw9xHnTFO.png"></p><p>我这里因为是一个比赛，再将这个txt处理为了json文件。<strong>不论是这里的处理代码还是上面对<code>detect.py</code>修改的代码，都可以在文末给出的Github仓库找到。</strong></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文介绍了如何使用YOLOv5在自己的数据集上进行训练，按部就班地进行了讲解。该项目在YOLOv5地源码基础上修改完成，代码开源于我的Github，欢迎star或者fork。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> yolov5自定义数据集训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>anchor-free目标检测</title>
      <link href="/2020/08/20/anchor-free-detection/"/>
      <url>/2020/08/20/anchor-free-detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Anchor-free目标检测"><a href="#Anchor-free目标检测" class="headerlink" title="Anchor-free目标检测"></a>Anchor-free目标检测</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>沿着two-step到one-step，anchor-based到anchor-free的发展路线，如今，目标检测（Object Detection，OD）已经进入anchor-free的时代。这些anchor-free方法因为适用于多目标跟踪等领域，也促使了MOT的飞速发展。本文将沿着anchor-free目标检测发展地路线，简单介绍一下主要地一些方法思路，包括目前关注度较大地<strong>FCOS</strong>和<strong>CenterNet</strong>。</p><h2 id="何为anchor"><a href="#何为anchor" class="headerlink" title="何为anchor"></a>何为anchor</h2><p>在了解anchor-free的方法前，我们得知道什么是anchor，在过去，目标检测通常被建模为对候选框的分类和回归，不过，按照候选区域的产生方式不同，分为二阶段（two-step）检测和单阶段（one-step）检测，前者的候选框通过RPN（区域推荐网络）网络产生proposal，后者通过滑窗产生anchor。</p><p><img src="https://i.loli.net/2020/08/20/keLHXrlPFBnqjAf.png"></p><p>本文所提到的anchor-free方法则通过完全不同的思路解决目标检测问题，这些思路都没有采用预定义的候选框的概念。这两年，从CornerNet开始，anchor-free的目标检测框架层出不穷，宣告着目标检测迈入anchor-free时代。</p><h2 id="anchor-free发展"><a href="#anchor-free发展" class="headerlink" title="anchor-free发展"></a>anchor-free发展</h2><p>其实anchor-free不是一个很新的概念，最早可以追溯到YOLO算法，这应该是最早的anchor-free模型，而最近的anchor-free方法主要分为<strong>基于密集预测</strong>和<strong>基于关键点估计</strong>两种。</p><h3 id="早期研究"><a href="#早期研究" class="headerlink" title="早期研究"></a>早期研究</h3><p>先是聊一聊目标检测比较古老的研究，分别是Densebox和YOLO，前者发表于2015年9月，后者则开源于2016年。</p><p><strong>Densebox</strong></p><p>首先来聊一聊Densebox，这是地平线的算法工程师黄李超于2015年设计的一个端到端检测框架，对此有专门的<a href="https://zhuanlan.zhihu.com/p/24350950">文章</a>介绍。Densebox是深度学习用于目标检测的开山之作之一，当时已经有不错效果的R-CNN不够直接且高效，因而Densebox作者从OverFeat方法上得到启发：在图像上进行卷积等同于使用滑窗分类，为何不能使用全卷积对整个图像进行目标检测呢？</p><p><img src="https://i.loli.net/2020/08/20/fMqbOVpkK1rFSsE.png"></p><p>在这个基础上，设计了一套端到端的多任务全卷积模型，如上图所示。该模型可以直接回归出目标出现的置信度和相对位置，同时为了处理遮挡和小目标，引入上采用层融合浅层网络特征，得到更大的尺寸的输出特征图。下图是网络的输入和输出，对每个像素会得到一个5维向量，表示分类置信度和bbox到该pixel的四个距离。</p><p><img src="https://i.loli.net/2020/08/20/sZfUd1VOzECyGXv.png"></p><p>Densebox的主要贡献有两个：证明了单FCN（全卷积网络）可以实现检测遮挡和不同尺度的目标；在FCN结构中添加少量层引入landmark localization，将landmark heatmap和score map融合能够进一步提高检测性能。</p><p><strong>遗憾的是，当时目标检测的学者们沿着RCNN铺好的路亦步亦趋，现在想想，如果当时，就有足够多的关注给与Densebox，今天的目标检测是否会是全新的局面呢？</strong></p><p><strong>YOLO</strong></p><p>2016年开源的YOLOv1算法，是目前工业界比较关注的算法之一，它开创性地将目标检测中的候选框生成和目标识别通过一步完成，因而论文名为“You only look once”，YOLO模型可以直接从整个图像上得到边界框和对应的置信度。比较详细的理解可以参考<a href="https://zhouchen.blog.csdn.net/article/details/105178437">我之前YOLO算法的文章</a>。</p><p><img src="https://i.loli.net/2020/08/20/whpbPxLcqWmsytH.png"></p><p>YOLO的最大创新就是速度快，一方面将候选框生成的步骤去除，另一方面，通过多个网格负责目标的检测，大大加快运行速度。</p><blockquote><p>Densebox和YOLO很类似，都可以理解为单阶段检测，不过前者为密集预测，针对每个像素进行预测；后者针对划分得到的网格进行预测。同时，作为anchor-free的两篇开山之作，它们为后来的anchor-free检测提供了很多的思路。</p></blockquote><h3 id="基于密集预测"><a href="#基于密集预测" class="headerlink" title="基于密集预测"></a>基于密集预测</h3><p>沿着上一节YOLO和Densebox的思路，2019年出现了很多以此为基础的目标检测方法，包括FCOS、FSAF以及FoveaBox等等方法。</p><p><strong>FCOS</strong></p><p>这是这两年受到广泛的关注的目标检测算法，一方面它确实是anchor-free系列打破anchor-based精度神话的关键之作，另一方面，业界对这种单阶段高效算法有着巨大的需求。</p><p><img src="https://i.loli.net/2020/08/20/UVEgZsIvuQtBPTm.png"></p><p>上图是FCOS的pipeline设计图，核心的就是一个特征金字塔和三分支头网络，通过backbone之后对feature map上每一个点进行回归预测，和以往目标检测任务不同的是，<strong>除了分类和回归分支，加入了center-ness以剔除低质量预测，它和分类分支的乘积为最终的置信度。</strong></p><p>FCOS创新点如下:</p><ol><li>突破基于Faster-RCNN修补的思路，开创性地不使用anchor而是直接对每个像素进行预测，并在效果是远超Faster-RCNN。这主要是考虑到anchor地存在会带来大量地超参数，如anchor number等，而且这些anchor要计算和GT地IOU，也是很消耗算力的。</li><li>由于是像素级别的密集预测，因此可以使用分割任务的一些trick并且通过修改目标分支可用于实例分割和关键点检测等任务。</li><li>由于是全卷积网络，拥有很多FCN任务的优势，也可以借用其思想。</li></ol><p><strong>FSAF</strong></p><p><img src="https://i.loli.net/2020/08/20/j6QLvatzU9DmlBn.png"></p><p>这是一个针对FPN的优化思路，提出FSAF模块，让网络自己学习anchor适配。​在RetinaNet的基础上，FSAF模块引入了2个额外的卷积层，这两个卷积层各自负责anchor-free分支的分类和回归预测。此外，提出了在线特征选择策略，​实例输入到特征金字塔的所有层，然后求得所有anchor-free分支focal loss和IoU loss的和，选择loss最小的特征层来学习实例。训练时，特征根据安排的实例进行更新。推理时，不需要进行特征更新，因为最合适的特征金字塔层必然输出高置信分数。</p><blockquote><p>虽然都是基于密集预测，但相比于YOLO和Densebox，FCOS和FSAF使用FPN进行多尺度预测，此前的方法只有单尺度预测；不过，相比于YOLO这个单分支模型，其他方法都是通过两个子网络来进行分类和回归。</p></blockquote><h3 id="基于关键点估计"><a href="#基于关键点估计" class="headerlink" title="基于关键点估计"></a>基于关键点估计</h3><p>不同于密集预测的思路，以关键点估计为手段，目标检测出现了一条全新的主线，它彻底抛开了区域分类回归思想，主要出现了CornerNet、ExtremeNet以及集大成者的CenterNet，由于有两篇目标检测的文章网络名都是CenterNet，这里特指的是关注度比较高的Objects as points这篇文章。</p><p><strong>CornerNet</strong></p><p>这篇文章是后来很多基于关键点估计处理目标检测的算法基础，它开创性地用一对角点来检测目标。对一幅图像，预测两组heatmap，一组为top-left角点，另一组为bottom-right角点，每组heatmap有类别个通道。下图为框架图。</p><p><img src="https://i.loli.net/2020/08/20/1SjiLUVbJGzXr9o.png"></p><p><strong>ExtremeNet</strong></p><p>不同于CornerNet使用角点检测目标，ExtremeNet通过极值点和中心点来检测目标，这应该是最大地区别，其他一些关键点估计方面地细节，这里不多提。</p><p><img src="https://i.loli.net/2020/08/20/QuToZCtJUprdxwX.png"></p><p><strong>CenterNet</strong></p><p>下面来看看关键点估计用于目标检测地集大成者，CenterNet。抛开了传统的边框目标表示方法，将目标检测视为对一个点进行的关键点估计问题。相比较于基于bbox的方法，该模型端到端可微，其简单高效且实时性高。在主流地OD数据集上超越了大部分SOTA方法，且论文称在速度上超越了YOLO3。</p><p><img src="https://i.loli.net/2020/08/20/CIp6humX5QG9dPN.png"></p><p>通过中心点来表示目标，然后在中心点位置回归出目标的其他属性，这样，目标检测问题变成了一个关键点估计问题。只需要将图像传入全卷积网络，得到热力图，热力图的峰值点就是中心点。这里可以把中心点看做形状未知的锚点。但是该锚点只在位置上，没有尺寸框，没有阈值进行前后景分类；每个目标只会有一个正的锚点，因此不会用到NMS；而且，CenterNet与传统目标检测相比，下采样倍数较低，不需要多重特征图。</p><h2 id="发展思路"><a href="#发展思路" class="headerlink" title="发展思路"></a>发展思路</h2><h3 id="成功原因"><a href="#成功原因" class="headerlink" title="成功原因"></a>成功原因</h3><p>anchor-free能在精度上追赶上anchor-based方法，最大地原因应该归属上面绝大多数方法避不开地FPN（特征金字塔网络），因为在每个位置只预测一个框地前提下，FPN对尺度信息进行了很好地弥补，而Focal loss则对区域地回归有一定辅助效果。</p><h3 id="anchor-free局限性"><a href="#anchor-free局限性" class="headerlink" title="anchor-free局限性"></a>anchor-free局限性</h3><p>当然，anchor-free地目标检测方法也有很大地局限性，这些方法虽然声称精度追上了较好地二阶段方法，但存在一些训练上地细节以及部分不公平地比较。不过，总体来说，速度上地突破还是吸引了很多工业界的关注的。</p><h3 id="GT的设计"><a href="#GT的设计" class="headerlink" title="GT的设计"></a>GT的设计</h3><p>上面的很多方法其实出发点都是bbox这个矩形框冗余信息太多，目标信息少，大部分是背景。它们大多都改变了GT的定义，如CornerNet将其定义为角点，ExtremeNet将其定义为极值点，FCOS虽然还是矩形框但也使用了center-ness进行抑制，FSAF则将GT定义为中心区域。对于GT目标的改进优化促使了目标检测的发展。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><strong>Densebox</strong>: Huang L, Yang Y, Deng Y, et al. Densebox: Unifying landmark localization with end to end object detection[J]. arXiv preprint arXiv:1509.04874, 2015.<br><br><strong>YOLO</strong>: Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[A]. Proceedings of the IEEE conference on computer vision and pattern recognition[C]. 2016: 779–788.<br><br><strong>FCOS</strong>: Tian Z, Shen C, Chen H, et al. FCOS: Fully Convolutional One-Stage Object Detection[J]. arXiv:1904.01355 [cs], 2019.<br><br><strong>FSAF</strong>: Zhu C, He Y, Savvides M. Feature Selective Anchor-Free Module for Single-Shot Object Detection[J]. arXiv:1903.00621 [cs], 2019.<br><br><strong>CornerNet</strong>: Law H, Deng J. CornerNet: Detecting Objects as Paired Keypoints[J]. arXiv:1808.01244 [cs], 2019.<br><br><strong>ExtremeNet</strong>: Zhou X, Zhuo J, Krähenbühl P. Bottom-up Object Detection by Grouping Extreme and Center Points[J]. arXiv:1901.08043 [cs], 2019.<br><br><strong>CenterNet</strong>: Zhou X, Wang D, Krähenbühl P. Objects as points[A]. arXiv preprint arXiv:1904.07850[C]. 2019.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> anchor-free目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/08/19/hello-world/"/>
      <url>/2020/08/19/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
