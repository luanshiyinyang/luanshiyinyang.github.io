<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>GFocalV2解读</title>
      <link href="2021/03/20/gfocalv2/"/>
      <url>2021/03/20/gfocalv2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文对作者基于GFocalV1的工作的扩展，取得了不错的效果，想要更加直白理解这篇文章的话可以参考作者的<a href="https://zhuanlan.zhihu.com/p/313684358">《大白话 Generalized Focal Loss V2》</a>。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在<a href="https://zhouchen.blog.csdn.net/article/details/114972223">之前的文章</a>介绍了GFocalV1这个开创性的工作，它将边界框建模为通用分布表示，这个分布式的表示其实在GFocalV1中对整个模型的贡献并不是非常大（相比于分类-质量联合表示），因此如何充分利用成为GFocalV2的出发点，事实上，既然分布的形状和真实的定位质量高度相关，那么这个边框的分布表示其实可以算出统计量指导定位质量估计，这就是GFocalV2的核心创新点。</p><p><img src="https://i.loli.net/2021/03/20/WSYk3tfLroGKMIw.png" alt=""></p><ul><li><p>论文标题</p><p>  Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2011.12885">http://arxiv.org/abs/2011.12885</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/implus/GFocalV2">https://github.com/implus/GFocalV2</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>当前的密集检测器由于其优雅且高效，成为业界比较关注的研究热点之一。这类框架的一个重要组件就是Localization Quality Estimation（LQE，定位质量评估模块），通过LQE，高质量检测框相比于低质量检测框有着更高的得分从而降低被NMS消除的风险。LQE的实现形式多变，早有基于IoU的Objectness，近有基于偏中心距离的Centerness，不过它们都有共同的特点，那就是它们都基于原始的卷积特征进行质量估计，如基于点、边界框或者区域的特征。</p><p>不同于之前的工作，GFocalV2从一个新的角度实现LQE，它利用了边界框分布的统计学特征而不使用原始的卷积特征。这里的边界框通用分布源于GFocalV1，它通过学习每个要预测的边的概率分布来描述边界框回归的不确定性，如下图的(a)所示。有趣的是，这个通用分布其实和真实的定位质量高度相关，作者将预测框分布的top1值和真实的IoU定位质量做了一个散点图，如下图的(b)所示，可以发现整个散点图其实是有y=x这一趋势的，这就是说，统计意义上来看，分布的形状其实与定位质量高相关。</p><p><img src="https://i.loli.net/2021/03/20/ORMhGf3wctZexYX.png" alt=""></p><p>更具体一点，从上图的(c)和(d)可以发现，边界框分布的尖锐程度可以清晰描述预测框的定位质量，对于那些模型很确信的预测它的分布是很尖锐的（在一个位置非常突出），而对于模型比较容易混淆的（上图的伞柄）预测框，分布是比较平缓甚至呈现双峰分布的。因此，分布的形状可以指导模型获得更好的LQE，那么如何刻画分布的形状呢？其实论文这里采用了一个非常简单的方法，就是topk数值来进行描述，这点后文会详细讲解。</p><p>总之，论文作者设计了一个非常轻量的子网络基于这些分布的统计量产生更加可靠的LQE得分，取得了可观的性能提升，文章将这个子网络称为Distribution-Guided Quality Predictor (DGQP)。引入这个DGQP之后的GFocalV1就成为了GFocalV2，这是一个新的密集检测器，在ATSS上获得了2.6AP的收益。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在之前的文章中，我已经介绍了GFocalV1了，那篇文章的核心创新点是<strong>将分类得分和IoU定位质量估计联合到了一起</strong>和<strong>将边界框表示建模为通用分布</strong>，这里就不多做回顾了。</p><h3 id="分解分类和IoU的表示"><a href="#分解分类和IoU的表示" class="headerlink" title="分解分类和IoU的表示"></a>分解分类和IoU的表示</h3><p>在尽管在GFocalV1中分类和质量估计的联合表示解决了训练和推理时的不一致问题，然而只是使用分类分支来产生联合表示还是有所局限，所以这篇论文中作者将其显式分解为利用分类（$C$）和回归（$I$）两个分支的结构，联合表示的$J=C\times I$。其中$\mathbf{C}=\left[C_{1}, C_{2}, \ldots, C_{m}\right], C_{i} \in[0,1]$是$m$个类别的分类表示，而$I \in[0,1]$是表示IoU表示的标量。尽管这里将$J$分解为了两部分，但是依然同时在训练和推理阶段使用$J$，所以依然保持了GFocalV1追求的一致性。</p><p>那么这个$C$是如何得到的呢，其实是通过设计的DGQP模块从$C$和$I$计算而来，J的监督依然由QFL损失进行，关于这个DGQP模块，下文会讲解。</p><h3 id="DGQP模块"><a href="#DGQP模块" class="headerlink" title="DGQP模块"></a>DGQP模块</h3><p>Distribution-Guided Quality Predictor（DGQP）模块是GFocalV2最核心的模块，它如下图中的红色框所示，将预测的边框分布$P$输入到一个非常简单的全连接子网络中，得到一个IoU质量标量$I$，它和分类得分相乘作为最终的联合表示$J$。和GFocalV1类似，它将相对四个边的偏移量作为回归目标，用通用分布表示，将四个边表示为${l,r,t,b}$，定义$w$边的概率分布为$\mathbf{P}^{w}=\left[P^{w}\left(y_{0}\right), P^{w}\left(y_{1}\right), \ldots, P^{w}\left(y_{n}\right)\right]$，其中$w \in{l, r, t, b}$。</p><p><img src="https://i.loli.net/2021/03/20/KsZ23TFoQ4MlWNH.png" alt=""></p><p>关于如何选择分布的统计量，作者这里采用的是分布向量$P^w$的Top-k个值和整个向量的均值作为统计特征，因此四条边概率向量的基础统计特征$\mathbf{F} \in \mathbb{R}^{4(k+1)}$通过下式计算得到，其中的Topkm表示求topk值加上均值计算，concat表示串联。</p><script type="math/tex; mode=display">\mathbf{F}=\operatorname{Concat}\left(\left\{\operatorname{Topkm}\left(\mathbf{P}^{w}\right) \mid w \in\{l, r, t, b\}\right\}\right)</script><p>之所以选择topk和均值，有以下两个好处。</p><ol><li>由于是概率分布，因此概率分布的和是固定为1的，因此topk和均值就能基本反映分布的平坦程度，越大越尖锐，越小越平坦。</li><li>topk和均值可以尽量和对象的尺度无关，也就是如下图所示的，不管是左边的小尺度目标还是右边的大尺度目标，它们的分布都是尖锐的（形状类似），因而统计值也应该差不多。这种表示方法更有鲁棒性。</li></ol><p><img src="https://i.loli.net/2021/03/20/AoeFYGd3DvHuXwQ.png" alt=""></p><p>下面来具体看一下DGQP模块的结构实现，如下图所示，上面的统计特征$F$输入DGQP模块（记为$\mathcal{F}$）中，这个模块只有两个全连接层（配合以激活函数），因而最终IoU预测标量如下式。</p><p><img src="https://i.loli.net/2021/03/20/rkWa9GvENXsHT7o.png" alt=""></p><script type="math/tex; mode=display">I=\mathcal{F}(\mathbf{F})=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{F}\right)\right)</script><p>其中参数$\mathbf{W}_{1} \in \mathbb{R}^{p \times 4(k+1)}$和$\mathbf{W}_{2} \in \mathbb{R}^{1 \times p}$，$k$就是topk的取值，$p$是隐藏层的神经元数目（论文中$k=4$且$p=64$），最终输出的$I$和分类得分$C$相乘得到联合表示。</p><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><p>需要注意的是，DGQP是非常轻量的，它引入的参数是可以忽略不急的。它也几乎不会带来计算开销。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验部分作者设计了不少消融实验，如统计量的选择、DGQP结构复杂度、统计特征指导和各种类型卷积特征的对比、联合表示的分解形式、和主流检测器的兼容性，这里我就不贴太多表格了。</p><p><img src="https://i.loli.net/2021/03/20/fD1LgoujM6q5xvE.png" alt=""></p><p>上图是GFocalV2和主流SOTA的对比，性能改善是蛮大的。此外，作者还进行下图的可视化，可以看到其他算法准确的框score往往都在第三第四而GFocalV2质量较高的框得分也较高，这进一步说明GFocalV2可以利用好更好的定位质量估计来保障更精准的结果。</p><p><img src="https://i.loli.net/2021/03/20/UpdRihj4WZMA1zN.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章作者从一个新的角度地思考质量估计，从而提出边框的分布表示的统计特征指导质量估计，这是一个全新的工作，应该算是检测历史上第一个用学习到的分布的统计学特征指导质量估计的，本文也只是对这篇文章进行了比较粗糙的解读，想要更详细理解的强烈推荐阅读原论文。最后，如果我的文章对你有所帮助，欢迎一键三连，你的支持是我不懈创作的动力。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GFocalV2解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GFocal解读</title>
      <link href="2021/03/18/gfocal/"/>
      <url>2021/03/18/gfocal/</url>
      
        <content type="html"><![CDATA[<blockquote><p>总的来说，GFocal这篇文章的数学性还是很高的，如果对此不感兴趣，可以阅读原作者的<a href="https://zhuanlan.zhihu.com/p/147691786">《大白话 Generalized Focal Loss》</a>。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>众所周知，单阶段检测器的发展是目标检测领域的重要研究方向，而单阶段方法往往将目标检测视为密集分类和定位任务（其中定位任务值得就是边框回归）。分类任务通常通过Focal Loss进行优化，而边框定位通常按照Dirac delta分布来学习。最近单阶段检测器的研究趋势为引入一个单独的分支来估计定位的质量（centerness或者iou），这个质量预测有助于分类任务的进行从而整体上提高检测的性能。GFocal这篇文章就是针对上述的三个表示（representation）进行的工作，分别是分类表示、定位表示和检测框的质量估计，这里的表示指的是检测器最终的输出也就是head的输出。这三种表示一般关系如下图所示。</p><p><img src="https://i.loli.net/2021/03/18/lmiICjUSWXuOvbL.png" alt=""></p><p>作者发现目前的表示学习有两个问题，第一，质量估计和分类训练和推理时时不一致的（两个分支单独训练，但是推理时乘在一起，如上图所示）；第二，Dirac delta分布的定位表示不够灵活，无法建模复杂场景下的歧义和不确定性。</p><p>为了解决这两个问题，论文作者设计了新的表示方式，将质量评估合并到分类预测向量中形成一个联合进行定位质量评估和分类的表示，此外也使用一个向量来表示任意的概率分布，这个概率分布用来建模边框位置信息（也就是说，边框回归的输出从定值变为了一个概率分布）。这两个方案可以有效解决上述的问题，特别是提出的联合表示可以有效消除不一致问题，但是因此标签也变为了连续值，因此作者提出对离散值的focal loss的扩展，即论文的Generalized Focal Loss （GFL），成功推广到了连续域上。在COCO数据集上，同等backbone和训练设置下，GFocal获得了0.45的AP，超越了SAPD和ATSS，而GFocal有着更快的速度。这文章最终收录在NeurIPS2020。</p><ul><li><p>论文标题</p><p>  Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2006.04388">http://arxiv.org/abs/2006.04388</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/implus/GFocal">https://github.com/implus/GFocal</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>近年来，密集检测器逐渐引领了目标检测的发展趋势，而对于边框表示和定位质量估计的研究也有了不错的进展。在过去数年中，边框表示被建模为简单的Dirac delta分布。FCOS中提出了一个额外的定位质量估计分支（使用center ness得分或者iou得分），这为检测的准确性带来了不小的提高，但是，在推理时质量估计和分类分支是组合到一起（通过乘法）作为NMS排序的置信度的。尽管有了很大的成功，但是论文作者发现了现有方法存在的一些问题。</p><ol><li><strong>质量估计和分类得分在训练和推理时的使用不一致</strong></li></ol><p><img src="https://i.loli.net/2021/03/18/uvFAmYjZHkWyJlD.png" alt=""></p><p>如上图(a)所示，在密集预测检测器中，定位质量评估和分类得分时分别训练的，但是推理时却是组合使用的。还有就是，定位质量评估的监督只对正样本进行，这是不可靠的，因为有些负样本也可能获得较高的质量得分（因为它们时没有监督的，所以负样本的质量预测时模型不了解的未定义行为），而这个不合理的高质量评估得分和很低的分类得分乘起来后用于NMS排序，可能使得这些负样本排到正样本前面，这当然是很不合理的（示例如下图(a)所示）。</p><p><img src="https://i.loli.net/2021/03/18/Jbp9QrTgoFuq4zd.png" alt=""></p><ol><li><strong>边界框表示不灵活</strong></li></ol><p>常用的边界框表示的方法可以认为是狄拉克分布，但是实际场景中目标的标注框往往具有歧义和不确定性，比如下图所示的不清晰的边界（被水模糊的滑板和严重遮挡的大象），狄拉克这种简单分布式难以建模的。后来诞生了一些方法采用高斯分布来建模边框，但是高斯分布还没有复杂到足以捕获目标框的位置分布，因为真实的分布可能是任意且灵活的，不需要高斯分布那样对称。</p><p><img src="https://i.loli.net/2021/03/18/i2vnPT7VxlqwYBH.png" alt=""></p><p>为了解决上述的两个问题，GFocal设计了新的边界框表示方法和新的定位质量估计表示。对于定位质量的表示，作者提出将其和分类得分进行合并得到一个统一的表示，而对于边界框的表示，作者则将其建模为连续空间上的概率分布（论文称为General distribution），下文详细。理解这两个思路。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p>上文说到，作者为了解决定位质量表示和分类得分表示不一致的问题，作者将定位质量融入到了分类得分中形成一个统一的表示向量，该向量GT类别下标处对应的置信度不再是分类置信度而是质量预测的得分（这个得分一般采用预测框和真实框的IoU来表示）。将这个统一的表示记为classification-IoU joint representation，它可以被端到端优化且在推理时直接使用，如下图所示。这种方法可以有效消除训练和测试的不一致并且建模了分类和质量评估之间的强相关性。进一步看，由于在分类分支中，因此负样本也会被监督（质量得分为0），使得质量评估更加可靠。</p><p><img src="https://i.loli.net/2021/03/18/kw2JYVRijM7PA9g.png" alt=""></p><p>对于边框的表示，作者提出用任意分布来建模边界框并在连续空间上直接学习概率分布，不引入如高斯分布那样较强的先验知识。这里其实简单理解就是，原本FCOS等方法预测中心点到边界的距离是一个值（狄拉克分布），而论文预测的时中心点到边界的概率分布向量。这种方法可以有效进行边框回归，同时感知边框的潜在分布。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p>方案出来了，接下来就是要面对的问题了，首先看第一个方案，此时其实预测的标签已经从离散的{0,1}变为了连续的0-1的，而Focal Loss是不支持离散标签的，它的公式可以简单表述如下（忽略原始论文的$\alpha_t$）。</p><script type="math/tex; mode=display">\mathbf{F L}(p)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right), p_{t}=\left\{\begin{aligned}p, & \text { when } y=1 \\1-p, & \text { when } y=0\end{aligned}\right.</script><p>这个公式中的$y \in {1,0}$表示真实类别，而$p \in[0,1]$表示估计的标签为$y=1$的概率，$\gamma$是可以调整的聚焦参数。具体来看，Focal loss由一个标准交叉熵的部分$-\log \left(p_{t}\right)$和动态缩放因子部分$\left(1-p_{t}\right)^{\gamma}$组成，这个缩放因子可以在训练时对容易样本降权而使得模型快速聚焦难样本的学习。</p><p>然而，对于分类质量的联合表示，标签已经变为了0-1的连续值，具体说，此时$y=0$表示质量得分为0的负样本，而$0&lt;y \leq 1$代表目标IoU标签为$y$的正样本，为了解决这个问题，作者对Focal Loss的两个部分分别进行了扩展以保证可以在联合表示下使用。首先，原来的交叉熵部分$-\log \left(p_{t}\right)$被修改为完整的形式$-((1-y) \log (1-\sigma)+y \log (\sigma))$；接着，缩放因子部分$\left(1-p_{t}\right)^{\gamma}$被推广到预测的sigmoid结果和连续标签$y$的绝对距离，即$|y-\sigma|^{\beta}(\beta \geq 0)$，两侧的绝对值确保非负。将这两个拓展部分组合到一起，形成完整的Quality Focal Loss（QFL），公式如下。</p><script type="math/tex; mode=display">\mathbf{Q F L}(\sigma)=-|y-\sigma|^{\beta}((1-y) \log (1-\sigma)+y \log (\sigma))</script><p>和Focal Loss类似，魔改后的调制因子也有关注难样本的功能，当预测的质量估计值与真实标签相差很多的时候，$|y-\sigma|^{\beta}$的值便较大，模型就能更关注于这些难分类的样本；而当预测值与真实标签很接近的时候，损失便也接近0，所以简单样本的损失权重便被降低了。</p><p><img src="https://i.loli.net/2021/03/18/iX93pYZe4O2TScr.png" alt=""></p><p>上如右侧的上半部分就是上面提到的QFL，下半部分就是我们后面要讲的DFL。</p><p>我们接着看，作者如何解决第二个方案遇到的问题，这篇文章采用的边框回归的目标是中心点到四个边的相对偏移，如上图所示的回归分支（采用哪种回归目标其实大同小异）。如上图左侧的回归分支所示，传统的边框回归是将标签$y$建模为狄拉克分布，也就是只有一个值概率为一，说的更通俗易懂就是只会产生一个输出值（比如到左边框的偏移是$x$），通常通过全连接层实现。作者认为应该不引入先验直接建模一个通用分布$P(x)$，也就是预测值是一个概率分布，可以理解为中心点到边的概率（因而由四个概率向量）。</p><p>假定标签$y$的最小值为$y_0$最大值为$y_n$（$y_{0} \leq y \leq y_{n}, n \in \mathbb{N}^{+}$），而模型预测的$\hat{y}$则可以通过预测的概率分布得到（这里$\hat{y}$同样满足$y_{0} \leq \hat{y} \leq y_{n}$），计算式如下。需要补充的是，实际上代码实现是，作者是将$y_0$设置为0，$y_n$设置为16的。</p><script type="math/tex; mode=display">\hat{y}=\int_{-\infty}^{+\infty} P(x) x \mathrm{~d} x=\int_{y_{0}}^{y_{n}} P(x) x \mathrm{~d} x</script><p>为了适配卷积神经网络，作者将连续域上的积分转换为离散表示，具体是将区间$\left[y_{0}, y_{n}\right]$离散化为集合$\left{y_{0}, y_{1}, \ldots, y_{i}, y_{i+1}, \ldots, y_{n-1}, y_{n}\right}$，集合每个元素间隔为了简单取1。此时有$\sum_{i=0}^{n} P\left(y_{i}\right)=1$，因而估计的回归值变为下式。$P(x)$可以通过Softmax函数实现，因此$P(y_i)$可以记作$S_i$。</p><script type="math/tex; mode=display">\hat{y}=\sum_{i=0}^{n} P\left(y_{i}\right) y_{i}</script><p>注意，因为已经得到了$\hat{y}$，因此它可以使用L1损失、IoU损失或者GIoU损失来训练，但是作者分析了一下发现一个积分目标可能对于无数种积分模式（如下图的面积相同的三种分布，显然第三种最合适，因为最合适的位置如果存在那么与标签不会相差太远），这有损学习效率。考虑到真实的分布通常不会距离标注的位置太远，所以作者又额外加了个loss，希望网络能够快速地聚焦到标注位置附近的数值，使得他们概率尽可能大。</p><p><img src="https://i.loli.net/2021/03/18/U6T2Kp8mkCchV39.png" alt=""></p><p>基于此，作者将这个损失取名Distribution Focal Loss (DFL) ，它显式增大最接近标签$y$的两侧的$y_i$和$y_{i+1}$的概率。此外，由于边框回归只考虑正样本没有不平衡问题的风险，简单将QFL的完整交叉熵部分应用过来就行，形成的DFL公式如下。公式中关于$S_i$和$S_{i+1}$如何能够接近目标，作者在论文补充材料进行了证明，这里不多赘述。</p><script type="math/tex; mode=display">\mathbf{D F L}\left(\mathcal{S}_{i}, \mathcal{S}_{i+1}\right)=-\left(\left(y_{i+1}-y\right) \log \left(\mathcal{S}_{i}\right)+\left(y-y_{i}\right) \log \left(\mathcal{S}_{i+1}\right)\right)</script><p>最后，其实归纳可以发现，QFL和DFL可以表示为一个通用的形式，也就是本文的标题由来Generalized Focal Loss (GFL)，我这里就简单给出这个公式，诸多细节可以阅读原论文。</p><script type="math/tex; mode=display">\mathbf{G F L}\left(p_{y_{l}}, p_{y_{r}}\right)=-\left|y-\left(y_{l} p_{y_{l}}+y_{r} p_{y_{r}}\right)\right|^{\beta}\left(\left(y_{r}-y\right) \log \left(p_{y_{l}}\right)+\left(y-y_{l}\right) \log \left(p_{y_{r}}\right)\right)</script><h3 id="训练损失"><a href="#训练损失" class="headerlink" title="训练损失"></a>训练损失</h3><p>最终的训练损失如下式，其中$\mathcal{L}_{\mathcal{Q}}$表示QFL而$\mathcal{L}_{\mathcal{D}}$表示DFL，而$\mathcal{L}_{\mathcal{B}}$指的是GIoU Loss，$N_{pos}$表示正样本数目，$\lambda_{0}$和$\lambda_1$是$\mathcal{L}_{\mathcal{Q}}$和$\mathcal{L}_{\mathcal{D}}$的平衡权重。在金字塔特征图的所有位置计算求和。这里的$\mathbf{1}_{\left{c_{z}^{<em>}&gt;0\right}}$为指示函数，当$c_{z}^{</em>}&gt;0$时为1，否则为0。</p><script type="math/tex; mode=display">\mathcal{L}=\frac{1}{N_{\text {pos }}} \sum_{z} \mathcal{L}_{\mathcal{Q}}+\frac{1}{N_{\text {pos }}} \sum_{z} \mathbf{1}_{\left\{c_{z}^{*}>0\right\}}\left(\lambda_{0} \mathcal{L}_{\mathcal{B}}+\lambda_{1} \mathcal{L}_{\mathcal{D}}\right)</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者进行了一些消融实验，这里就不细说了，这里直接说明两个结论。</p><ol><li>QFL和DFL两个损失的作用是正交的，他们的增益互不影响，所以结合使用会有更好的效果。</li><li>GFocal与各大模型相比，取得了较为不错的speed-accuracy trade-off，基本上属于无痛涨点的利器。</li></ol><p>下图是单模型单尺度的结果对比，可以看到，GFocal取得了相当亮眼的表现。</p><p><img src="https://i.loli.net/2021/03/18/UDpNoYtPE4rXAZL.png" alt=""></p><p>此外，也附上和SOTA对比的实验结果图。</p><p><img src="https://i.loli.net/2021/03/18/mlHK1fx2nQUZtvL.png" alt=""></p><p><strong>文章的补充材料也有不少干货，关于centerness和IoU的对比我就不赘述了，此外作者也进行了分布式边框表示的可视化，以下图为例，可视化四个边的预测分布，可以发现一个有趣的现象，当伞不算伞柄也就是白色GT框可以定位伞，但是算上伞兵绿色预测框又可以定位伞，概率分布也得确是一个双峰分布，这其实表达标注和模型对某个物体的两种理解，这个双峰分布很有趣，给后来的研究留下了无数遐想。</strong></p><p><img src="https://i.loli.net/2021/03/18/bkFAfqJMmGwsV7I.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章从表示（representation）这一模型学习的关键出发提出了GFocal，将边框建模为概率表示极具开创性。论文基于mmdetection实现，也很便于源码的阅读和使用，是相当亮眼的工作，值得一读。最后，本文我只是做了一些基本的解读，更详细的请参考原论文，如果我的文章对你有所帮助，欢迎一键三连，你的支持是我不懈创作的动力。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GFocal解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TTFNet解读</title>
      <link href="2021/03/16/ttfnet/"/>
      <url>2021/03/16/ttfnet/</url>
      
        <content type="html"><![CDATA[<blockquote><p>正巧最近CenterNet的作者发布了CenterNet2，趁此机会也读一读这篇之前针对CenterNet训练时间改进的TTFNet。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>针对当前目标检测算法的痛点：其实大多数检测器都无法同时做到<strong>短的训练时间</strong>、<strong>快的推理速度</strong>和<strong>高的模型精度</strong>。为了追求这三者的平衡，TTFNet（Training-Time-Friendly Network，训练时间友好网络）被提了出来，在这个工作中，作者的起点是轻量的head（light-head）、单阶段（single-stage）和anchor-free的设计，这类方法可以保证推理速度较快，继而开始思考如何加快训练时间。回顾前人的方法，可以发现使用更多的训练样本等效于增大batch siz，有助于增大学习率加快训练过程。文中反复出现的encode more training samples的意思其实是采用更多的训练样本点，CenterNet只使用中心点进行目标回归，FCOS则采用GT中所有点进行目标回归，本文则采用高斯核控制参与回归的样本数量，从而引入更多回归样本加速收敛，又避免了FCOS那样的NMS后处理。实验表明，TTFNet获得了很好的训练时间、推理速度和精度的平衡，在获得SOTA精度的同时减少7倍的训练时间，超快版本的TTFNet-18和TTFNet-53超越SSD300和YOLOv3的同时，只需要它们十分之一的训练时间。</p><ul><li><p>论文标题</p><p>  Training-Time-Friendly Network for Real-Time Object Detection</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/1909.00700">http://arxiv.org/abs/1909.00700</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/ZJULearning/ttfnet">https://github.com/ZJULearning/ttfnet</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>训练时间、推理速度和模型精度是目标检测领域目前研究的主要方向，但是，很少有工作在这三者间追求一个平衡。直观上看，推理速度快的检测器就应该有更短的训练时间，然而事实往往不是这样，大多数实时检测器比起非实时的检测器需要更长的训练时间。精度往往是我们最关心的，精度高的前提下将检测器分为两种：<strong>推理速度慢</strong>的和<strong>训练时间久</strong>的。前者往往有着庞大的head和复杂的后处理，这种设计虽然精度更高收敛更快但是不可避免为推理带来了压力，降低了推理速度，并不适用于实时应用。因此，后者这种简化head和后处理设计同时保持精度的检测器应运而生，CenterNet就是代表方法之一，其推理时间几乎和backbone网络相当。然而，这类推理速度快的网络都不可避免地需要漫长的训练，这也很容易理解，这类模型相对比较简单因而难以训练，需要大量的数据增强和训练调度。举例而言，CenterNet在COCO上需要140轮的训练，而那种推理速度较慢的网络只需要12轮。</p><p>因此，TTFNet这篇文章主要关注如何将CenterNet这类网络的训练时间缩短同时保证精度和实时推理的速度。此前的研究表明，batch size越大可以采用更大的学习率，而且大多数情况下两者符合线性关系。而作者注意到，从标注框中编码更多的训练样本的效果是类似于增大batch size的。相比于特征提取的时间，编码特征和损失计算的时间是几乎可以忽略不计的，所以基本上可以在不增加额外开销的情况下安全地获得更快的收敛速度。遗憾的是，CenterNet仅仅使用目标的中心来进行尺寸回归，没有利用目标中心附近的信息，TTFNet作者通过实验证明，就是这个设计导致CenterNet收敛很慢。</p><p>为了缩短训练时间，论文提出了一种新的使用高斯核来编码更多训练样本的方法来进行目标的定位和回归，这种策略使得网络可以更好地利用标注框产生更多的监督信号，这给快速收敛提供了保证。具体而言，通过高斯核函数在目标中心周围构造一个子区域，然后从该区域提取密集的采样点作为训练样本点。此外，将高斯概率作为回归样本的权重，来对靠近目标中心的目标赋予更高的权重。之后，作者还提出适当的规范化来利用大框的更多信息且保持小框的信息。论文提出的方法可以减少混淆核低质量样本而不需要任何的其他组件（如FPN），它也不需要位移预测来辅助定位，因此TTFNet是高效的、统一的、直观的。</p><p><img src="https://i.loli.net/2021/03/16/JmfaY9b1ihkDyXI.png" alt=""></p><p>整体来看，论文第一次提出了一个训练时间、推理速度和精度平衡良好的检测器，相比CenterNet和其他实时检测器减少了至少7倍的训练时间，并且效果上保证了sota水平。最快版本的TTFNet-18和TTFNet-53可以获得25.9的AP及112的FPS（8卡1080Ti需要1.8小时训练）和32.9的AP及55的FPS（8卡1080Ti需3.1小时训练），这是就作者所知，目前COCO上达到这一精度的最短训练时间。它们training from scratch分别需要19小时和32小时，获得的效果和任何SOTA<br>相比都不遑多让。</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>编码更多的训练样本和使用更大的batch size都可以为每一步的更新提供更多的监督信号，加快模型的训练。这里的训练样本指的是依据标注框编码的特征。回顾前人的工作，线性缩放规则于2017年被首次提出，它指的是，一般情况下batch size变为k倍，学习率也需要调整为k倍。而事实上，作者经过公式推导发现，对目标检测而言，训练样本量增加和batch size增加是几乎等效的。</p><p>CenterNet作为一种新的anchor-free检测器，推理速度几倍于当前检测器，但是需要长时间的训练，这是因为模型相对简单从而需要大量的数据增强，这些数据增强虽然保证了模型性能的稳步上升但也减缓了模型的收敛速度。作者这里去掉了数据增强并且提高了学习率，得到如下图所示的结果，较大的学习率并不能加快模型的收敛，移除数据增强也导致了性能的急剧下降。发生的原因就是CenterNet只采用一个回归样本点，导致大量信息的丢失，从而需要复杂的数据增强和长时间训练，导致该网络训练上很不友好。</p><p><img src="https://i.loli.net/2021/03/16/VNkFlJhHz9qDT18.png" alt=""></p><p>为了消除CenterNet的这个弊端，作者推断采用一个更好的编码回归样本的策略是必要的。以此为出发点，TTFNet诞生了。</p><h2 id="TTFNet"><a href="#TTFNet" class="headerlink" title="TTFNet"></a>TTFNet</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>首先回顾一下CenterNet这个方法，它将目标检测视为两部分：<strong>中心定位</strong>和<strong>尺寸回归</strong>。对于定位问题，它采用和CornerNet一样的高斯核来产生热力图，这种方式可以保证网络在目标的中心附近产生更强力的激活值。对于回归问题，它定义处于目标中心的像素作为训练样本并且直接预测目标的宽高，同时也预测一个偏移（offset）来恢复由于下采样造成的离散化误差。由于网络推理时可以在目标中心点附近产生更强的激活值，因此NMS不是必须的，后处理步骤的计算开销可以忽略不计。</p><p>为了消除NMS的需求，因此TTFNet这篇文章的<strong>中心定位</strong>和CenterNet是类似的，只不过进一步考虑了高斯核中边框的宽高比，这是CenterNet没有考虑的，因此CenterNet的策略是次优的。</p><p>而对于<strong>尺寸回归</strong>，主流的方法是及那个整个标注框内的像素点或者标注框内子框区域内的像素点作为训练样本。作者则认为，应当将整个高斯区域内的所有像素点作为训练样本。此外，基于目标尺寸和高斯概率计算的权重被应用到这些样本上以便于更好的特征提取，整个框架如下图所示，并不需要其他的预测分支来辅助纠错，因此是简单且有效的检测框架。</p><p><img src="https://i.loli.net/2021/03/16/JmfaY9b1ihkDyXI.png" alt=""></p><h3 id="训练用高斯核"><a href="#训练用高斯核" class="headerlink" title="训练用高斯核"></a>训练用高斯核</h3><p>类似CenterNet，TTFNet对于输入图像会得到两个预测特征$\hat{H} \in R^{N \times C \times \frac{H}{r} \times \frac{W}{r}}$和$\hat{S} \in R^{N \times 4 \times \frac{H}{r} \times \frac{W}{r}}$，前者用于表明目标中心的位置后者则用来获取关于目标尺寸的信息。其中的$N$、$C$、$H$、$W$和$r$分别表示batch size、类别数目、输入图像的宽和高以及输出缩放比例（下采样率）。实验中设置$C=80$且$r=4$，为了方便叙述后面都不再提及$N$这个维度，在论文的方法中高斯核既用于定位也用于回归，定义了两个标量$\alpha$和$\beta$分别来控制核的尺寸。</p><h3 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h3><p>假定属于第$c_m$类的第$m$个标注框，首先它会被线性映射到特征图的尺度上。然后，2D高斯核$\mathbf{K}_{m}(x, y)= \exp \left(-\frac{\left(x-x_{0}\right)^{2}}{2 \sigma_{x}^{2}}-\frac{\left(y-y_{0}\right)^{2}}{2 \sigma_{y}^{2}}\right)$会被用来生成$H_{m} \in R^{1 \times \frac{H}{r} \times \frac{W}{r}}$，这里的$\sigma_{x}=\frac{\alpha w}{6}, \sigma_{y}=\frac{\alpha h}{6}$。最后，通过用$H_m$进行逐元素最大值求解来更新$H$上第$c_m$个通道。可以看到，其实$H_m$的获得依赖于参数$\alpha$、中心位置$(x_0,y_0)_m$和边框尺寸$(h,w)_m$。使用取整操作$\left(\left\lfloor\frac{x}{r}\right\rfloor,\left\lfloor\frac{y}{r}\right\rfloor\right)$来限制中心像素的位置必然是整数，这和CenterNet一致，这里$\alpha$被设置为0.54，这并非精心设计的值。</p><p>高斯分布的峰值即边框中心的像素，被视为正样本，其他的均为负样本，这里损失采用modified focal loss，它在预测的$\hat{H}$和定位目标$H$之间计算，计算式如下，其中的$\alpha_f$和$\beta_f$为focal loss和modified focal loss的超参数，$M$代表标注框的数目，论文设置$\alpha_f = 2$而$\beta_f = 4$。</p><script type="math/tex; mode=display">L_{l o c}=\frac{1}{M} \sum_{x y c}\left\{\begin{array}{l}\left(1-\hat{H}_{i j c}\right)^{\alpha_{f}} \log \left(\hat{H}_{i j c}\right) \quad \text { if } H_{i j c}=1 \\\left(1-H_{i j c}\right)^{\beta_{f}} \hat{H}_{i j c}^{\alpha_{f}} \log \left(1-\hat{H}_{i j c}\right) \quad \text { else }\end{array}\right.</script><h3 id="尺寸回归"><a href="#尺寸回归" class="headerlink" title="尺寸回归"></a>尺寸回归</h3><p>考虑已经被映射到特征图尺度上的第$m$个标注框，另一个高斯核被用来产生$S_{m} \in R^{1 \times \frac{H}{r} \times \frac{W}{r}}$，这个高斯核的参数由$\beta$控制，若$\alpha$和$\beta$相等，可以使用同一个高斯核。在$S_m$中的非零部分定义为高斯区域$A_m$，如下图所示，蓝色框代表标注框，下图黑色区域代表样本点，下图是几种选择样本的方式。由于$A_m$始终在第$m$个标注框内部，因此下文均简称为子区域。</p><p><img src="https://i.loli.net/2021/03/16/TWyLmBPZUS9g7jK.png" alt=""></p><p>每个子区域内的像素均被视作回归样本，考虑$A_m$中的一个像素$(i,j)$以及下采样率$r$，回归的目标就是从$(ir,jr)$到第$m$个标注框的四个边的距离，用一个四维向量表示为$\left(w_{l}, h_{t}, w_{r}, h_{b}\right)_{i j}^{m}$。在$(i,j)$处的预测框则表示如下。</p><script type="math/tex; mode=display">\begin{array}{l}\hat{x}_{1}=i r-\hat{w}_{l} s, \hat{y}_{1}=j r-\hat{h}_{t} s \\\hat{x}_{2}=i r+\hat{w}_{r} s, \quad \hat{y}_{2}=j r+\hat{h}_{b} s\end{array}</script><p>上式中的$s$是一个固定的标量用于增大预测结果以便于优化，在论文中设置$s=16$，而且这里的$\left(\hat{x}_{1}, \hat{y}_{1}, \hat{x}_{2}, \hat{y}_{2}\right)$是在图像尺度上而不是特征图尺度上的。若一个像素不在任何子区域中，那么训练时他将被忽略。如果一个像素存在于多个子区域里，那么这是一个有歧义的样本，它的GT为具有最小面积的目标。</p><p>考虑预测$\hat{S}$和回归目标$S$，从$S$中收集训练目标$S^{\prime} \in R^{N_{r e g} \times 4}$和相应的预测结果$\hat{S}^{\prime} \in R^{N_{r e g} \times 4}$（来自$\hat{S}$），这里的$N_{reg}$表示回归样本的数目。对这些所有的样本，利用上面的式子解码出预测的边框和相应的标注框，通过GIOU计算损失，因此回归分支的损失如下。</p><script type="math/tex; mode=display">L_{r e g}=\frac{1}{N_{r e g}} \sum_{(i, j) \in A_{m}} \operatorname{GIoU}\left(\hat{B}_{i j}, B_{m}\right) \times W_{i j}</script><p>上式中的$\hat{B_{ij}}$表示预测框$\left(\hat{x}_{1}, \hat{y}_{1}, \hat{x}_{2}, \hat{y}_{2}\right)_{i j}$，而$B_{m}=\left(x_{1}, y_{1}, x_{2}, y_{2}\right)_{m}$表示真实的图像尺度上的第$m$个标注框。$W_{ij}$表示采样权重，用来平衡每个样本对损失的贡献。</p><p>继续考虑到目标的尺度变化可能是很大的，大目标也许会产生数千的样本而小目标仅仅会产生非常少量的样本，再依照样本贡献度标准化损失之后，小样本带来的损失是可以忽略不计的，这对检测器在小目标上的性能损害是很大的。因此上述的采样权重$W_{ij}$在损失平衡方面尤其重要。假定$(i,j)$在第$m$个标注的子区域$A_m$中，$W_{ij}$的计算式如下。</p><script type="math/tex; mode=display">W_{i j}=\left\{\begin{array}{ll}\log \left(a_{m}\right) \times \frac{\mathrm{G}_{m}(i, j)}{\sum_{(x, y) \in A_{m}} \mathrm{G}_{m}(x, y)} & (i, j) \in A_{m} \\0 & (i, j) \notin A\end{array}\right.</script><p>这个式子里的$\mathrm{G}_{m}(i, j)$表示$(i,j)$位置处的高斯概率，$a_m$是第$m$个标注框的面积。该方案可以很好地利用大目标中包含的更多注释信息，同时保留小目标中的注释信息。它还可以强调目标中心附近的这些样本，减少模糊和低质量样本的影响。</p><h3 id="总体损失"><a href="#总体损失" class="headerlink" title="总体损失"></a>总体损失</h3><p>讲完最关键的size regression之后，最终的损失如下式所述，它是定位损失和回归损失的加权求和结果，两个收工设计的权重$w_{loc}$和$w_{reg}$分别是1.0和5.0。</p><script type="math/tex; mode=display">L=w_{\text {loc }} L_{\text {loc }}+w_{\text {reg }} L_{r e g}</script><h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><p>整个TTFNet的结构与如下图所示，论文中backbone采用ResNet和DarkNet，特征图通过MDCN和上采样层调整到原图的1/4分辨率。这个特征图随后分别进入两个head中，定位head（下图上面分支）用来产生目标中心附近激活值更强的热度图，回归head（下图下面分支）直接预测定位目标中心到边框四个边的距离。由于目标中心是热度图上的局部最大值，因此不需要nms只需要使用最大池化即可定位目标。通过这个方法找到的局部最大值用来产生回归结果，进而完成目标检测。</p><p><img src="https://i.loli.net/2021/03/16/JmfaY9b1ihkDyXI.png" alt=""></p><p>TTFNet这种设计高效利用了大尺寸和中等尺寸目标的标注信息，但是受限于小目标信息的确过少，所以作者又通过短路连接将高分辨率低语义信息的特征图传到最后，这些特征图来自stage2、stage3和stage4。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>这篇论文进行了大量的消融实验论证论文提出的方法，采用的模型为TTFNet-53。首先验证了回归权重$W_{ij}$的重要性，每个标注框产生的样本量是不同的，这点上面已经阐述了，主要是大目标训练样本很多而小目标训练样本很少。不进行任何平衡处理的结果如下表的第一列，结果很差。有个直接的策略就是对标注框的损失按照样本量进行标准化，如第$m$个标注框有$n_m$个样本，那么损失就乘以权重$\frac{1}{n_{m}}$，这个结果也不是最优的，因为模型丧失了利用大目标更多信息的机会。这个结果是下表的第二列。下标的Sqrt和Log则表示对边框区域面积sqrt和log的结果作为采样权重。最后，采用文中提出的高斯概率配合面积log值作为采样权重获得了最好的效果，如下表最后一列，它能解决上述的其他方法的问题。</p><p><img src="https://i.loli.net/2021/03/16/ITFhYgUdLe6bpDE.png" alt=""></p><p>这种高斯核的设计引入回归权重中会优雅且高效地处理低质量或有歧义的样本，这个有歧义的样本指的是定位在重叠区域的样本，而低质量样本指的是远离目标中心的样本。此前已经有不少方法解决这两种问题样本，如centerness分支，高斯核加权的引入可以毫无边缘效应地处理这两种样本，它会在标注框内部产生一个子区域，这个子区域的大小仅受到超参数$\beta$的控制，$\beta$越大则可以利用更多的信息也会引入更多的上述两种样本。关于$\beta$的选择，如下表所示。</p><p><img src="https://i.loli.net/2021/03/16/4nBPD853xptHj92.png" alt=""></p><p>实验表明，模型性能差的主要原因是低质量样本，引入高斯核可以有效选择训练样本，因为高斯核区域的样本都是目标高相关的。不同于CenterNet，作者还考虑了高斯核中的box的长宽比。如下图所示，显然，考虑长宽比会带来不小的收益。</p><p><img src="https://i.loli.net/2021/03/16/kPhMeIyqt7O3D6K.png" alt=""></p><p>之后，作者研究了短路连接给模型带来的收益（针对小目标），显然，是有效果的，结果如下图所示。</p><p><img src="https://i.loli.net/2021/03/16/9b387NzA6rBdVKp.png" alt=""></p><p>样本量对学习率的影响，如下表所示，可以发现$\beta$越大引入样本越多，可以采用更大的学习率，这和使用更大的batch size效果类似。</p><p><img src="https://i.loli.net/2021/03/16/ieDJtnmRH5yPjpQ.png" alt=""></p><p>下图是traing from scratch的结果，需要更长的训练时间。</p><p><img src="https://i.loli.net/2021/03/16/mlT9PgEdwNWqLKr.png" alt=""></p><p>下图则是喜闻乐见的和SOTA的对比，与一些比较新的推理很快的方法对比，精度更高训练耗时更短。</p><p><img src="https://i.loli.net/2021/03/16/mAbJnxL57sDeq6r.png" alt=""></p><p>最后，还和对标的CenterNet进行了对比，如下。</p><p><img src="https://i.loli.net/2021/03/16/XLhregVy72jqKwR.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TTFNet针对CenterNet训练样本不足的问题使用高斯核进行样本选择，是平衡训练时间、推理速度和准确性的优雅而高效的解决方案。理论分析和实验都很详实，是不可多得的好文章。设计的TTFNet大大减少了训练速度，对设备不太充足的研究者是很友好的，也适用于NAS等技术。本文只是做了一些基本的解读，如果我的文章对你有所帮助，欢迎一键三连，你的支持是我不懈创作的动力。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TTFNet解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TrackFormer解读</title>
      <link href="2021/03/14/trackformer/"/>
      <url>2021/03/14/trackformer/</url>
      
        <content type="html"><![CDATA[<blockquote><p>TrackFormer和TransTrack一样是基于Transformer在MOT上的工作，其中一位作者就是DETR的作者，相比TransTrack虽然MOTA上不是很高，但是IDF1和IDs都是好不少的，个人觉得也是值得关注的工作，甚至有点更简明的感觉。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>此前已有TransTrack首开先河，我也对此做了<a href="https://zhouchen.blog.csdn.net/article/details/112427217">解读</a>，这篇TrackFormer也是基于Transformer做的MOT工作，也参考了DETR的思路，引入了track query来以自回归的方式跟踪目标，这个track query由DETR检测器生成并且随着时间集成了对应目标的位置信息，而Transformer的decode在帧之间调整track query，从而跟随了目标位置的变化。TrackFormer因此以一种新的tracking-by-attention范式实现了一种无缝的帧间数据关联，注意力机制确保了模型同时考虑位置、遮挡和目标的识别特征。在MOT17和MOTS20上，TrackFormer取得了SOTA表现，进一步推动了Transformer在计算机视觉中的发展。</p><ul><li><p>论文标题</p><p>  TrackFormer: Multi-Object Tracking with Transformers</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2101.02702">http://arxiv.org/abs/2101.02702</a></p></li><li><p>论文源码</p><p>  暂未开源</p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在人群密集的场景中跟踪多个目标其实是非常大的挑战，因为对人而言集中注意力关注一个目标已经不是很容易的事情，因此将Transformer这种强大的自注意力引入MOT任务中，是一种很好的选择。而随着图像级别的目标检测器的发展，大部分MOT方法都采用tracking-by-detection的思路，它主要分为两步：首先，在单帧上检测出目标，接着在帧间检测结果之间进行关联，从而形成沿着时间的轨迹。</p><p>很多方法在数据关联这一步做了研究，关联其实也才是跟踪的核心问题。传统的TBD（tracking-by-detection）方法关联检测框采用时序的稀疏或密集图优化方法，或者使用卷积神经网络预测检测框之间的匹配分数。最近的一些工作则产生了一些新的思路，如tracking-by-regression方法，这类方法中检测器不仅仅输出逐帧的检测结果，同时还取代了数据关联步骤，以轨迹位置变化的回归来实现。这类方法隐式地进行数据关联，不过还是依赖于额外的图优化或者运动和外观模型。这在很大程度上是由于缺乏对象标识和局部边界框回归的概念。</p><p>这篇论文中，作者提出了一种将MOT问题视为tracking-by-attention的方法，即TrackFormer。TrackFormer以一个统一的方式同时实现目标检测和数据关联，如下图所示，TrackFormer基于CNN和Transformer（基于DETR中的结构）实现轨迹的形成。就像我之前在<a href="https://zhouchen.blog.csdn.net/article/details/107006263">Transformer的文章</a>中所说，这篇论文的主要工作其实也集中在decoder的query上。它通过新提出的track query以自回归方式在视频序列中在空间和时间上跟踪一个对象。在每一帧上，模型会对多个track query进行变换，这个track query表示对应目标的空间位置。Transformer在帧级特征和track query上执行注意力操作，以推理目标位置和身份（伴随遮挡）以及新物体的出现。新目标的出现是同一个Transformer以统一的方式为新进入场景的目标生成track query来实现的。</p><p><img src="https://i.loli.net/2021/03/14/tFp1Nm8IkAMZWb2.png" alt=""></p><p>TrackFormer可以端到端训练完成检测和跟踪任务，和DETR类似，优化目标也是一个集合预测损失。它实现了与注意力的隐式关联轨迹，不需要额外的匹配，优化或者运动和外观的建模。在benchmark评估中，将TrackFormer应用到MOT17数据集上，它达到了SOTA表现。此外，我们展示了我们的模型输出分割掩模的灵活性，并展示了多目标跟踪和分割（MOTS20）上SOTA成果。</p><h2 id="TrackFormer"><a href="#TrackFormer" class="headerlink" title="TrackFormer"></a>TrackFormer</h2><p>TrackFormer的核心其实是提出了一个track query的东西，这和TransTrack中的learned query结合track feature的思路非常类似，它的整体结构如下图所示。</p><p><img src="https://i.loli.net/2021/03/14/7d6l98S1NtOZvfa.png" alt=""></p><p>论文首先回顾了DETR并进行了一些数学上的推导，我这里就在已了解DETR的基础上直接这篇论文的工作了。首先我们看上图最左侧初始帧上，其实这就是标准的DETR过程（因此后面CNN特征送入encoder就不讲解了），直接看右侧decoder过程，最下面的白色框框表示learnable object query，共有$N_{object}$个（一般大于单帧最大目标数），它会查询到对应数目的output embedding，这个object embedding一方面用于后续检测任务的head中，如边框回归和类别预测。对那些成功预测出目标（即非背景类）的output embedding（图上的红色、绿色和蓝色框），还将其初始化为传入下一帧的track query。<strong>这里需要注意的是，不算head层，其实Transformer结构会将输入进行频繁的自注意力，但是输出获得内容信息的embedding维度是不变的，这里可以理解为一个黑盒，因此这里初始化的track query其实和后面每帧为了新目标检测使用的object query是同维的。</strong></p><p>下面，对于不是第一帧的后续帧而言，decoder输入的query不仅仅有每帧初始化用于检测的$N_{object}$个object query，还有上一帧已经成功检测的目标的$N_{track}$个track query（显然它不是固定的，而是依赖于上一帧的），共有$N_{object}+N_{track}$个query，decoder接受这个query之后查询到当前帧的检测结果（我这里的理解其实是，object query用于空间上查询，track query带有时序信息进行查询），所以指导训练的集合预测损失变为下式，其中$\hat{\sigma}$表示最接近的预测和GT。</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{set}}(y, \hat{y}, \hat{\sigma})=\sum_{i=1}^{N_{\mathrm{object}}+N_{\text {track }}} \mathcal{L}_{\mathrm{object}}\left(y, \hat{y}_{i}, \hat{\sigma}\right)</script><p>因此，track query查到的目标如果成功检测到了，那就赋予同一个id（如上图中间部分的前面的红绿蓝框），没检测到则表示目标消失（如上图右边部分的蓝色框），那些object query检测成功（非背景类，上图中不打叉的）的则作为新目标。接着，这些新旧目标的output embedding一起作为下一帧的track query。<strong>这样，以一种相对优雅的方式完成了数据关联以致整个跟踪任务。</strong></p><p><img src="https://i.loli.net/2021/03/14/ujtLrIhNTJnUQS1.png" alt=""></p><p>从Transformer的结构上来看，其结构如上图所示，这很容易看懂，我就不多赘述了。这里唯一需要说明的是，其实track query并非完全和object query同等对待的，它先经过了一个额外的多头自注意力层（track query attention）来对其预处理，这个操作的理由其实是因为前一帧的output embedding用于目标的分类和回归任务，与objectquery并不在一个空间内。经过一个变换再和object query进行concat会好一些。</p><p>从整体上来看，TrackFormer可以被理解为一个基于自适应回归track query的持续的重检测跟踪目标的过程。其实无论是object query还是track query，其实query都是潜在的当前帧目标的表示，只是track query带有时序信息而已，而将其引入下一帧的检测，当注意力在整个集合上允许时，会自动避免重复目标的检测。此外，从整体上看，只要轨迹一直存在，其实目标的track query其实会被逐帧更新，TrackFormer因此也实现了一种隐式的多帧注意力。</p><h2 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h2><p>就训练来看，损失是和DETR类似的集合预测损失，集合预测损失通过匹配预测结果和GT之后计算二者损失得到。不过，TrackFormer的GT分配策略和DETR不同，它分为两步，先处理track query再处理object query。</p><p>不妨记第$t$帧上的轨迹集为$K_{t} \subset K$，上一帧的检测结果被安排的id是上一帧$K_{t-1} \subset K$中的，它带着id信息作为当前帧的track query，如果当前帧的$K_t$中仍然存在对应的id，那么将该id的GT分配给该track query，否则给予其背景类表示该目标没出现过，剩下的没有分配的id则按照DETR的思路分配给object query。</p><p>至此，TrackFormer就完成了训练过程，从这个过程不难看出，它的训练是基于两帧样本的。之后，为了更好地训练及构建泛化能力足够的track query，使用下面三种数据增强策略。</p><ol><li>时序增强，不仅仅采用相邻帧，而是由一定局部范围内随机选择的两帧构成训练样本；</li><li>在第二帧输入track query时，对其按一定比率（$p_{FN}$）进行抹除，以消除对前一帧的依赖。这个操作可以减少对track query的依赖，从而保证检测和跟踪的平衡性。</li><li>为了处理轨迹被赋予背景类意外终止（一般发生在遮挡严重的情况下），随机从前一帧的背景类output embedding中选择一部分作为当前帧的track query。</li></ol><p>就推理过程来看，就比较简单了，在初始帧上直接进行DETR就行，在后续帧上decoder依据track query和object query进行当前帧的检测。如果object query得到的框置信度高于阈值$\sigma_{detection}$则认为新目标产生，track query的置信度如果小于阈值$\sigma_{track}$则认为轨迹终止，同时对track query的结果进行track nms消除高遮挡情况。至于我们平时比较关心也很费资源的轨迹暂存和找回，TrackFormer没有考虑。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>关于是否进行数据增强、是否需要后处理的消融实验我这里就不提了，可以参考原论文，下面是MOT17和MOTS20上与SOTA方法的对比。就Transformer引入MOT中这个创新来看，其结果是相当不错的，而且相比于TranTrack，IDs可以说是大幅度降低，相当不错。</p><p><img src="https://i.loli.net/2021/03/14/1WAktpunOSwIC2c.png" alt=""></p><p><img src="https://i.loli.net/2021/03/14/JiECTU83KNhPY4m.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TrackFormer提出了一种全新的MOT框架，并将Transformer引入了MOT中，通过track query这个设计完成了DETR到track with DETR思路的迁移，并在MOT17等benchmark上取得了SOTA表现，是值得关注的工作。最后，如果我的文章对你有所帮助，欢迎一键三连，你的支持是我不懈创作的动力。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TrackFormer解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SA-Net解读</title>
      <link href="2021/03/08/sa-net/"/>
      <url>2021/03/08/sa-net/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇文章是南京大学Yu-Bin Yang等人于2021年初开放的一篇文章，已经被收录于ICASSP2021，文章提出了一种新的视觉注意力机制，称为Shuffle Attention（置换注意力），它通过置换单元组合空间注意力和通道注意力，相比此前的混合注意力更加高效，是一种非常轻量的注意力结构。实验表明，在ImageNet、COCO等benchmark上，超越了当前的SOTA注意力模型如SE、SGE等，且拥有更低的计算复杂度和参数量。</p><ul><li><p>论文标题</p><p>  SA-Net: Shuffle Attention for Deep Convolutional Neural Networks</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2102.00240">http://arxiv.org/abs/2102.00240</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/wofmanaf/SA-Net">https://github.com/wofmanaf/SA-Net</a></p></li></ul><p><img src="https://i.loli.net/2021/03/08/BU2gOqlutNsfJvH.png" alt=""></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>注意力机制如今已经被广泛用于卷积神经网络中，大大提升了很多任务上的性能表现。目前视觉中的注意力机制主要有两种，如下图所示，分别是<strong>通道注意力</strong>和<strong>空间注意力</strong>（我在<a href="https://zhouchen.blog.csdn.net/article/details/111302952">之前的文章</a>介绍了视觉中一些比较有名的注意力方法，可以访问查看）。</p><p><img src="https://i.loli.net/2021/03/08/qZLEOnafbC4Nxk9.png" alt=""></p><p>通道注意力着重于捕获通道间的依赖而空间注意力则关于像素间的关系捕获，不过它们都是通过不同的聚合策略、转换方法和强化函数来从所有位置聚合相同的特征来强化原始特征。CBAM和GCNet同时处理空间和通道信息，获得了较好的精度提升，然而它们通常存在收敛困难、计算负担大等问题。也有一些工作关于简化注意力结构，如ECA-Net将SE模块中通道权重的计算改用了1D卷积来简化。SGE沿着通道维度对输入进行分组形成表示不同语义的子特征，在每个子特征上进行空间注意力。遗憾的是，这些方法都没用很好地利用空间注意力和通道注意力之间的相关性，所以效率偏低，所以自然而然引出这篇文章的出发点：能否以一个高效轻量的方式融合不同的注意力模块？</p><p>不妨先回顾一下轻量级网络的代表之一的ShuffleNetv2，它构建了一个可并行的多分支结构，如下图所示，在每个单元的入口，含有$c$个通道的输入被分割为$c-c’$和$c’$两个分支，接着几个卷积层用来提取关于输入更高级的信息，然后结果concat到一起保证和输入通道数相同，最后channel shuffle操作用来进行两个分支之间的信息通讯。类似的，SGE将输入沿着通道分组，所有子特征并行增强。</p><p><img src="https://i.loli.net/2021/03/08/Bvy21fpmVnxuReO.png" alt=""></p><p>基于这些前人的工作，论文作者提出了一种更加轻量但是更加高效的Shuffle Attention（SA）模块，它也是将输入按照通道进行分组，对每组子特征，使用Shuffle Unit来同时构建通道注意力和空间注意力。对每个注意力模块，论文还设计了针对每个位置的注意力掩码来抑制噪声加强有效的语义信息。论文主要的贡献为设计了一个轻量但是有效的注意力模块，SA，它将输入特征图按照通道分组并对每个分组用Shuffle Unit实现空间和通道注意力.</p><h2 id="Shuffle-Attention"><a href="#Shuffle-Attention" class="headerlink" title="Shuffle Attention"></a>Shuffle Attention</h2><p><img src="https://i.loli.net/2021/03/08/U2Pxr5bm1tO7kGe.png" alt=""></p><p>所谓一图胜千言，这篇文章其实忽略掉一些细节上图就已经展示了Shuffle Attention模块（SA模块）的设计了，不过我这里还是按照论文的思路来逐步理解作者的设计，整个SA模块其实分为三个步骤，分别为<strong>特征分组</strong>、<strong>混合注意力</strong>、<strong>特征聚合</strong>。</p><h3 id="Feature-Grouping"><a href="#Feature-Grouping" class="headerlink" title="Feature Grouping"></a>Feature Grouping</h3><p>首先来看特征分组这个操作，它将输入特征图分为多组，每组为一个子特征（sub-feature）。具体来看，输入特征图$X \in \mathbb{R}^{C \times H \times W}$被沿着通道维度分为$G$组，表示为$X=\left[X_{1}, \cdots, X_{G}\right], X_{k} \in \mathbb{R}^{C / G \times H \times W}$，其中每个子特征$X_k$随着训练会逐渐捕获一种特定的语义信息。这部分对应上图最左边的<strong>Group</strong>标注的部分。</p><h3 id="Channel-Attention-and-Spatial-Attention"><a href="#Channel-Attention-and-Spatial-Attention" class="headerlink" title="Channel Attention and Spatial Attention"></a>Channel Attention and Spatial Attention</h3><p>接着，$X_k$会被分为两个分支，依然是沿着通道维度划分，两个子特征表示为$X_{k 1}, X_{k 2} \in \mathbb{R}^{C / 2 G \times H \times W}$，如上图中间<strong>Split</strong>标注后的部分，两个分支分别是绿色和蓝色表示，上面的绿色分支实现通道注意力开采通道间的依赖，下面的蓝色分支则捕获特征之间的空间依赖生成空间注意力图，这样，模型同时完成了语义和位置信息的注意。</p><p>具体来看通道注意力这个分支，这里可以作者没用采用SE模块的设计，主要是考虑到轻量化设计的需求（SE的参数还是比较多的），也没有使用ECA-Net的设计采用一维卷积（ECA-Net要想精度高，对卷积核尺寸要求比较大），而是采用最简单的GAP+Scale+Sigmoid的单层变换，公式如下。</p><script type="math/tex; mode=display">s=\mathcal{F}_{g p}\left(X_{k 1}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} X_{k 1}(i, j)</script><script type="math/tex; mode=display">X_{k 1}^{\prime}=\sigma\left(\mathcal{F}_{c}(s)\right) \cdot X_{k 1}=\sigma\left(W_{1} s+b_{1}\right) \cdot X_{k 1}</script><p>上面的式子只有两个变换参数，即$W_{1} \in \mathbb{R}^{C / 2 G \times 1 \times 1}$和$b_{1} \in \mathbb{R}^{C / 2 G \times 1 \times 1}$。</p><p>下面来看空注意力这个分支，思路也很简单，先是对输入特征图进行Group Norm，然后也是通过一个变换$\mathcal{F}_{c}(\cdot)$来增强输入的表示，具体公式如下。这里的参数也只有$W_2$和$b_2$，它们的尺寸都是$\mathbb{R}^{C / 2 G \times 1 \times 1}$。</p><script type="math/tex; mode=display">X_{k 2}^{\prime}=\sigma\left(W_{2} \cdot G N\left(X_{k 2}\right)+b_{2}\right) \cdot X_{k 2}</script><p>最后，两种注意力的结果被concat到一起$X_{k}^{\prime}=\left[X_{k 1}^{\prime}, X_{k 2}^{\prime}\right] \in \mathbb{R}^{C / G \times H \times W}$，此时它已经和该组的输入尺寸一致了。</p><h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h3><p>最后一步的聚合也很简单，通过ShuffleNetv2采用的channel shuffle操作来保证各组子特征之间的交互，最后得到和输入$X$同维的注意力图。这是上图中最右边的部分。</p><p>至此，SA模块的构建已经完成，其实$W_1$、$b_1$、$W_2$和$b_2$就是整个SA模块所有的参数，它可以通过PyTorch轻易的实现。SA模块可以取代SE模块，因此替换SENet的网络称为SA-Net。</p><p><img src="https://i.loli.net/2021/03/08/trHARNiDCfo6kZK.png" alt=""></p><p>为了验证SA模块对语义信息的提取能力，作者在ImageNet上训练了没用channel-shuffle的SA-Net50B和有channel-shuffle的SA-Net50，给出精度对比如下图。一方面在SA采用之后，top1精度表示出了提升，这也就意味着特征分组可以显著提升特征的语义表达能力；另一方面，不同类别的分布在前面的层中非常相似，这也就意味着在前期特征分组的重要性被不同类别共享，而随着深度加深，不同的特征激活表现出了类别相关性。</p><p><img src="https://i.loli.net/2021/03/08/GaLpREQmSoKOrBX.png" alt=""></p><p>为了验证SA有效性，基于GradCAM进行可视化，得到下图的结果，可以发现，SA模块使得分类模型关注于目标信息更相关的区域，进而有效的提升分类精度。</p><p><img src="https://i.loli.net/2021/03/08/wkSidlBmEcJ4TYj.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>为了验证性能，作者也在ImageNet、COCO上进行了分类、检测和分割任务的实验，分别如下。</p><p><img src="https://i.loli.net/2021/03/08/jwUe8ErmDzhgqCo.png" alt=""></p><p><img src="https://i.loli.net/2021/03/08/8MAJWjps96YPZRn.png" alt=""></p><p><img src="https://i.loli.net/2021/03/08/jO6mX9BhzsnDIkH.png" alt=""></p><p>实验表明，SA模块相比SE等注意力方法在参数量更少的情况下，达到了更高的精度，是非常高效的轻量级注意力机制。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从ShuffleNet得到启发，作者设计了轻量级网络上适用的高效注意力机制Shuffle Attention，推动了注意力在轻量级网络上的发展，在多个任务上达到SOTA表现。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SA-Net解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CoordAttention解读</title>
      <link href="2021/03/08/coordattention/"/>
      <url>2021/03/08/coordattention/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在轻量级网络上的研究表明，通道注意力会给模型带来比较显著的性能提升，但是通道注意力通常会忽略对生成空间选择性注意力图非常重要的位置信息。因此，新加坡国立大学的Qibin Hou等人提出了一种为轻量级网络设计的新的注意力机制，该机制将位置信息嵌入到了通道注意力中，称为coordinate attention（简称CoordAttention，下文也称CA），该论文已被CVPR2021收录。不同于通道注意力将输入通过2D全局池化转化为单个特征向量，CoordAttention将通道注意力分解为两个沿着不同方向聚合特征的1D特征编码过程。这样的好处是可以沿着一个空间方向捕获长程依赖，沿着另一个空间方向保留精确的位置信息。然后，将生成的特征图分别编码，形成一对方向感知和位置敏感的特征图，它们可以互补地应用到输入特征图来增强感兴趣的目标的表示。</p><p><img src="https://i.loli.net/2021/03/08/j5ytfTsdqpRFkh9.png" alt=""></p><p>CoordAttention简单灵活且高效，可以插入经典的轻量级网络（如MobileNetV2）在几乎不带来额外计算开销的前提下，提升网络的精度。实验表明，CoordAttention不仅仅对于分类任务有不错的提高，对目标检测、实例分割这类密集预测的任务，效果提升更加明显。</p><ul><li><p>论文标题</p><p>  Coordinate Attention for Efficient Mobile Network Design</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2103.02907">http://arxiv.org/abs/2103.02907</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/Andrew-Qibin/CoordAttention">https://github.com/Andrew-Qibin/CoordAttention</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><a href="https://zhouchen.blog.csdn.net/article/details/111302952">注意力机制</a>常用来告诉模型需要更关注哪些内容和哪些位置，已经被广泛使用在深度神经网络中来加强模型的性能。然而，在模型容量被严格限制的轻量级网络中，注意力的应用是非常滞后的，这主要是因为大多数注意力机制的计算开销是轻量级网络负担不起的。</p><p><img src="https://i.loli.net/2021/03/08/97YAhCH2wjGNiVR.png" alt=""></p><p>考虑到轻量级网络有限的计算能力，目前最流行的注意力机制仍然是SENet提出的SE Attention。如上图所示，它通过2D全局池化来计算通道注意力，在相当低的计算成本下提供了显著的性能提升。遗憾的是，SE模块只考虑了通道间信息的编码而忽视了位置信息的重要性，而位置信息其实对于很多需要捕获目标结构的视觉任务至关重要。因此，后来CBAM等方法通过减少通道数继而使用大尺寸卷积来利用位置信息，如下图所示。然而，卷积仅仅能够捕获局部相关性，建模对视觉任务非常重要的长程依赖则显得有些有心无力。</p><p><img src="https://i.loli.net/2021/03/08/pEU62YbflIPk7KD.png" alt=""></p><p>因此，这篇论文的作者提出了一种新的高效注意力机制，通过将位置信息嵌入到通道注意力中，使得轻量级网络能够在更大的区域上进行注意力，同时避免了产生大量的计算开销。为了缓解2D全局池化造成的位置信息丢失，论文作者将通道注意力分解为两个并行的1D特征编码过程，有效地将空间坐标信息整合到生成的注意图中。更具体来说，作者利用两个一维全局池化操作分别将垂直和水平方向的输入特征聚合为两个独立的方向感知特征图。然后，这两个嵌入特定方向信息的特征图分别被编码为两个注意力图，每个注意力图都捕获了输入特征图沿着一个空间方向的长程依赖。因此，位置信息就被保存在生成的注意力图里了，两个注意力图接着被乘到输入特征图上来增强特征图的表示能力。由于这种注意力操作能够区分空间方向（即坐标）并且生成坐标感知的特征图，因此将提出的方法称为坐标注意力（coordinate attention）。</p><h2 id="Coordinate-Attention"><a href="#Coordinate-Attention" class="headerlink" title="Coordinate Attention"></a>Coordinate Attention</h2><p>相比此前的轻量级网络上的注意力方法，coordinate attention存在以下优势。首先，它不仅仅能捕获跨通道的信息，还能捕获方向感知和位置感知的信息，这能帮助模型更加精准地定位和识别感兴趣的目标；其次，coordinate attention灵活且轻量，可以被容易地插入经典模块，如MobileNetV2提出的inverted residual block和MobileNeXt提出的 sandglass block，来通过强化信息表示的方法增强特征；最后，作为一个预训练模型，coordinate attention可以在轻量级网络的基础上给下游任务带来巨大的增益，特别是那些存在密集预测的任务（如语义分割）。 </p><p><img src="https://i.loli.net/2021/03/08/5azgNKbvPdkZtBL.png" alt=""></p><p>一个coordinate attention模块可以看作一个用来增强特征表示能力的计算单元。它可以将任何中间张量$\mathbf{X}=\left[\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{C}\right] \in \mathbb{R}^{C \times H \times W}$作为输入并输出一个有着增强的表示能力的同样尺寸的输出$\mathbf{Y}=\left[\mathbf{y}_{1}, \mathbf{y}_{2}, \ldots, \mathbf{y}_{C}\right]$。</p><h3 id="SE模块"><a href="#SE模块" class="headerlink" title="SE模块"></a>SE模块</h3><p>由于CA（coordinate attention）是基于SENet的思考，所以首先来回顾一下SE Attention（详细关于SENet的解读可以参考<a href="https://zhouchen.blog.csdn.net/article/details/110826497">我的博文</a>）。标准的卷积操作是很难建立通道之间的关系的，但是显式建模通道之间的关系可以增强模型对信息通道的敏感性，从而对最终的决策产生更多的影响。因此，SE模块对通道关系进行显式建模，取得了突破性的进展。</p><p><img src="https://i.loli.net/2021/03/08/97YAhCH2wjGNiVR.png" alt=""></p><p>从上图的结构上来看，SE模块可以分为两步：<strong>压缩（squeeze）</strong>和<strong>激励（excitation）</strong>，分别用于全局信息的嵌入和自适应通道关系的加权。给定输入$X$，第$c$个通道的squeeze操作可以表述如下式，$z_c$就是第$c$个通道的输出。输入$X$来自固定核大小的卷积层，因此可以被看作一堆局部描述的集合。squeeze操作使得模型能够收集全局的信息。</p><script type="math/tex; mode=display">z_{c}=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} x_{c}(i, j)</script><p>SE模块的第二步是excitation操作，旨在完全捕获通道之间的依赖，可以表述如下式，这里的$\cdot$表示逐通道乘法，$\sigma$表示Sigmoid函数，$\hat{\mathbf{z}}$是通过变换函数生成的，变换函数为$\hat{\mathbf{z}}=T_{2}\left(\operatorname{ReLU}\left(T_{1}(\mathbf{z})\right)\right)$，其中$T_1$和$T_2$表示两个可学习的线性变换，用来捕获每个通道的重要性（对应上图的两个Fully Connected及其中间的部分）。</p><script type="math/tex; mode=display">\hat{\mathbf{X}}=\mathbf{X} \cdot \sigma(\hat{\mathbf{z}})</script><p>近些年来，SE模块被广泛使用在轻量级网络中，并且成为获得SOTA表现的关键组件。然而，它仅仅考虑了建模通道间的关系来对每个通道加权，忽略了位置信息，而位置信息对生成空间选择性特征图是分外重要的。因此，论文作者设计了一种新的同时考虑通道关系和位置信息的注意力模块，coordinate attention block（CA模块）。</p><h3 id="CA模块"><a href="#CA模块" class="headerlink" title="CA模块"></a>CA模块</h3><p>CA模块通过精确的位置信息对通道关系和长程依赖进行编码，类似SE模块，也分为两个步骤：坐标信息嵌入（coordinate information embedding）和坐标注意力生成（coordinate attention generation），它的具体结构如下图。</p><p><img src="https://i.loli.net/2021/03/08/5azgNKbvPdkZtBL.png" alt=""></p><p>首先，我们来看坐标信息嵌入这部分。全局池化常用于通道注意力中来全局编码空间信息为通道描述符，因此难以保存位置信息。为了促进注意力模块能够捕获具有精确位置信息的空间长程依赖，作者将全局池化分解为一对一维特征编码操作。具体而言，对输入$X$，先使用尺寸$(H,1)$和$(1,W)$的池化核沿着水平坐标方向和竖直坐标方向对每个通道进行编码，因此，高度为$h$的第$c$个通道的输出表述如下。</p><script type="math/tex; mode=display">z_{c}^{h}(h)=\frac{1}{W} \sum_{0 \leq i<W} x_{c}(h, i)</script><p>类似，宽度为$w$的第$c$个通道的输出表述如下。</p><script type="math/tex; mode=display">z_{c}^{w}(w)=\frac{1}{H} \sum_{0 \leq j<H} x_{c}(j, w)</script><p>上面这两个变换沿着两个空间方向进行特征聚合，返回一对方向感知注意力图。这和SE模块产生一个特征向量的方法截然不同，这两种变换也允许注意力模块捕捉到沿着一个空间方向的长程依赖，并保存沿着另一个空间方向的精确位置信息，这有助于网络更准确地定位感兴趣的目标。这个coordinate information embedding操作对应上图的X Avg Pool和Y Avg Pool这个部分。</p><p>接着，为了更好地利用上面coordinate information embedding模块产生的具有全局感受野并拥有精确位置信息的表示，设计了coordinate attention generation操作，它生成注意力图，遵循如下三个标准。</p><ul><li>首先，对于移动环境中的应用来说，这种转换应该尽可能简单高效；</li><li>其次，它可以充分利用捕获到的位置信息，精确定位感兴趣区域；</li><li>最后，它还应该能够有效地捕捉通道之间的关系，这是根本。</li></ul><p>作者设计的coordinate attention generation操作具体来看，首先级联之前模块生成的两个特征图，然后使用一个共享的1x1卷积进行变换$F_1$，表述如下式，生成的$\mathbf{f} \in \mathbb{R}^{C / r \times(H+W)}$是对空间信息在水平方向和竖直方向的中间特征图，这里的$r$表示下采样比例，和SE模块一样用来控制模块的大小。</p><script type="math/tex; mode=display">\mathbf{f}=\delta\left(F_{1}\left(\left[\mathbf{z}^{h}, \mathbf{z}^{w}\right]\right)\right)</script><p>接着，沿着空间维度将$\mathbf{f}$切分为两个单独的张量$\mathbf{f}^{h} \in \mathbb{R}^{C / r \times H}$和$\mathbf{f}^{w} \in \mathbb{R}^{C / r \times W}$，再利用两个1x1卷积$F_{h}$和$F_{w}$将特征图$\mathbf{f}^{h}$ and $\mathbf{f}^{w}$变换到和输入$X$同样的通道数，得到下式的结果。</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{g}^{h} &=\sigma\left(F_{h}\left(\mathbf{f}^{h}\right)\right) \\\mathbf{g}^{w} &=\sigma\left(F_{w}\left(\mathbf{f}^{w}\right)\right)\end{aligned}</script><p>然后对$g_h$和$g^w$进行拓展，作为注意力权重，CA模块的最终输出可以表述如下式。</p><script type="math/tex; mode=display">y_{c}(i, j)=x_{c}(i, j) \times g_{c}^{h}(i) \times g_{c}^{w}(j)</script><p>这部分coordinate attention generation对应上图剩余的部分，至此CA模块同时完成了水平方向和竖直方向的注意力，同时它也是一种通道注意力。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者采用下图所示的结构进行实验，验证设计的注意力机制的效果，分别是MobileNetV和MobileNeXt设计的两种残差模块。</p><p><img src="https://i.loli.net/2021/03/08/x2P3WGH8TVKUFIb.png" alt=""></p><p>作者首先对两个方向的必必要性进行验证，结果如下图，显然，两个方向都是必要的，CA模块可以在保证参数量的前提下，提高精度。</p><p><img src="https://i.loli.net/2021/03/08/aY7UefAJSsZGVg1.png" alt=""></p><p>接着，进行权重因子的消融实验，下图先后是MobileNetV2和MobileNeXt基础上的结果，CA模块均取得了最好的效果，无论以哪个模型为baseline或者选择怎样的权重因子，CA模块均靠设计上的优越性取得了最好效果。</p><p><img src="https://i.loli.net/2021/03/08/O8EsLI7RbKvcoiY.png" alt=""></p><p><img src="https://i.loli.net/2021/03/08/8YCHhWIB3KqvS5a.png" alt=""></p><p>关于下采样比例也做了实验，CA模块随着r的下调精度上升但是模型变大，依旧表现最佳，鲁棒性很强。</p><p><img src="https://i.loli.net/2021/03/08/fgxWbSXw9raqBvp.png" alt=""></p><p>之后，还对SE、CBAM和CA模块注意力结果可视化，大致能看出来CA更能精确关注感兴趣目标。</p><p><img src="https://i.loli.net/2021/03/08/AxY3uVqmnzileOb.png" alt=""></p><p>为了检验所提CA模块的性能，采用EfficientNet-b0作为baseline，作者简单地用CA模块代替SE模块。并和其他同样强大的网络对比，CA模块依旧有着强大的表现。</p><p><img src="https://i.loli.net/2021/03/08/Sc4GqBudWCsnrgF.png" alt=""></p><p>此外，作者还做了目标检测和语义分割任务上的实验，性能提升更大，由于位置信息的加入，这种依赖位置信息的密集预测效果明显更好，我这里就不贴了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>为了将空间信息加入通道注意力，论文作者设计了Coordinate Attention，在轻量级网络上取得了比较大的成功，它既能捕获通道之间的依赖也能很好地建模位置信息和长程依赖，实验表明其在图像识别、目标检测和语义分割任务上都有不错的改进。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CoordAttention解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ATSS解读</title>
      <link href="2021/02/28/atss/"/>
      <url>2021/02/28/atss/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Anchor-free方法成为单阶段检测之后又一个热门的目标检测的研究方向，不过实际上不论是anchor-based方法还是anchor-free方法，其最关键的区别其实在于正负样本的选择方式，这直接导致了它们效果的区别。这个观点在ATSS这篇文章中被提出，论文还认为如果anchor-free方法和anchor-based方法采用同一个正负样本的选择方式，那么无论是基于点还是基于anchor进行回归，效果差别不会特别大。由此，论文提出了自适应样本选择机制（Adaptive Training Sample Selection，ATSS）来根据目标的统计信息自动选择正负样本。这极大地改善了anchor-based方法和anchor-free方法并弥补了连各种方法论之间地性能鸿沟，将ATSS用到当前SOTA检测器上也能带来明显地任务涨点。</p><ul><li><p>论文标题</p><p>  Bridging the Gap Between Anchor-based and Anchor-free Detection via<br>Adaptive Training Sample Selection</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/1912.02424">http://arxiv.org/abs/1912.02424</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/sfzhang15/ATSS">https://github.com/sfzhang15/ATSS</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>卷积神经网络兴起后，目标检测长期被anchor-based方法通知，前后经历了二阶段方法到单阶段方法的转变。不过无论哪种方法都是在图像上平铺大量预定义的anchor，然后经过一次或者多次的anchor的类别预测和坐标修正，最后输出修正过的anchor作为检测结果。这里二阶段方法会对anchor进行多次修正，而单阶段只会进行一次修正，因此二阶段方法精度较高单阶段方法速度较快。</p><p><img src="https://i.loli.net/2021/02/28/TAiPCfUh9RDOIHe.png" alt=""></p><p>但是，最近目标检测研究的目光投向了anchor-free目标检测器的研究上，anchor-free的兴起和FPN和Focal Loss的提出是密不可分的（kaiming大神牛逼）。anchor-free不使用预定义的anchor直接检测目标，主要分为两种，分别是keypoint-based方法和center-based方法。前者首先定位一些预定义或者自学习的关键点接着对目标的空间范围进行约束。后者则使用目标的中心点或者中心区域定义正样本，然后预测正样本到目标边界的四个距离。无论是哪种anchor-free方法都能消除那些关于anchor定义的超参数（实际工程中预定义这些超参数其实并不容易）并且获得和anchor-based方法相似的精度表现，使得其在泛化能力啊上有更多的潜力，因而受到广泛关注。</p><p>在上述的两种anchor-free检测器中，keypoint-based方法遵循基本的关键点估计的pipeline，这和anchor-based方法是完全不同的。不过，center-based方法和anchor-based方法其实是类似的，只不过是将点（point）而不是anchor作为预设的样本。以单阶段anchor-based检测器RetinaNet和center-based anchor-free方法FCOS为例，它们主要有三个不同：(1)<strong>每个位置平铺的anchor数目不同。</strong> RetinaNet每个位置会平铺几个anchor box而FCOS每个位置只会设定一个anchor point（由于center-based方法可以理解为point就是anchor）。(2)<strong>正负样本的定义方式不同。</strong> RetinaNet根据IOU来选择正负样本，FCOS则利用空间和尺度约束来选择样本。 (3) <strong>回归的起始状态。</strong> RetinaNet从预定义的anchor处回归边界框，而FCOS则从预定义的point上定位目标。</p><p><img src="https://i.loli.net/2021/02/28/cqloZb6CMQUNO2F.png" alt=""></p><p>FCOS获得了比RetinaNet好得多的表现，上述三个因素中哪个起决定作用是值得考虑的。ATSS论文中，作者通过消除anchor-based方法和anchor-free方法的所有不一致因素以一个公平的方式研究了两者的不同之处。实验表明，两种方法关键性的不同在于正负样本的定义方式，这是导致它们性能差距的核心因素。如果它们训练时采用相同的正负样本选择策略，那么无论是基于anchor还是基于point，不会有什么太明显的差距。</p><p>因此，其实如何选择正负样本是值得研究的，论文提出了一种自适应训练样本选择策略（ATSS），它基于目标的特性自动选择正负样本，弥补了anchor-based方法和anchor-free方法之间的差距。此外，论文还通过实验发现，每个位置平铺多个anchor是没有必要的。将ATSS用于现有的检测器，可以在COCO数据集上获得新的SOTA表现。</p><h2 id="不同点分析"><a href="#不同点分析" class="headerlink" title="不同点分析"></a>不同点分析</h2><p>作者以RetinaNet和FCOS作为实验方法，消除它们之间的不对等设置研究影响它们性能差距的究竟是<strong>正负样本选择</strong>还是<strong>回归起始状态</strong>（另一个因素每个位置的anchor数目后面讨论，这里的实验每个位置只采用一个anchor），作者这边首先叙述了实验设置（包括数据集、训练设置、推理设置），然后将FCOS的一些技巧加到RetinaNet中，逐步实验，得到如下表所示的结果，消除这些不一致的配置之后，两个方法差距仍有0.8，现在就能来探索它们之间的关键区别了。（表格中RetinaNet后的#A=1表示每个位置anchor数目为1。）</p><p><img src="https://i.loli.net/2021/02/28/XAKv9hwiqH85cNn.png" alt=""></p><p>在消除了上述不一致之后，其实还剩下两个不同，一个是分类子任务中的定义正负样本的方式，另一个是回归子任务中，回归的起点是anchor box还是anchor point。</p><h3 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h3><p>这部分主要是讲解正负样本选择机制的不同，因为讲述是基于论文中的下图，所以我先对图中进行一些必要性的说明以方便看懂。首先，这里整体上分为上下两个部分，对应两个特征图尺度（源于FPN），上面的是包含较多空间信息的特征图，下面的是包含较多语义信息的特征图；接着，图的左侧是RetinaNet选择正负样本的说明，右侧是FCOS选择正负样本的说明；最后，网格图中的1表示正样本区域，0表示负样本区域，蓝色框、红色框和红色点分别表示GT框、anchor box和anchor point。</p><p><img src="https://i.loli.net/2021/02/28/Gf3sCirDMg62hme.png" alt=""></p><p>那么RetinaNet是如何做的呢，它在每个尺度的特征图上计算anchor box和GT box之间的IOU。首先将每个目标大于IOU阈值$\theta_p$且最优的anchor标记为正样本，然后将小于IOU阈值$\theta_n$的anchor标记为负样本，其他的都是训练时忽略的样本。如上图左侧所示，上一半的特征图尺度较大，anchor主要负责小目标，没有大于阈值的，所以最终输出全是0，下一半的特征图尺度较小，最中间位置的anchor和GT的IOU超过阈值则将其认定为正样本，标记为1。</p><p>接着来看FCOS是如何做的呢，它主要通过空间和尺度约束来划分anchor point。它首先将GT范围内的anchor point作为候选正样本（图上标记为问号），然后根据每个尺度特征图上定义的尺度范围选择正样本，未选择为正样本的就是负样本。如上图右侧上一半所示，大尺度特征图上回归距离超过预定义的范围则定为负样本，均标记为0，而下一半所示的回归距离不在负样本范围内，那么定为正样本，填充为1。</p><p>通过上面的分析，可以发现，其实FCOS是先在空间维度上通过空间约束找到合适的候选正样本，再在尺度维度上通过尺度约束选择最终的正样本。RetinaNet则使用IOU同时在空间和尺度维度上完成正负样本的划分。自然的想法就是互换这两种方法的正负样本选择策略呢？就得到了下表的结果，RetinaNet从37.0涨到了37.8，而FCOS从37.8掉到了36.9，这说明正负样本的定义方式就是两类检测器性能差距的关键原因。</p><p><img src="https://i.loli.net/2021/02/28/zDcPVw2R9qmCYNd.png" alt=""></p><h3 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h3><p>完成了正负样本的选择（也就是分类任务，目标检测中是先分类后回归的）就可以在正样本的基础上进行目标位置的回归了。如上图a所示，蓝色框和蓝色点表示GT框和GT框的中心点，红色框和红色点表示anchor框和anchor中心点。如图b所示，RetinaNet从anchor框回归四个与GT的偏移量得到预测框；如图c所示，FCOS从anchor point回归与GT的四个距离来得到预测框。</p><p><img src="https://i.loli.net/2021/02/28/yvdVJ4kq6OhMbmg.png" alt=""></p><p>这就是说，RetinaNet和FCOS回归的起点分别是box和point，然而之前的实验表格已经表明，即使它俩用的回归其实状态不同，最终在相同正样本的前提下，最终精度是差不多的，这说明回归起始状态并不是一个很重要的影响因素，甚至没什么影响。</p><h2 id="自适应训练样本选择"><a href="#自适应训练样本选择" class="headerlink" title="自适应训练样本选择"></a>自适应训练样本选择</h2><p>上一节的实验表明，如何定义正负样本其实是两种类型检测器性能差距的关键，在目标检测中，我们是先完成正负样本的定义和分类再进行正样本的回归的，所以正负样本的选择策略值得研究。FCOS提出了一种不同于以往IOU方法的新策略，获得了较大的性能提升，因此论文也进行了研究并提出了一种自适应训练样本选择策略（ATSS），相比之前的方法，ATSS是一种几乎无参数的策略并具有强大的鲁棒性。</p><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>之前的样本选择策略都有比较敏感的超参数，比如IOU阈值或者尺度范围。在设置了这些超参数的基础上，所有的GT框必须基于固定的规则选择对应的正样本，这适用于大多数目标但是还是会有部分超出范围的目标被忽略。因此，超参数不同，产生的结果也是大相径庭。</p><p>为此，论文作者提出ATSS算法，它自动依据目标的统计特性划分正负样本而几乎不需要任何超参数。下图的Algorithm描述了ATSS针对一幅输入图像的工作流程。对图像上每个GT框$g$，首先找出候选正样本。如算法图中3-6行的描述，对每一级特征图选择center距离$g$的center最近的$k$个anchor box（基于L2距离）。假定有$\mathcal{L}$级特征图，那么$g$就有$k \times \mathcal{L}$个候选正样本。之后，按照算法图第7行来计算这写候选正样本和GT框$g$之间的IOU，并保存为集合$\mathcal{D}_{g}$。接着，如算法图8，9两行所示，计算$\mathcal{D}_{g}$的均值$m_g$和标准差$v_g$。基于这两个统计值，设置$g$的IOU阈值为$t_g = m_g + v_g$，如第10行所示。最后，选择大于等于IOU阈值$t_g$的候选正样本作为最终的正样本用于后续训练，这部分对应算法图的11-15行，不过在第12行也对候选正样本中心要在$g$内进行了限制。若一个anchor box被多个GT框选择为正样本，那么最高IOU的被最终选择。除了上述算法命中的，其余的都是负样本。</p><p><img src="https://i.loli.net/2021/02/28/ZifYl1y43VMjI6L.png" alt=""></p><p>算法讲完了，论文也分析了上述算法提出的思路来源。首先是为什么用中心点距离来选择候选，这主要考虑到，RetinaNet的IOU阈值中心点靠近时会更高，FCOS也是使用的中心点距离并且效果更好，因此，以框的中心点距离来衡量会有更好的候选产生。</p><p>然后就是为什么采用均值和标准差的和作为IOU阈值，其实IOU的均值可以表明anchor对于GT的适用性。如下图a所示，一个较高的IOU均值表示大部分候选质量都比较高则IOU阈值也应该设置较高，如下图b所示，均值较低则表示候选质量较差则IOU阈值也应该设置较低。而IOU的标准差其实可以表明哪一层特征图更加适合用来检测目标。如下图a所示，标准差较大表示有某个级别的特征图特别适合检测，将标准差加到均值上作为IOU阈值可以只获得来自该级别特征图的anchor，如下图b所示，较低的标准差表示几个级别的特征图都比较合适，那么将标准差加上去得到较低IOU阈值来选择适当的几个级别特征图上的anchor。<strong>因此，使用均值和标准差的和作为IOU阈值可以基于目标的统计特征自适应选择来自适当级别特征图的足够的正样本。</strong></p><p><img src="https://i.loli.net/2021/02/28/qj1oDZ5XkFMP2KL.png" alt=""></p><p>接着就是为什么要对正样本的中心进行限制，这就很直白了，中心不在GT框内的anchor会基于很多不属于目标的特征进行预测，显然是不合适的候选框。</p><p>还有就是如何确保不同目标之间的公平性。根据统计学理论，大约会有$16\%$的样本在置信区间$\left[m_{g}+v_{g}, 1\right]$内。尽管候选框的IOU并不是一个标准的正态分布，但是统计结果表明每个目标有$0.2 * k \mathcal{L}$的正样本，这是不受尺度、长宽比和位置影响的。相比之下，RetinaNet和FCOS的策略对于较大的目标往往有更多的正样本，导致不同对象之间的不公平性。</p><p>最后就是关于超参数的问题了，其实ATSS只有一个超参数$k$\，实验表明，这是个高度不敏感的超参数，因此本方法可以视为无超参数的。</p><h3 id="算法验证"><a href="#算法验证" class="headerlink" title="算法验证"></a>算法验证</h3><p>作者在RetinaNet和FCOS的基础上添加ATSS模块，结果如下表。在RetinaNet上是有了不小的提高的，在FCOS上比较了原来的尺度选择策略和ATSS策略，显然ATSS有更好的效果。</p><p><img src="https://i.loli.net/2021/02/28/qAmbG2CBQouUa1P.png" alt=""></p><h3 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h3><p>目标检测算法训练时，除了ATSS超参数$k$的选择，还有一个相关因素就是anchor的尺寸，作者这里也进行了讨论。</p><p>首先，在COCO上使用不同的$k$值进行实验，发现影响不是特别地大，也就是说ATSS其实是一个几乎无参的方法。</p><p><img src="https://i.loli.net/2021/02/28/Mglf7LVy8DBcOIo.png" alt=""></p><p>下面是关于anchor size的两个实验图，实验结果表明最终的检测结果其实对该变量也不是很敏感，这进一步验证了ATSS的鲁棒性。</p><p><img src="https://i.loli.net/2021/02/28/Ppr7sBklShEjU4V.png" alt=""></p><h3 id="模型对比"><a href="#模型对比" class="headerlink" title="模型对比"></a>模型对比</h3><p>和SOTA方法比一比，涨点明显。</p><p><img src="https://i.loli.net/2021/02/28/nTwd1RcDmLNshjg.png" alt=""></p><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p>之前忽略的问题，一个位置是否需要多个anchor，事实上，实验说明，采用基于IOU的正负样本选择策略时，平铺多个anchor会带来比较大的收益，但是采用ATSS的话得出的结论是，只要正负样本的选择是恰当的，其实每个位置几个anchor并没有太大区别。所以，在每个位置平铺多个anchor在ATSS方法下是无用的。</p><p><img src="https://i.loli.net/2021/02/28/hRAd68QvMxPCpb2.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文指出了单阶段anchor-based方法和center-based的anchor-free方法最核心的性能差别的原因就是正负样本的定义策略，并设计了一种自适应训练样本选择策略（ATSS），从而几乎消除了anchor-based和anchor-free的性能差异。并且，在ATSS下，每个位置平铺多个anchor是没有必要的操作。时至今日，ATSS已经受到目标检测社区的关注，被CVPR2020收录为oral，在竞赛中也是非常有效的涨点神器。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ATSS解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MOTDT解读</title>
      <link href="2021/02/24/motdt/"/>
      <url>2021/02/24/motdt/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近MOT的新文章不是特别多，正好趁此机会来回顾一些比较经典的文章，以弥补那些年为了赶调研进度没能精读而错过的一些方法。本文就是逛知乎的时候看到的<a href="https://zhuanlan.zhihu.com/p/136091505">文章</a>，我就去读了一下这篇2018年的论文，虽然和今天那些动辄MOTA达到60、70的SOTA相比已经是距离挺远的了，但是其中的思路还是对后来研究者有不少启发的。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>MOTDT是清华大学TNList实验室发表的收录于ICME2018的一篇文章，作者基于TBD范式提出从检测结果和跟踪结果中收集候选框从而解决检测不可靠的问题，这种产生冗余候选框的思路来源于检测和跟踪可以在不同场景下信息互补。高质量的检测可以阻止长期跟踪产生的轨迹偏移，轨迹的预测又可以防范由于遮挡造成的噪声检测。为了实时从大量候选框中进行最优选择，我们提出了一种基于全卷积神经网络的新的评分函数，该函数共享了整个图像的大部分计算。此外，外观的表示学习采用一个深度神经网络在三个大规模行人重识别数据集上训练得到。实验表明，由检测、外观特征提取、分层数据关联构成的这个多目标跟踪框架MOTDT达到实时SOTA表现。</p><ul><li><p>论文标题</p><p>  Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-Identification</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/1809.04427">http://arxiv.org/abs/1809.04427</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/longcw/MOTDT">https://github.com/longcw/MOTDT</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>众所周知，多目标跟踪（Multiple Object Tracking，MOT）的任务要求是估计同类多个目标的轨迹，如今我们解决跟踪问题（帧间关联）大多依赖行人重识别技术。不过，在TBD（Tracking-by-Detection）范式的基础下，检测任务过去数年的发展为MOT带来了新的活力，因此现在的跟踪器大多采用的思路是先通过检测器在每一帧上得到检测框，然后根据某个度量标准（如外观相似度）来将帧间的目标关联到一起从而形成轨迹。</p><p><strong>同类目标的遮挡</strong>和<strong>不可靠的检测</strong>是目前MOT的主要挑战。同类目标的遮挡和目标之间相似的外观都会造成帧间数据关联时的歧义混淆，很多工作发现通过同时考虑目标的运动、几何和外观特征可以<strong>缓解</strong>这个问题，这也是目前主流的解决方案。另一方面，检测器得到的检测结果有可能也是不可靠的，密集场景下姿态的变化和人与人之间的遮挡经常造成错误的检测如误检、漏检、边界框不准确等。前人针对这个问题提出了离线方法（也叫batch模式），这种方法可以引入未来帧的信息，具体是通过一个固定大小的滑窗或者整个视频上进行轨迹的关联，将其转换为batch内的全局优化问题。不过，很遗憾，这种策略是没法在实时性要求的场景下实现的，但这种实时视频流跟踪往往又是MOT最主流的攻坚领域。</p><p><img src="https://i.loli.net/2021/02/24/XOfhqru92aSALTw.png" alt=""></p><p>论文在online模式下设计了一种解决不可靠检测的框架，该框架如上图所示，它从每帧的检测框和跟踪框输出中选择最优的候选框。<strong>现有的TBD方法在数据关联时，与现有轨迹关联的候选框仅有检测结果构成，但有前人发现应该将跟踪器和检测器看作两个互相独立的部件，并将其结果均作为候选框，这也是本文的出发点。</strong> 因为这两个任务是可以互补的。一方面，跟踪器可靠预测出来的跟踪框可以用来短时进行数据关联以防漏检和不准确的边界框检测；另一方面，高置信度的检测框对于放置长期跟踪时轨迹偏移向背景也是至关重要的。 <em>不过，我这里个人觉得其实检测框和跟踪框相比，后者是天然弱势的，因为跟踪框本质上还是源于检测框，比较是tracking-by-detection。</em></p><p>不过前人选择候选框的方式是基于手工特征的，如颜色直方图、光流和运动特征。所谓选择候选框，其实就是为冗余的候选框打分，如何设计一个统一的方式来为检测框和跟踪框打分还是一个需要研究的问题。考虑到CNN的发展，作者引入深度神经网络来解决不可靠检测和同类目标遮挡问题。首先，引入深度神经网络来完成候选框的打分；第二，提出分层数据关联策略，充分利用空间信息和外观信息（ReID特征）；最后，实验表明，作者提出的MOT框架MOTDT可以实现实时跟踪下的SOTA结果。</p><h2 id="MOTDT"><a href="#MOTDT" class="headerlink" title="MOTDT"></a>MOTDT</h2><p>在这篇论文中，作者通过从检测框和跟踪框的输出中生成候选框的方式拓展了TBD跟踪框架，该框架包含两个序列任务：<strong>候选框选择</strong>和<strong>数据关联</strong>。</p><p>首先，评估所有的候选框采用一个统一的<strong>评分函数</strong>，这个函数由一个判别训练的<strong>目标分类器</strong>和一个精心设计的<strong>轨迹段置信度计算模块</strong>构成。随后，NMS用来基于候选框的评分进行冗余候选框消除。最后，在消除冗余的候选框的基础上，使用<strong>外观表示</strong>和空间信息来分层关联候选框和已有轨迹。</p><h3 id="目标分类器"><a href="#目标分类器" class="headerlink" title="目标分类器"></a>目标分类器</h3><p>组合预测框和检测框构建的候选框数目不少，所以作者这里采用R-FCN（基于区域的全卷积网络）来构建目标分类器，该结构在整个图像上共享大多数计算。因此它比那些在图像块上进行分类的结构高效得多。</p><p>这个网络结构如下图所示，给定一帧图像，得分图（图上的score maps）通过全卷积得到。每个候选框都可以定义为一个感兴趣区域（RoI）$\mathbf{x}=\left(x_{0}, y_{0}, w, h\right)$，其中$(x_0, y_0)$表示左上角点坐标，$w$和$h$表示区域的宽高。为了计算的高效，我们希望每个RoI的分类概率可以通过共享的score maps投票得到。有个直接的策略就是从图像上所有像素点的概率值上跳出RoI范围内的，然后求平均就得到了RoI的前景概率。然而这个简单的策略其实放弃了空间信息，例如，即使RoI只覆盖对象的一部分，仍然可以获得较高的置信度。</p><p><img src="https://i.loli.net/2021/02/24/W3Zfqg4iYha1nLc.png" alt=""></p><p>为了显式地将空间信息编码进score maps中，作者采用了位置敏感的RoI pooling层处理score maps并从最终得到的$^2$个位置敏感的score maps $\mathbf{z}$中估计目标的分类概率。具体来说，将一个RoI区以网格的方式划分为$k \times k$个bins，每个bin的尺寸为$(\frac{w}{k}, \frac{h}{k})$，表示目标的一个特定位置。然后，从$k^2$的score maps上提取$k \times k$个bins的结果，每个score map只对应一个bin。最终一个RoI区域$\mathbf{x}$的最终分类置信度计算式如下，其中$\sigma(x)=\frac{1}{1+e^{-x}}$表示sigmoid函数，而$z_i$表示第$i$个score map。</p><script type="math/tex; mode=display">p(y \mid \mathbf{z}, \mathbf{x})=\sigma\left(\frac{1}{w h} \sum_{i=1}^{k^{2}} \sum_{(x, y) \in b i n_{i}} \mathbf{z}_{i}(x, y)\right)</script><p>在训练过程中，我们在GT框周围随机抽取RoI作为正样本，并从背景中抽取相同数量的RoI作为负样本。通过对网络进行端到端的训练，解码器部分的顶部输出，即$^k2$的score maps，会学会对物体的特定空间位置做出反应。举例而言，如果$k=3$那么就会有$9$个score maps对应左上角到右下角9个位置。按照这个策略，RoI pooling层对位置敏感且具有判别性目标分类能力，而不需要可学习参数。上图这个网络就是用于候选框分类的，无法进行边框回归。</p><h3 id="轨迹段置信度计算模块和评分函数"><a href="#轨迹段置信度计算模块和评分函数" class="headerlink" title="轨迹段置信度计算模块和评分函数"></a>轨迹段置信度计算模块和评分函数</h3><p>对于当前帧，用卡尔曼滤波来估计现有轨迹在该帧上的位置，可以一定程度上解决遮挡等造成的错误检测，但是卡尔曼滤波并不适用于长期跟踪，因为长时间不使用检测来更新卡尔曼滤波器的话，它的准确率会大大下降。因此，设计了轨迹段置信度计算模块来评估使用时序信息的卡尔曼滤波器的准确率。</p><p>一个轨迹段是连续帧上候选框关联形成的，可以将一个目标的完整轨迹拆分为多个轨迹段，因为一个轨迹可以在其生命周期内多次中断和恢复。每次一个轨迹从丢失状态恢复的时候，卡尔曼滤波器会被重新初始化。因此，仅仅需要使用最后的轨迹段的信息来进行轨迹段置信度计算。这里不妨定义$L_{det}$为关联到这个轨迹段上的检测框数目，$L_{trk}$为在这个轨迹段上，最后一个detection被关联后来自轨迹预测框的数目，继而定义下面的轨迹段置信度计算公式。</p><script type="math/tex; mode=display">s_{t r k}=\max \left(1-\log \left(1+\alpha \cdot L_{t r k}\right), 0\right) \cdot \mathbb{1}\left(L_{d e t} \geq 2\right)</script><p>上式中，$\mathbb{1}(\cdot)$是指示函数，输入为真得1，否则得0。要求$L_{det} \geq 2$来在这个轨迹作为候选框之前构建合理的使用观测到的检测框运动模型。</p><p>融合上面的目标分类器得到的分类置信度和轨迹段置信度计算模块得到的置信度，构建统一的打分函数（scoring function）如下，其中$C_{det}$表示来自检测框的候选框，$C_{trk}$表示来自跟踪框的候选框，$s_{trk}$取值在$(0,1)$之间来惩罚来自不确定轨迹的候选框。</p><p>$\left.s=p(y \mid \mathbf{z}, \mathbf{x}) \cdot\left(\mathbb{1}\left(\mathbf{x} \in C_{\text {det }}\right)\right)+s_{\text {trk }} \mathbb{1}\left(\mathbf{x} \in C_{\text {trk }}\right)\right)$</p><p>通过上面的打分函数得到的分数送入NMS算法进行候选框选择，定义$\mathcal{T}_{n m s}$为IOU阈值，定义$\mathcal{T}_{s}$为置信度阈值。</p><h3 id="外观表示"><a href="#外观表示" class="headerlink" title="外观表示"></a>外观表示</h3><p>候选框间的相似度计算是数据关联的关键，实验表明以数据驱动的深度学习外观表示模型超过了手工设计的方法，所以作者这里采用行人重识别领域的一个网络Deeply-learned part-aligned representations for person reidentification用于外观特征的学习，在三个常用的ReID数据集上进行训练，具体细节可以参考原文，这里记这个网络为$H_{reid}$。给定一个行人的RGB图像$\mathbf{I}$，其外观特征向量为$f=H_{\text {reid }}(\mathbf{I})$，文中使用特征向量之间的欧氏距离来评估两个行人图像$\mathbf{I}_{i}$和$\mathbf{I}_{j}$之间的相似度。训练时，网络采用三元组损失进行训练，这属于ReID的内容，这里不更细展开了。</p><h3 id="分层数据关联"><a href="#分层数据关联" class="headerlink" title="分层数据关联"></a>分层数据关联</h3><p>人群密集时轨迹预测可以一定程度上缓解漏检的问题，但是受到同类目标遮挡的影响，这些预测框也可能包含其他目标。为了避免将其他不需要的对象和背景带入外观表示，我们使用不同的特征分层地将轨迹与不同的候选框相关联。</p><p>具体而言，首先对来自检测的候选框进行基于外观表示的数据关联，最大相似度阈值定义为$\mathcal{T}_{d}$。接着，对剩下的候选框和未被关联的轨迹基于IOU进行关联，阈值为$\mathcal{T}_{iou}$。只有当轨迹关联上检测框的时候才会更新外观表示。最后，在剩下的检测框的基础上初始化新的轨迹。整个算法如下图所示。</p><p><img src="https://i.loli.net/2021/02/24/X42HiOYdyzKtD1U.png" alt=""></p><p>利用分层数据关联，我们只需要从每帧检测中提取一次候选框的ReID特征。结合之前的高效评分函数和轨迹段置信度计算，MOTDT框架可以以实时速度运行。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者首先进行消融实验对比了各个组件的效果，结果如下表，其中C表示目标分类器，T表示轨迹段置信度计算，A表示外观特征，实验表明每个部分都很重要。</p><p><img src="https://i.loli.net/2021/02/24/AJwBKRI6il4yNGs.png" alt=""></p><p>然后对比了一下ReID模型和手工特征效果，虽然深度模型是好一些，但没有我想象中好那么多，可能那时候ReID还没有特别成熟吧。</p><p><img src="https://i.loli.net/2021/02/24/obLsjTtVcZxeGX4.png" alt=""></p><p>最后在MOT16上和当前SOTA进行对比，精度速度都很快。</p><p><img src="https://i.loli.net/2021/02/24/wjK6lsNktzMLIDZ.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文中作者提出了一种新的TBD多目标跟踪框架MOTDT，充分利用了深度神经网络的强大，通过从检测框和跟踪框中共同生成候选框来解决检测不可靠的问题，为此设计了多个高效的模块，在主流的benchmark上达到当时的SOTA。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MOTDT解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NFNet解读</title>
      <link href="2021/02/19/nfnet/"/>
      <url>2021/02/19/nfnet/</url>
      
        <content type="html"><![CDATA[<blockquote><p>针对BN做的一个工作，这段时间这样的工作还是不少的，当一个领域的技术发展得比较成熟时，我们往往就会考虑一些我们习以为常的东西的优化和改进，如之前的RepVGG、又如现在的NFNet。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>最近有不少文章介绍了NFNet，但是没怎么看到针对论文较为详细的解读，所以这边就结合论文谈谈个人的见解。NFNet（Normalizer-Free ResNets）是DeepMind提出了一种不需要Batch Normalization的基于ResNet的网络结构，其核心为一种AGC（adaptive gradient clipping technique，自适应梯度裁剪）技术。如下图所示，最小的NFNet版本达到了EfficientNet-B7的准确率，并且训练速度快了8.7倍，最大版本的模型实现了新的SOTA效果。</p><ul><li><p>论文标题</p><p>  High-Performance Large-Scale Image Recognition Without Normalization</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2102.06171">http://arxiv.org/abs/2102.06171</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/deepmind/deepmind-research/tree/master/nfnets">https://github.com/deepmind/deepmind-research/tree/master/nfnets</a></p></li></ul><p><img src="https://i.loli.net/2021/02/19/y2I9cK8GQAlsopH.png" alt=""></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>目前计算机视觉中很多网络网络都是基于ResNet的变种，使用Batch Normalization（下文简称BN）进行训练。BN和残差结构的组合已经被业界证明十分有效，可以很容易地训练深层网络。BN的存在可以平滑loss使得更大学习率更大batch size的训练稳定进行，BN也有一定的正则化效果。不可否认，BN是非常有效的，但是它存在问题，论文中总结了三个非常典型的缺点如下。</p><ul><li><strong>BN是带来了额外的不小的计算开销。</strong> 计算均值等需要在内存中保存临时变量，导致了内存开销的增大，并且，在某些网络中增加了梯度评估的时间。</li><li><strong>BN造成了模型训练和推理时的行为差异。</strong> 在Pytorch中实现上时<code>model.train()</code>和<code>model.eval()</code>的差异，也就是说BN带来了需要调整的隐藏超参数。</li><li><strong>BN打破了小批量样本之间的独立性。</strong> 这就是说，其实选择哪些样本其实挺重要的。</li></ul><p>这三个特性导致了一系列不好的后果。一方面，具体而言，研究者已经发现，使用BN的网络通常难以在不同的硬件设备之间精确复制，BN往往就是这种细微错误的原因，特别是分布式训练时。分布式训练时，如果数据并行，在不同的机器上都有BN层，那么就需要将信号发送到BN层，然后在BN层之间传递均值等统计信息，不这样的话这个批次就没有均值和方差，这使得网络可以欺骗某些损失函数，造成信息泄露等问题。另一方面，BN层对batch size是非常敏感的，bs过小BN网络就会效果很差，这是因为bs很小的时候样本很少，均值其实是噪声的近似。</p><p><img src="https://i.loli.net/2021/02/19/MntigJhaZXAd9VB.png" alt=""></p><p>因此，尽管BN推动了深度学习的发展，但是从长远来看，它其实阻碍了深度网络的进步。其实如上图所示，业内已经出现了Layer Norm、Group Norm等BN的替代品，它们获得了更好的精度表现但也带来了不小的计算量。幸运的是，这些年也出现了一些具有代表性的无BN的网络，这些工作的核心思路就是通过抑制残差分支上隐藏激活层的尺度从而训练非常深的无BN的ResNet网络。最简单的实现方法就是在每个残差分支的末尾引入一个初始值为0的可学习标量，不过这种技巧的精度表现并不好。另一些研究表明，ReLU这个激活函数会带来均值偏移现象，这导致不同样本的隐藏激活值随着网络的深度增加越来越相关。此前已经有工作提出了Normalize-Free ResNets，它在初始化的时候抑制残差分支并且使用Scaled Weight Standardization来消除均值偏移现象，通过额外的正则化，这种网络在ImageNet上获得了和有BN网络相媲美的效果，但是它在大的bs时训练并不稳定并且其实距离目前的SOTA也就是EfficientNet还有一定距离。因此，这篇论文提出了解决它劣势的新方法，称为NFNet，论文的主要贡献如下。</p><ul><li>提出Adaptive Gradient Clipping (AGC)模块，该方法基于逐单元梯度范数与参数范数的单位比例来裁剪梯度，实验表明，AGC允许NFNet以大的bs和强数据增强条件进行训练。</li><li>设计了一系列的Normalize-Free ResNets，称为NFNets，最简单版本的NFNet达到了EfficientNet-B7的精度，但训练速度是8.7倍。最优版本的NFNet在不适用额外数据的情况下实现了新的SOTA。</li><li>在3亿张带有标签的大型私有数据集进行预训练后，对ImageNet进行微调时，NFNet与批归一化网络相比，其验证准确率要高得多。最佳模型经过微调后可达到89.2％的top-1 accuracy。</li></ul><p>这篇文章后面两节主要是叙述BN的效果，以及前人如何在去除了BN之后保留这些优势所做的工作，这里感兴趣的可以查看文章的第二节和第三节，我这里就直接来将这篇论文的方法论了。</p><h2 id="AGC（自适应梯度裁剪模块）"><a href="#AGC（自适应梯度裁剪模块）" class="headerlink" title="AGC（自适应梯度裁剪模块）"></a>AGC（自适应梯度裁剪模块）</h2><p>梯度裁剪技术常用于语言模型来稳定训练，最近的研究表明，与梯度下降相比，它允许以更大的学习率进行训练从而加速收敛。这对于条件较差的损失或大批量训练尤为重要，因为在这些设置中，最佳学习率往往会受到最大学习率的限制。因此作者假定梯度裁剪有利于NFNet的大批尺寸训练。梯度裁剪往往是对梯度的范数进行约束来实现的，对梯度向量$G=\partial L / \partial \theta$而言，$L$表示损失值，$\theta$则表示模型所有参数向量，标准的裁剪算法会在更新$\theta$之前以如下的公式裁剪梯度。</p><script type="math/tex; mode=display">G \rightarrow\left\{\begin{array}{ll}\lambda \frac{G}{\|G\|} & \text { if }\|G\|>\lambda \\G & \text { otherwise }\end{array}\right.</script><p>上式的$\lambda$是必须调整的超参数，根据经验，作者发现虽然这个裁剪算法能够以比以前更高的批尺寸进行训练，但训练稳定性对裁剪阈值$\lambda$的选择极为敏感，在改变模型深度、批尺寸或学习率时都需要精调阈值。</p><p>为了解决这个问题，作者引入了自适应梯度裁剪算法（AGC），下面详细叙述这个算法。记$W^{\ell} \in \mathbb{R}^{N \times M}$为第$\ell$层的权重矩阵，$G^{\ell} \in \mathbb{R}^{N \times M}$为对应于$W^{\ell}$的梯度矩阵，$|\cdot|_{F}$表示$F$范数，即有$\left|W^{\ell}\right|_{F}=\sqrt{\sum_{i}^{N} \sum_{j}^{M}\left(W_{i, j}^{\ell}\right)^{2}}$。AGC算法的动机源于观察到梯度与权重的范数比$\frac{\left|G^{\ell}\right|_{F}}{\left|W^{\ell}\right|_{F}}$，这其实是一个单次梯度下降对原始权重影响的简单度量。举个例子，如果使用无动量的梯度下降算法，有$\frac{\left|\Delta W^{\ell}\right|}{\left|W^{\ell}\right|}=h \frac{\left|G^{\bar{\ell}}\right|_{F}}{\left|W^{\ell}\right|_{F}}$，那么第$\ell$层的参数更新公式为$\Delta W^{\ell}=-h G^{\ell}$，其中$h$表示学习率。直观上，我们认为如果$\left|\Delta W^{\ell}\right| /\left|W^{\ell}\right|$很大那么训练就会变得不稳定，这就启发了一种基于$\frac{\left|G^{\ell}\right|_{F}}{\left|W^{\ell}\right|_{F}}$的梯度裁剪策略，然而实际上，逐单元的梯度范数和参数范数比会比逐层的效果好，因此定义第$\ell$层上第$i$个单元的梯度矩阵$G_{i}^{\ell}$（表示$G_{\ell}$的第$i$行）的裁剪公式如下，其中$\lambda$是一个标量超参数，定义$\left|W_{i}\right|_{F}^{\star}=\max \left(\left|W_{i}\right|_{F}, \epsilon\right)$（其中默认$\epsilon=10^{-3}$），这避免0初始化参数总是裁剪为0。对于卷积滤波器中的参数，我们在扇入范围(包括通道和空间维度)上评估逐单元范数。</p><script type="math/tex; mode=display">G_{i}^{\ell} \rightarrow\left\{\begin{array}{ll}\lambda \frac{\left\|W_{i}^{\ell}\right\|_{F}^{\star}}{\left\|G_{i}^{\ell}\right\|_{F}} G_{i}^{\ell} & \text { if } \frac{\left\|G_{i}^{\ell}\right\|_{F}}{\left\|W_{i}^{\ell}\right\|_{F}^{\star}}>\lambda \\G_{i}^{\ell} & \text { otherwise. }\end{array}\right.</script><p>使用上述的AGC模块，NFNet能够以高达4096的批尺寸训练同时使用RandAugment这样的数据增强策略，不适用AGC模块，NFNet是无法这样训练的。注意，最优裁剪参数$\lambda$可能取决于优化器的选择，学习率和批大尺寸。经验上，batch size越大，$\lambda$应该越小。</p><p><img src="https://i.loli.net/2021/02/19/NfEXCLZdBQg2lOt.png" alt=""></p><p>上图是论文针对AGC做的两个消融实验，左图a表示使用BN的REsNet以及使用和不使用AGC的NFNet之间的对比，实验表明AGC使得NFNet有着媲美BN网络的效果，而且批尺寸越小，AGC收益越低。右图b则表示不同批尺寸不同$\lambda$选择的效果，结果表明，当批尺寸较大的时候，应该选择较小的$\lambda$以稳定训练。</p><p>后续作者也对AGC的作用层进行了消融实验，得到一些结论，比如对最终的线性层裁剪是不需要的等。</p><h2 id="NFNet"><a href="#NFNet" class="headerlink" title="NFNet"></a>NFNet</h2><p><img src="https://i.loli.net/2021/02/19/YJZMVeWQhfpnHok.png" alt=""></p><p>上一节提到了可以让网络以较大批尺寸和较强数据增强方法进行训练的梯度裁剪手段，同时为了配置这个AGC模块，论文也对模型结构进行了探索。EfficientNet是当前的分类SOTA网络，它基于NAS技术搜索而得，拥有很低的理论计算复杂度，但是实际硬件上的表现并不是很好，所以作者这边选择了手工探索模型空间以获得较好的表现，在SE-ResNeXt-D模型的基础上，对其先是应用了前人的Normalizer-Free配置，修改了宽度和深度模式，以及第二个空间卷积（下图表示这种配置上的修改，更具体的可以查看论文附加材料，给出的图如下）。接着，应用AGC到除了最后的线性层上的每一层，得到最终的NfNet配置。</p><p><img src="https://i.loli.net/2021/02/19/URQyekCTVOMmtPh.png" alt=""></p><p>得到的NfNets如下表所示，构成一个网络系列。</p><p><img src="https://i.loli.net/2021/02/19/FhokB3OlVtxATgK.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>下表是NFNet使用各种数据增强在ImageNet上和其他方法的对比，当之无愧的SOTA。</p><p><img src="https://i.loli.net/2021/02/19/xBVNslkjWRPrILA.png" alt=""></p><p>也进行了预训练的实验，证明了Normalize-Free ResNets在迁移学习上效果也是能够很强的。</p><p><img src="https://i.loli.net/2021/02/19/7zX9Dch8Fkxq1oK.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>关于BN这个结构我之前其实也有一些思考，虽然不否认它为深度学习做出的巨大贡献（我自己进行实验过程中深感BN结构的有效性），但是它也确实存在一些问题，DeepMind的这篇文章在保留BN优势克服BN劣势的基础上，实现了一个非常成功的Normalize-Free ResNets，是很值得关注的工作。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NFNet解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DEFT解读</title>
      <link href="2021/02/10/deft/"/>
      <url>2021/02/10/deft/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>最近不少2D多目标跟踪（Multiple Object Tracking，MOT）的成果表明，使用SOTA检测器加上简单的基于空间运动的帧间关联就可以获得相当不错的跟踪表现，它们的效果优于一些使用外观特征进行重识别丢失轨迹的方法。Uber等最近提出的DEFT（Detection Embeddings for Tracking）是一种联合检测和跟踪的模型，它是一个以检测器为底层在上层构建基于外观的匹配网络的框架设计。在二维跟踪上，它能达到SOTA效果并具有更强的鲁棒性，在三维跟踪上它达到了目前SOTA方法的两倍性能。</p><ul><li><p>论文标题</p><p>  DEFT: Detection Embeddings for Tracking</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2102.02267">http://arxiv.org/abs/2102.02267</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/MedChaabane/DEFT">https://github.com/MedChaabane/DEFT</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>卷积神经网络的发展推动了目标检测领域的进步，TBD范式（Tracking by Detection）的MOT也取得巨大的突破，最近的研究表明，在SOTA跟踪器上添加简单的跟踪机制就可以比依赖旧检测架构的复杂跟踪器效果更好。TBD范式的跟踪框架通常有两步：<strong>检测</strong>：在当前帧检测出所有目标；<strong>关联</strong>：将当前帧上的目标和之前帧上的目标进行链接。跨帧关联的方法有很多，但是那些特征可学习关联方法通常更有趣一些，因为它们有望解决建模和启发式方法失败的情况。</p><p>即使有了可学习关联，二阶段方法也可能产生准确性和效率方面的次优结果。因此最近的一个趋势就是在一个网络中联合学习检测和跟踪任务，这会带来性能上的提升。可以假设，一个可学习的目标匹配模块可以添加到主流的CNN检测器中，从而产生高性能的多目标跟踪器，进而通过联合训练检测和跟踪（关联）模块，两个模块彼此适应实现更好的性能。相比于那种将检测作为黑盒模型输入到关联模块中，这种让目标检测和帧间关联共享backbone的思路会有更好的速度和精度。</p><p>所以这篇论文的作者提出了DEFT这个新的方法，在该方法中每个目标的embedding（我这里翻译为嵌入）通过多尺度检测backbone获得，并且这个embedding作为后续的object-to-track关联子网络的外观特征。DEFT可以灵活用于多个常见的目标检测backbone上，并且由于检测和跟踪共享特征的特性，这种使用外观和运动信息的方法在速度上相比于那些使用更简单的关联策略的方法在速度上页不遑多让。</p><h2 id="DEFT"><a href="#DEFT" class="headerlink" title="DEFT"></a>DEFT</h2><p>下面来看整个DEFT的网络设计，总体来说，它是非常类似JDE和FairMOT的一个工作。基于TBD范式，DEFT提出应该使用目标检测器（本文将目标检测器作为backbone）的中间特征图来提取目标的embedding从而用于目标匹配子网络。如下图所示，单看上半部分其实就是整个网络的结构，图像进入上面的Detector分支，同时检测器的不同stage的特征图用于下面的Embedding Extractor模块的外观特征学习，不同帧间的检测目标的外观特征送入Matching Head中获得关联相似度得分矩阵。</p><p>在DEFT中检测器和目标匹配网络是联合训练的，训练时，目标匹配网络的损失会反传给检测backbone从而优化外观特征的提取和检测任务的表现。此外，DEFT还在使用了一个低维的LSTM模块来为目标匹配网络提供几何约束，以避免基于外观的但在空间变化上不可能发生的帧间关联。虽说可以基于多个检测器，DEFT在CenterNet上做了主要的工作，获得了SOTA并且比其他类似方法要快，这种速度上的快时来自于在DEFT中，目标关联只是额外的一个小模块，相比于整个检测任务只会有很小的延时。</p><p><img src="https://i.loli.net/2021/02/10/5XGa7qQdn9f34HI.png" alt=""></p><p>在介绍各个模块之前，我先讲述一下整个DEFT推理时的pipeline，这和上面的训练其实有所不同。如下图所示，Embedding Extractor使用检测backbone多个阶段的特征图和检测框作为输入来获得获得每个目标的外观嵌入。Matching Head使用这些嵌入来计算当前帧目标和历史帧目标（也就是当前轨迹）来计算相似度。之后，一个使用LSTM的运动预测模块会对相似度矩阵进行限制，从而保证那些物理上不可能产生的轨迹的链接。最后，匈牙利算法会基于相似度矩阵计算最终的在线匹配结果，将检测结果链接到轨迹上。</p><p><img src="https://i.loli.net/2021/02/10/PHdiypFRKQO6qkh.png" alt=""></p><p>下面我们按照论文第三章的叙述思路来详细理解这个网络，下面的叙述还是基于下图，不过我会忽略一些细节上的数学表示。</p><p><img src="https://i.loli.net/2021/02/10/5XGa7qQdn9f34HI.png" alt=""></p><h3 id="Object-Embeddings"><a href="#Object-Embeddings" class="headerlink" title="Object Embeddings"></a>Object Embeddings</h3><p>在图中对应的为Embedding Extractor，该模块从检测backbone的中间特征图提取具有表示意义的embedding，在跟踪的过程中帮助关联（或者叫re-identify）。从图中可以看到，这个模块的输入是检测器的多个stage的特征图（每个特征图有不同的尺度，即有不同的感受野），这种策略会提高单感受野策略的鲁棒性。DEFT网络的输入是视频中的一帧图像，检测head会输出包含多个目标的检测结果的bbox集$\mathrm{B}_{t}=\left{b_{1}^{t}, b_{2}^{t}, \ldots, b_{N_{t}}^{t}\right}$，不妨用$N_{t}=\left|\mathrm{B}_{t}\right|$来表示这一帧检测框的数目。</p><p>对每个检测到的目标，依据其2D检测框中心点来提取object embedding，对于3D检测框，则将其中心点投影到二维空间。那么如何将这个中心点在各stage的特征图上找到特征呢，其实这里就是做了一个简单的比例映射。对于在size为$W\times H$的图像上第$i$个目标的中心点坐标为$(x, y)$，对当前图像又存在$M$个选择的stage的特征图，该目标在第$m$个size为$W_{m} \times H_{m} \times C_{m}$的特征图上映射的中心点坐标为$\left(\frac{y}{H} H_{m}, \frac{x}{W} W_{m}\right)$，在这个第$m$个特征图上包含一个$C_m$维度的特征向量，这就是这个stage特征图上目标的object embedding，记为$f_i^m$，然后将$M$个stage通过这种方式得到的特征图级联（concatenate）到一起，得到该目标的object embedding，记为$f_{i}=f_{i}^{1} \cdot f_{i}^{2} \ldots f_{i}^{M}$，它是一个$e$维向量。<strong>不过，由于不同stage特征图上通道数是不同的，一般是浅层channel少，高层channel多，所以对第$m$个特征图处理之前，作者使用1x1卷积对浅层特征图进行升维对高层特征图进行降维，使得不同stage含有的特征量在最终的object embedding中分布合理。</strong></p><h3 id="Matching-Head"><a href="#Matching-Head" class="headerlink" title="Matching Head"></a>Matching Head</h3><p>关于最后这个Matching Head，作者思路采用的是DAN（Deep Affinity Network，是端到端数据关联的一个著名成果）的思路，使用object embedding作为输入来估计两帧之间两两相似度得分。首先，对每一帧设定最大目标个数的限制$N_{max}$，那么就可以构建张量$E_{t, t-n} \in \mathbb{R}^{N_{\max } \times N_{\max } \times 2 e}$，它通过将$t$帧中的每个object embedding和$t-n$帧的每个object embedding沿着深度维度级联。为了保证$E_{t, t-n}$维度固定，会进行补零操作。这个汇总得到的$E_{t, t-n}$会被输入Matching Head，这个匹配模块由几层1x1卷积构成，该Head的输出就是相似度矩阵$A_{t, t-n} \in \mathbb{R}^{N_{\max } \times N_{\max }}$。</p><p>尽管我们学习到了嵌入之间的相似性，然而不能保证沿着帧前向和后向的相似度得分是对称的，即$t$和$t-n$之间的相似度与$t-n$和$t$之间的相似度是不同的。因此对两个方向分别计算相似度矩阵，用下标’fwd’和’bwd’表示前向和后向。同时为了考虑有些目标不参与关联（新目标或者离开场景的目标），在相似度矩阵$A_{t,t-n}$添加一列填充常数值$c$，对$A_{t,t-n}$每一行应用softmax可以得到矩阵$\hat{A}^{b w d}$，它表示包括非匹配分数在内的最终相似度。$c$的选择并不是特别敏感的，网络会学会为大于$c$的进行真实匹配。</p><p>每个$\hat{A}^{b w d}[i, j]$表示两个目标框$b_i^t$和$b_j^{t-n}$的估计关联概率，$\hat{A}^{b w d}\left[i, N_{\max }+1\right]$则表示$b_i^t$是一个不在$t-n$帧中出现的目标的概率。类似的，前向相似度矩阵$\hat{A}^{f w d}$通过对原始相似度矩阵的转置$A_{t, t-n}^{T}$添加列并逐行softmax得到。在推理过程中，目标框$b_i^t$和$b_j^{t-n}$之间的相似度为$\hat{A}^{b w d}[i, j]$和$\hat{A}^{f w d}[j, i]$的平均值。</p><h3 id="Online-Data-Association"><a href="#Online-Data-Association" class="headerlink" title="Online Data Association"></a>Online Data Association</h3><p>在DEFT中，保留了最近$\delta$帧中存在的轨迹的每个目标的object embedding，这些组成了memory。一个新bbox和已有轨迹的关联需要计算这个bbox对象和memory中所有轨迹目标的相似度。为了应对遮挡和漏检，track memory会维持几秒，以便新检测和之前的目标高度相关时，进行轨迹恢复。如果一个轨迹$N_{age}$帧都没有匹配上，那么它会被删除。</p><p>对于轨迹$T$的定义是一个相关联的检测框集合，来自于$t-n$帧到$t-1$帧，注意，不是每一帧该轨迹都有检测框，可能某一帧没有对应的检测结果。轨迹的长度显然为$T$，它表示检测框数目或者object embedding数目。那么，可以定义当前的第$t$帧上的第$i$个检测框$b_i^t$与第$j$个轨迹$T_j$之间的距离为下面的式子。</p><script type="math/tex; mode=display">d\left(b_{i}^{t}, \mathrm{~T}_{j}\right)=\frac{1}{\left|\mathrm{~T}_{j}\right|} \sum_{b_{k}^{t-n} \in \mathrm{T}_{j}} \frac{\hat{A}_{t, t-n}^{f w d}[k, i]+\hat{A}_{t, t-n}^{b w d}[i, k]}{2}</script><p>检测框和轨迹T_j的匹配满足互斥对应原则，是一个二分图匹配问题，记$K = {T_j}$为当前轨迹集，构建检测集到轨迹集的相似度矩阵$D \in \mathbb{R}^{|K| \times\left(N_{t}+|K|\right)}$，它来自于所有检测和轨迹的size为$|K| \times N_t$逐对距离矩阵追加一个size为$K \times K$的矩阵$X$，这个$X$表示当一个轨迹不和任何当前帧的检测关联的情况。$X$对角线上的元素为轨迹中所有检测的平均不匹配得分，非对角线元素设置为$-\infty$，具体而言，$D$按照下式构建。</p><script type="math/tex; mode=display">\begin{aligned}D &=[S \mid X] \\S[j, i] &=d\left(b_{i}^{t}, \mathrm{~T}_{j}\right) \\X[j, k] &=\left\{\begin{array}{ll}\frac{1}{\left|\mathrm{~T}_{j}\right|} \sum_{b_{k}^{t-n} \in \mathrm{T}_{j}} \hat{A}_{t, t-n}^{f w d}\left[k, N_{\max }+1\right] & j=k \\-\infty, & j \neq k\end{array}\right.\end{aligned}</script><p>接着，只要将$D$送入匈牙利算法中进行求解即可。只有亲和度大于阈值$\gamma_1$会被关联，为被匹配的检测会作为新的轨迹，连续$N_age$帧没被匹配到的轨迹会被认为离开场景，从而删除。</p><h3 id="Motion-Forecasting"><a href="#Motion-Forecasting" class="headerlink" title="Motion Forecasting"></a>Motion Forecasting</h3><p>如果仅仅使用学习到的外观嵌入进行帧间匹配，那么很可能出现两个目标其实真的外观空间上很相似，从而造成匹配上的问题。常见的手段是添加一个几何或者时间约束来限制匹配，常用的是卡尔曼滤波或者LSTM。论文采用的是LSTM设计运动预测模块，该模块会依据过去$\Delta T_{\text {past}}$帧预测未来$\Delta T_{\text {pred }}$帧轨迹所在的位置。这个运动预测模块用来约束那些物理上不可能存在的关联，它将距离轨迹预测位置太远的检测框的相似度距离置为$-\infty$。这个模块的具体设计细节可以查看原论文的补充材料。</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><img src="https://i.loli.net/2021/02/10/5XGa7qQdn9f34HI.png" alt=""></p><p>如上图所示，为了训练网络的匹配模块，训练时将间隔$n$帧的一个两帧组成的帧对（pair）输入网络。图像对被$1≤n≤n_{gap}$的随机数帧分开，以鼓励网络学习对临时遮挡或漏检具有鲁棒性。对每个输入帧对，会有两个ground truth的匹配矩阵$M^{fwd}$和$M^{bwd}$分别表示前向和后向关联，这个矩阵每个元素$[i, j] \in{0,1}$并且维度为$N_{\max } \times\left(N_{\max }+1\right)$来允许未被关联的目标。矩阵中为$1$的位置表示一个关联或者未被关联的目标，其他情况为$0$。</p><p>匹配损失由前向匹配损失$\mathcal{L}_{\text {matcl }}^{b w d}$和后向匹配损失$\mathcal{L}_{\text {matcl }}^{b w d}$平均组成，前者表示$t$帧和$t-n$帧的匹配误差，后者相反。具体定义如下。</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L}_{\text {match }}^{*} &=\sum_{i=1}^{N_{\max }} \sum_{j=1}^{N_{\max }+1} M^{*}[i, j] \log \left(\hat{A}^{*}[i, j]\right), \\ \mathcal{L}_{\text {match }} &=\frac{\mathcal{L}_{\text {match }}^{f w d}+\mathcal{L}_{\text {match }}^{b w d}}{2\left(N_{t}+N_{t-n}\right)} \end{aligned}</script><p>整个网络的训练损失由匹配损失和检测器损失构成，它俩的相对权重网络自动学习得到。</p><script type="math/tex; mode=display">\mathcal{L}_{\text {joint }}=\frac{1}{e^{\lambda_{1}}}\left(\frac{\mathcal{L}_{\text {detect }}^{t}+\mathcal{L}_{\text {detect }}^{t-n}}{2}\right)+\frac{1}{e^{\lambda_{2}}} \mathcal{L}_{\text {match }}+\lambda_{1}+\lambda_{2}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>数据集使用的是MOT16、MOT17和KITTI作为2D评估，nuScenes作为3D评估标准数据集。</p><p>首先是对比多个backbone检测器，效果如下图，发现CenterNet效果最好，所以后面都采用CenterNet作为backbone。</p><p><img src="https://i.loli.net/2021/02/10/eg8FmGyZpckjEqB.png" alt=""></p><p>在MOT17、KITTI和nuScenes上的与其他方法对比如下，均达到了SOTA表现，作者还进行了效果分析和消融实验，感兴趣的可以查看原论文（包括补充材料）。</p><p><img src="https://i.loli.net/2021/02/10/Qljqs2OTNoFvbht.png" alt=""></p><p><img src="https://i.loli.net/2021/02/10/N6pkR2BD3tdZYFE.png" alt=""></p><p><img src="https://i.loli.net/2021/02/10/zUVAYCOmIXDdoj3.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文提出了一种新的联合检测和跟踪的MOT方法，在多个基准上达到SOTA表现，可以在主流的目标检测器的基础上构建，非常灵活高效，是值得关注的MOT新方法。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DEFT解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DetNet解读</title>
      <link href="2021/02/02/detnet/"/>
      <url>2021/02/02/detnet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在目标检测最常见的二阶段和单阶段范式下，一般将目标检测模型分为backbone、neck和head三个部分，其中backbone指的是骨干网络，它用于提取图像的特征。目前最流行的backbone选择是ResNet系列及其变种，这主要归功于ResNet的流行，有着大量的结构优化和预训练模型。</p><p>不过，其实在2018年，旷世就提出了一种名为DetNet的backbone，旨在更好地适应目标检测这个任务。DetNet的设计初衷是什么呢，其实是因为目标检测通常都按照迁移学习的思路直接采用ImageNet上预训练的图像分类模型来作为backbone（如ResNet50），然而，图像分类和目标检测其实还是有不少不同之处的。不少目标检测新方法都采用不同于图像分类任务的额外stage来处理不同尺度的目标，这是因为，目标检测不仅仅需要识别出目标实例的类别，还要在空间上进行定位，较大的下采样倍率获得更大的感受野有利于全图级别的图像分类，但不利于网络对于目标的定位能力。</p><p>针对这些目标检测和图像分类的gap，论文设计了一种全新的backbone—-DetNet，通过对ResNet进行修改，它能够在较深层保持较高的空间分辨率，实验表明，DetNet无论在目标检测还是实例分割任务上都能获得SOTA结果。</p><ul><li><p>论文标题</p><p>  DetNet: A Backbone network for Object Detection</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/1804.06215">http://arxiv.org/abs/1804.06215</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/guoruoqian/DetNet_pytorch">https://github.com/guoruoqian/DetNet_pytorch</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>如上一节所述，目标检测任务和图像分类任务是有区别的，这是由它们任务本身决定的，目标检测不仅仅需要识别对象的类别，还需要空间上定位出对象的边界框。</p><p>具体而言，用分类模型作为检测的backbone存在两个问题：第一，FPN等结构会在原来的backbone结构基础上添加额外的stage来完成不同尺度目标的检测，这个额外添加的stage是无法在ImageNet上预训练的。第二，由于这些backbone是来自于分类任务，因此它们一般会采取较大的下采样倍率来获取较大的感受野，更大的感受野保证了更多的语义信息，这是有利于分类任务的，然而这种较小的特征图空间分辨率太小是不利于大目标和小目标的定位的。</p><p>一个好的backbone应该能解决上述的问题，因此作者提出了DetNet。由于目标的尺度变化，因此DetNet也类似FPN那样使用了额外的stage，不过即使在额外的stage中，空间分辨率不会进行下采样从而保持不变。但是，高分辨率特征图的代价就是会带来比较多的计算量，因此作者设计了一个膨胀bottleneck来保证DetNet的高效。</p><h2 id="DetNet"><a href="#DetNet" class="headerlink" title="DetNet"></a>DetNet</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>分类网络不适用于目标检测，因此诞生了一些新的方法如Feature Pyramid Network（FPN），如下图的A所示。使用FPN以及膨胀卷积是后来目标检测器的常用配置，不过这些检测方法仍然存在一些问题。</p><p><img src="https://i.loli.net/2021/02/02/REr2cKCV3TxkoNe.png" alt=""></p><ol><li><strong>网络的stage数目不同。</strong> 上图的B是经典的分类网络，它由5个stage组成（上图各个网络第一个stage没有画出来），每个stage会下采样2倍，因此最终会下采样$2^5 = 32$倍。但是FPN在此基础上上添加了新的stage即P6，RetinaNet添加了P6和P7来处理大目标，这些stage是无法在ImageNet上预训练的。</li><li><strong>大目标识别能力差。</strong> 较高的下采样倍率会产生更多的语义信息，但也会丢失细节信息因而不利于目标的定位。FPN中，大目标的识别和定位由较深层进行，此时目标的边界细节其实已经严重丢失，因此回归的难度很大。</li><li><strong>小目标识别能力差。</strong> 随着特征图空间分辨率的降低以及上下文信息的整合，小目标信息很容易被削弱（因为其信息本身就不多）。因此，FPN在较浅层进行小目标的预测，然而浅层不具有充分的语义信息，无法满足分类的需要，因此检测器必须使用深层的语义信息包含的上下文信息来加强它，这就是上图A所示的自底向上的路径的由来，深层特征图是通过上采样和上层特征图融合的。但是，当小目标由于不断地下采样，在深层特征图上丢失的时候，其实它也就没有所谓的上下文信息了。</li></ol><p>为了解决这些问题，DetNet其实有几个初衷。一方面，DetNet所有层都是可以在ImageNet上预训练的，另一方面希望得到兼具高分辨率和高语义的特征。</p><h3 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a>结构设计</h3><p>DetNet以最常用的ResNet50为baseline，并保证1-4的stage和ResNet50一致。要设计一个适用于目标检测的backbone其实主要有两个难点，一方面，保证网络输出的高分辨率是非常消耗算力的；另一方面，减少下采样倍率会导致语义信息不充分，从而有害于分类等任务。DetNet就是精心设计来处理这两个难点的，它的前4个stage和ResNet50一样，区别从第5个stage开始。<strong>需要说明的是，这个网络当然也要适用于分类任务，因为需要在ImageNet上预训练。</strong></p><p>首先，DetNet从stage4开始，下采样倍率维持16不变，并且引入了新的stage即P6来用于后续的目标检测任务，这和FPN是类似的。</p><p>接着，为了保证stage4之后的特征图尺寸不变，每个stage会首先经过一个以1x1卷积（这个1x1卷积其实担负了跨通道的信息融合的作用，非常有效）为核心的膨胀bottleneck中，它的结构如下图B所示，它使用了膨胀卷积来扩大感受野。膨胀卷积也是很消耗算力的，所以stage5和stage6的通道数不会像ResNet那样加倍，而是保持不变，以此减少计算开销。整个网络和FPN类似，最后的几个stage如下图的D和E，在整个数据流动过程中和ResNet是类似的，只是重复B模块和A模块，但是，在自底向上进行特征图融合的时候，经过一个1x1卷积来进行跨stage的信息融合。</p><p><img src="https://i.loli.net/2021/02/02/QLbDGK9qT2lImsf.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>按照一般的设置预训练backbone并用于检测，和其他方法的对比如下，实验表明，DetNet59其实是在精度上对标ResNet101的，但是算力消耗上其实只比ResNet50多一些。</p><p><img src="https://i.loli.net/2021/02/02/P5fcwh1xdsLzq6R.png" alt=""></p><p>同时，作为一个设计用于目标检测的backbone，它不在ImageNet上预训练的表现如何呢，如下表所示，效果也是比ResNet50好不少的。</p><p><img src="https://i.loli.net/2021/02/02/5IFzDcTYxXp87v3.png" alt=""></p><p>作者也在准确率和召回率方面做了实验，可以看到，DetNet在大物体的准确率上提升其实挺大的，且IOU阈值越大越显著，这也说明其最终特征图还保留不少分辨率信息，对定位很有效。此外，IOU阈值要求不高的时候，小目标召回率很高，这说明它也能很好处理小目标漏检问题。</p><p><img src="https://i.loli.net/2021/02/02/bj8ZpAJxVeigGs5.png" alt=""></p><p><img src="https://i.loli.net/2021/02/02/9bM8F7uJCliApBK.png" alt=""></p><p>当然，作者也做了实验证明了我之前所说的那个1x1卷积的巨大效果，还有膨胀卷积的实验我这里就不贴了。</p><p><img src="https://i.loli.net/2021/02/02/sQKg3qr1TAuoRft.png" alt=""></p><p>最后附上一张在检测赛道上和SOTA的对比，提升还是蛮显著的，这篇文章的实验也非常丰富。</p><p><img src="https://i.loli.net/2021/02/02/DpqN2LRXde1ztZf.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，DetNet针对图像分类模型不适用于目标检测任务进行了改进，在backbone上做文章还取得了很不错的效果，实现简单，计算量小，也不需要太多的trick，还是很值得了解的。</p><p><img src="https://i.loli.net/2021/02/02/HnCQwRhbdqya3O6.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DetNet解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MLT解读</title>
      <link href="2021/01/21/mlt/"/>
      <url>2021/01/21/mlt/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在很多方法都在检测上做文章的时候，能看到MLT这样一篇在Public赛道做研究并取得相当好的效果的文章是很难得的，就像论文的标题一样，它直面了MOT目前针对二维图像的一个痛点，那就是遮挡问题尤其是密集人群场景下的遮挡问题。论文提出了一种基于图的near-online跟踪方法，该方法设计了一种检测multiplexing（多路复用）技术并设计了一种多标签图（multiplex labeling graph，MLG）模型，<strong>这种一个检测框可以拥有多个ID的复用思路是本文最大的创新点之一</strong>。恶心哦此外，这篇文章还引入LSTM来对构建的MLG进行优化，实验证明其在多个数据集上达到了SOTA表现，在MOT Challenge公榜（public赛道，不使用私有检测，将研究重点放在跟踪问题上）成为了新的SOTA，最新的MOT17的全榜结果如下图，其实可以看到，在整个榜单上，它MOTA距离第一不算太多，IDF1和Frag这些比较依赖跟踪性能而不是检测性能的指标上相对于其他方法都是当之无愧的SOTA。</p><ul><li><p>论文标题</p><p>  Multiplex Labeling Graph for Near-Online Tracking in Crowded Scenes</p></li><li><p>论文地址</p><p>  <a href="https://ieeexplore.ieee.org/document/9098857">https://ieeexplore.ieee.org/document/9098857</a></p></li></ul><p><img src="https://i.loli.net/2021/01/21/nGr67Em5eyVw4Ff.png" alt=""></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>多目标跟踪（multiple object tracking，MOT）是计算机视觉领域一个颇受关注的话题，其对于监控分析、远程控制等工业场景应用广泛。不同于车辆跟踪，只发生在固定的车道约束内，行人的移动往往是没什么约束的。因此行人跟踪会更加困难。TBD（tracking by detection）范式是目前MOT领域最常用的框架，它的思路分为两步，先是将每一帧上的目标检测出来，然后通过一定的度量指标对帧间的目标进行数据关联。很多基于TBD范式的MOT方法在人群稀疏的场景下效果不错，然而人群密集的场景下仍旧缺少比较高效的方法。</p><p><img src="https://i.loli.net/2021/01/21/qtOQMEzYgb546PL.png" alt=""></p><p>密集场景下跟踪的难点其实在于行人频繁的互动遮挡，为了分析行人之间的交互问题，论文中分析并定义了三种交互模式。gathering；side by side；dispersion。含义就是字面意思，其他复杂的交互都可以由着三种基本模式组合而成，如上图所示，当行人并排走的时候，遮挡也就发生了，这给跟踪带来了严重的干扰。</p><p>当然，当多个行人发生严重的遮挡时，检测器其实很难得到多个结果，因为他们离得太近了，因此检测器只会给出一个包裹整个范围的检测框，如下图所示，先忽略其两个id的双色框，其实这个检测框（下文简称bbox）包含两个目标，只是其中一个被遮挡了。<strong>这种单框输出是检测器的选择，因为对于目标检测这个不需要考虑时序信息的空间任务而言，那么“所见即所得”，那个位置当然只有一个可见目标。然而，对于跟踪这个需要时序信息的任务而言，两个目标空间上合二为一会在关联时造成和现有轨迹的一对多匹配问题。</strong> 因此，现有的跟踪器通常有一个基本假设，那就是一个bbox就对应一个目标，从而一对多的关联任务被当作冲突处理问题对待，多个轨迹只有一个被保留，其他的都被移除（这样，由于大部分MOT方法都不会立即销毁轨迹，短暂的消失之后还会匹配回来）。</p><p><img src="https://i.loli.net/2021/01/21/dfgRFGAKjHz4SZQ.png" alt=""></p><p>上述这种假设单框对单目标的假设存在一个严重的问题，如本节第一张图的b所示，两个行人不是图a这种轨迹的短暂交互，而是聚集之后一直并肩移动，其中一个人一直处于另一个人的遮挡下，且他们长时间没有分离开。这种情况下，检测器当然只会反馈一个bbox，这个bbox会被赋予一个轨迹ID，而另一个轨迹就彻底消失了。为了生成两个完整的轨迹，论文引入了一种检测复用方法来解决这个一对多关联问题并提出多标签图跟踪模型（MLG）。</p><p>在MLG模型中，图中的任意一个节点可以同时表示多个目标，也就是被赋予多个ID。同时，LSTM结构用来评估检测框的相似度以进行数据关联，基于LSTM用于运动和外观建模的模型结构分别为MAN和AAN。</p><h2 id="MLG"><a href="#MLG" class="headerlink" title="MLG"></a>MLG</h2><p>这一节先来介绍一下MLG这个基于图的跟踪模型是如何实现的。首先，不同于其他的基于图的方法，论文从一个新的角度重新审视了交互过程中检测冲突的问题。大部分MOT方法都基于一个基本假设，每个bbox都表示现实世界地一个目标，实际上这在三维空间中是成立的，但是三维压缩到二维图像中就不一定了，因此，在二维空间中，多个轨迹共享一个bbox的情况是有理由的。</p><p>检测器生成的bbox作为基础节点来表示目标，$D=\left{d_{1}, d_{2}, d_{3}, \ldots, d_{n}\right}$表示视频中所有的检测结果集合，对每个$d_i$，它的宽高为$(w_i,h_i)$，位置为$(x_i,y_i)$，$c_i$表示置信度，此外，$\left(a_{i, 1}, a_{i, 2}, \ldots, a_{i, 1536}\right)$一个1536维CNN特征向量用来描述$d_i$的外观信息，这个向量由在Market1501上训练的PCB网络产生。</p><h3 id="图的构建"><a href="#图的构建" class="headerlink" title="图的构建"></a>图的构建</h3><p>假定第t帧之前的轨迹已经生成了，MLG是一个滑窗内的有向图$G={V ; E}$，如下图所示，该图沿着$t-1$帧到$t+2$帧构建，此时滑窗的窗口为3（实际上算法采用的窗口大小为20时效果较好，这部分后面说明）。</p><p><img src="https://i.loli.net/2021/01/21/WuOjgI2BtTkFdEJ.png" alt=""></p><p>考虑到真实世界的情况和计算效率，在所有检测之间建边是没有必要的，而且，由于检测框已经被NMS处理，因此一帧内不存在一个目标有多个bbox的情况。因此，只需要链接不同帧的bbox即可构成轨迹。将滑窗范围内所有的检测bbox形成的节点组成的集合称为Node Set，将有关联的两个不同的节点之间的有向边组成的集合称为Edge Set，显然任意有向边的出发点应该在结束点之前的帧上。示例如上图所示，<strong>显然，这个边的形成是可以跨帧存在的，这不同于二分图匹配，而是一种滑窗内部的有约束全图匹配。</strong></p><p>通过上面的分析，其实可以看到，图$G$其实可以表示为一个严格的上三角矩阵如下，其中的上三角元素$E_{i, j}$是一个$n_i\times n_j$的矩阵用来描述第$i$帧的$n_i$个检测框和第$j$帧的$n_j$个检测框之间的关联。下三角部分之所以都是$0$矩阵是因为有向边只会在时间线上从前向后存在。</p><script type="math/tex; mode=display">\left[\begin{array}{cccc}0 & E_{t-1, t} & E_{t-1, t+1} & E_{t-1, t+2} \\0 & 0 & E_{t, t+1} & E_{t, t+2} \\0 & 0 & 0 & E_{t+1, t+2} \\0 & 0 & 0 & 0\end{array}\right]</script><p>漂移（Drift）在目标跟踪中一直是一个问题，随着目标的外观变化，轨迹逐渐偏离GT是很常见的现象。论文中通过设置较高的阈值来保证通过边关联的检测来自同一个目标，阈值的确定基于目标的位置关系和目标的外观相似度。具体如下式，其中$t_{ij}$是阈值指示器用来决定两个bbox是否关联到一起。两个bbox$d_i$和$d_j$之间当前仅当$t_{ij}=1$的时候构建边。阈值由两部分组成：<strong>运动</strong>和<strong>外观</strong>，$\tau_{m}$是一个为运动信息设置的自适应阈值，它设置为两个bbox的最大宽度$\max \left(w_{i}, w_{j}\right)$；另一个阈值被设置用来描述外观相似度，采用$a_i$和$a_j$的余弦距离作为指标，$\tau_{a}$设置为$0.9$以避免漂移（注意，这里的$a_i$表示检测框$d_i$的外观向量）。</p><script type="math/tex; mode=display">\begin{aligned}t_{i j} &=t_{m, i j} \cdot t_{a, i j} \\t_{m, i j} &=\left\{\begin{array}{ll}0, & \left\|\left(x_{i}, y_{i}\right),\left(x_{j}, y_{j}\right)\right\|_{2}>\tau_{m} \\1, & \left\|\left(x_{i}, y_{i}\right),\left(x_{j}, y_{j}\right)\right\|_{2} \leq \tau_{m}\end{array}\right.\\t_{a, i j} &=\left\{\begin{array}{ll}0, & \cos \left(a_{i}, a_{j}\right)<\tau_{a} \\1, & \cos \left(a_{i}, a_{j}\right) \geq \tau_{a}\end{array}\right.\end{aligned}</script><h3 id="多标签优化"><a href="#多标签优化" class="headerlink" title="多标签优化"></a>多标签优化</h3><p>在图构建完成之后，MOT任务其实可以视作一个给节点安排标签的图优化问题，不同于其他图方法，论文中的方法不约束每个节点优化过程只拥有一个标签（指的就是人的ID），因此MLG中每个节点同时可以拥有多个标签，这也是MLG名为multiplex labeling graph的由来。对整个视频的跟踪其实通过一个滑动窗口完成，如帧$1$到帧$k$，帧$k$到帧$2k-1$，这里的$k$就是窗口大小，整个跟踪框架如下图所示。</p><p><img src="https://i.loli.net/2021/01/21/6cVks145GDAn2ZX.png" alt=""></p><p>上图描述的是跟踪的大体过程，其中彩色的实心圆表示已经确认的检测框、白色实心圆表示未确认的检测框（确不确认表示是否链接到轨迹上），虚线圆则表示误检。MLG上做跟踪其实就是在滑窗内将未确认的检测框关联到已经存在的轨迹上或者形成新的轨迹。</p><p>因为下面关于图优化的叙述比较复杂，作者这里先引入了几个概念，将节点分为如下四种，在滑窗内其定义如下：</p><ol><li><strong>Discrete Node</strong>，指的是没有任何关联节点的节点，它的入度和出度都是0；</li><li><strong>Initial Node</strong>，指的是一个节点在帧$k$上，它仅仅关联到$k$帧后的帧上的节点，它的入度为0。窗口内第一帧上的节点不是discrete node，而是initial node。</li><li><strong>Terminal Node</strong>，指的是一个节点在帧$k$上，如果它仅仅关联到$k$帧前的帧上的节点，它的出度为0。同上，窗口内最后一帧上的节点不是discrete node，而是terminal node。</li><li><strong>Intermediate Node</strong>，指的是不属于上述三种节点的节点。</li></ol><p>在MLG中，一个节点同时可以表示真实世界的多个目标，为了描述这个属性，为其定义了multiplex degree（多路复用度，简称MD）来表征一个节点表示的目标数目。考虑到现在的检测器精度都不低，一个bbox不可能包含太多的目标，因此设置了$d_max$来表示最大多路复用度（maximum multiplex degree，MMD），来限制一个节点表示的目标数目。MLT这个方法中，$d_max$被设置为2且得分阈值设置为1。现在将跟踪任务定义为在MLG上找到拥有最大得分的最优路径，公式如下。</p><script type="math/tex; mode=display">\begin{array}{rl}\arg \max _{x} & E=\sum S_{i} x_{i} \\& \forall v_{i} \in V, \quad D\left(v_{i}\right) \leq d_{\max } \\& x_{i}=0,1\end{array}</script><p>上面这个公式中，$S_i$表示路径$i$的得分，而$x_i$表示是否路径$i$被选中，$D(V_i)$表示节点$v_i$的多路复用度（下文简称MD），MMD用$d_max$表示。下图表示各种节点的转换机制，当一个检测得到的未定义节点进入窗口，它会被依据关联关系划分为不同的类型，这个类型会随着滑窗转换。</p><p><img src="https://i.loli.net/2021/01/21/wT489mVOcAnvsPq.png" alt=""></p><p><img src="https://i.loli.net/2021/01/21/I2AyXx6wErnopSe.png" alt=""></p><p>上图是真个MLG优化算法，它的输入是图$G$而输出是轨迹集$T$，算法描述相对比较直白，这里就不多解释了。下图是MLG求解的一个示例，图中共有5条轨迹形成，其中track-4是新轨迹，track-1和track-2在第$t$帧共享同一个检测框，这在现实世界的表现就是这两个人在帧$t$发生了遮挡，但随后两人分开了。</p><p><img src="https://i.loli.net/2021/01/21/BEwMa2VeCKzDptN.png" alt=""></p><h2 id="LSTM目标关联"><a href="#LSTM目标关联" class="headerlink" title="LSTM目标关联"></a>LSTM目标关联</h2><p>此前提到的最优化公式中，$S$作为得分被安排给MLG中每个路径来评估其为真实世界轨迹的概率，这个得分（score）由两部分组成，分别是运动因子$S_m$和外观因子$S_a$，最终的score由两者的和$S=S_{m}+S_{a}$表示。这里为候选轨迹分配score的过程实际上是一个数据关联问题，论文引入了使用LSTM的关联打分方法，不同于此前的IOU打分或者外观打分。</p><p>基于LSTM诞生了两者模型思路，分别是生成式模型和判别式模型，前者根据检测预测现有轨迹未来的位置和外观，后者则对当前帧给出每个检测框属于现有轨迹的概率。考虑到训练数据集的尺度，文中采用判别式模型的思路，构建LSTM网络进行关联度量。</p><p>由于运动信息和外观信息差别较大，因此设计了两个网络分别处理，前者为motion association network（MAN），后者为appearance association network（AAN）。首先来看MAN，它用来评估候选轨迹的运动特征，它是如下图所示的一个sequence-to-one型的网络结构，它的输入是$n$个四维向量，输出是这$n$个检测框属于同一个目标的概率。输入是$n$个bbox的标准化的运动特征，具体对检测$d_i$而言，运动特征为下式，其中$W$和$H$表示视频的宽高，而$\mu$和$\sigma$表示每个维度的均值和标准差。输出的正概率作为$S_m$。</p><script type="math/tex; mode=display">\left(\frac{\frac{x_{i}}{W}-\mu_{x}}{\sigma_{x}}, \frac{\frac{y_{i}}{H}-\mu_{y}}{\sigma_{y}}, \frac{\frac{w_{i}}{W}-\mu_{w}}{\sigma_{w}}, \frac{\frac{h_{i}}{H}-\mu_{h}}{\sigma_{h}}\right)</script><p><img src="https://i.loli.net/2021/01/21/MOPeBLrWKJlDbfv.png" alt=""></p><p>同样的，AAN也是一个sequence-to-one型网络，其结构如下图，输入是$n$个1536维外观特征向量，输出同样是$n$个检测框属于同一个目标的概率，输入的外观特征同样经过了标准化。输出的正概率作为$S_a$。</p><p><img src="https://i.loli.net/2021/01/21/ZsVToUdnl2rwHpL.png" alt=""></p><p>就这样，对一个路径$p_i$，依据MAN和AAN计算其得分$S_i=S_{m,i}+S_{a,i}$，因此这个得分的取值范围是从0到2。</p><p>最后，作者为了训练这两个网络，使用了两个额外数据集CVPR19 challenge和PathTrack，它们的数据量比较充足，对这两个数据集作者进行了一些预处理以方便模型的训练，这部分可以参考原论文。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者主要在MOT16和MOT17上进行实验，首先验证了MLT框架的可行性，在baseline基础上的最大改进就是单节点多标签策略，采用不同的滑动窗口大小的结果如下表，实验表明，效果提升是非常显著的，而且一般MMD设置为2即可。</p><p><img src="https://i.loli.net/2021/01/21/VnDhHK4pJy9OPeo.png" alt=""></p><p><img src="https://i.loli.net/2021/01/21/QEnYdRqgtzU9l6L.png" alt=""></p><p>上图是作者做的一个可视化，我这里只分析第一行，图a两个目标逐渐接近，最后遮挡其中一个目标，通过多标签技术，在图b时检测框拥有两个目标ID，最后，图c上两人分开，检测框也带着ID分离。这表明，MLT对于遮挡的鲁棒性很强。</p><p>接着，对参数进行了一些分析，比如窗口大小、MMD设置数值等，结果如下。</p><p><img src="https://i.loli.net/2021/01/21/flsEdGXrcxgouwj.png" alt=""></p><p><img src="https://i.loli.net/2021/01/21/MNoeW9u8slDaXFT.png" alt=""></p><p>最后，使用公开检测打榜，在MOT16和MOT17上结果如下，它在多个跟踪相关的指标上都实现了新的SOTA。此外，榜单上的MLT_p为使用私检的MLT版本，它在MOTA达到了70.6且IDF1为72.1，此外，MLT方法的IDSw数量大幅减小，这证明轨迹的稳定性时很强的。</p><p><img src="https://i.loli.net/2021/01/21/m3BA2xc6Ltwrfj4.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>抛开速度不谈，MLT其实是一个非常具有创新性的MOT方法，它采用单检测多标签这个开创性的想法，非常针对性地处理了密集人群场景下的遮挡问题，在关于ID等跟踪方面的指标大幅度提升。MLT在视频处理领域如监控分析任务上有巨大的潜力，值得工业界的关注，尽管由于它时near-online方法几乎无法用于realtime跟踪，但是为MOT的研究开辟了新的道路。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MLT解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FcaNet解读</title>
      <link href="2021/01/18/fcanet/"/>
      <url>2021/01/18/fcanet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>注意力机制，特别是通道注意力机制在计算机视觉中取得了巨大的成功，很多工作着重于设计更加高效的通道注意力结构，却忽略了一个重要的问题，那就是为了得到每个通道全局表示所使用的全局平均池化（Global Average Pooling，GAP）真的合适吗？FcaNet这篇文章，作者从频域角度重新思考GAP，为了弥补了现有通道注意力方法中特征信息不足的缺点，将GAP推广到一种更为一般的2维的离散余弦变换（DCT）形式，通过引入更多的频率分量来充分的利用信息。设计的高效Fca模块甚至在现有的通道注意力方法基础上，只需要修改一行代码即可实现。</p><ul><li><p>论文标题</p><p>  FcaNet: Frequency Channel Attention Networks</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2012.11879">http://arxiv.org/abs/2012.11879</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/cfzd/FcaNet">https://github.com/cfzd/FcaNet</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>注意力机制在计算机视觉中受到了广泛的关注，它让网络更加关注于部分重要的信息，按照作用维度的不同，我们将注意力分为空间注意力、通道注意力和自注意力，其中，由于简单高效，下图所示的通道注意力直接对不同的通道加权，成为一种主流的注意力范式。</p><p><img src="https://i.loli.net/2021/01/18/lC34dQHptugA6M7.png" alt=""></p><p>SENet和ECANet致力于设计不同的通道加权函数，如全连接或者一维卷积，然后这些函数的输入都是每个通道一个标量，这个标量默认都是来自于GAP，这是因为GAP相当的简洁高效，但是GAP也有不可忽略的问题，那就是GAP没办法捕获丰富的输入表示，这就导致了经过GAP得到的特征缺乏多样性，这主要是因为GAP对一个通道所有空间元素取其均值，而这个均值其实不足以表达不同通道的信息。</p><p>作者对全局平均池化即GAP进行了理论上的分析，最终得出如下结论：首先，不同的通道有极大概率出现相同的均值，然而它们的语义信息是不同的，换句话说，GAP抑制的通道之间的多样性；其次，从频域角度来看，作者证明了GAP其实是离散余弦变换（DCT）的最低频分量，这其实忽略了很多其他有用的分量；最后，CBAM的成功也佐证了只使用GAP得到的信息是不足够的。</p><p>在这些结论的基础上，作者设计了一种新的高效多谱通道注意力框架。该框架在GAP是DCT的一种特殊形式的基础上，在频域上推广了GAP通道注意力机制，提出使用有限制的多个频率分量代替只有最低频的GAP。通过集成更多频率分量，不同的信息被提取从而形成一个多谱描述。此外，为了更好进行分量选择，作者设计了一种二阶段特征选择准则，在该准则的帮助下，提出的多谱通道注意力框架达到了SOTA效果。</p><h2 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h2><h3 id="通道注意力"><a href="#通道注意力" class="headerlink" title="通道注意力"></a>通道注意力</h3><p>通道注意力的权重学习如下式所示，它表示输入经过GAP处理后由全连接层学习并经过Sigmoid激活得到加权的mask。</p><script type="math/tex; mode=display">a t t=\operatorname{sigmoid}(f c(\operatorname{gap}(X)))</script><p>然后，mask与原始输入经过下式逐通道相乘得到注意力操作后的输出。</p><script type="math/tex; mode=display">\tilde{X}_{:, i,:,:}=a t t_{i} X_{:, i,:, :}, \text { s.t. } i \in\{0,1, \cdots, C-1\}</script><h3 id="离散余弦变化"><a href="#离散余弦变化" class="headerlink" title="离散余弦变化"></a>离散余弦变化</h3><p>DCT是和傅里叶变换很相似，它的基本形式如下，$f \in \mathbb{R}^{L}$为DCT的频谱，$x \in \mathbb{R}^{L}$为输入，$L$为输入的长度。</p><script type="math/tex; mode=display">f_{k}=\sum_{i=0}^{L-1} x_{i} \cos \left(\frac{\pi k}{L}\left(i+\frac{1}{2}\right)\right), \text { s.t. } k \in\{0,1, \cdots, L-1\}</script><p>进而，我们推广得到二维DCT如下，$f^{2 d} \in \mathbb{R}^{H \times W}$是二维DCT的频谱，$x^{2 d} \in \mathbb{R}^{H \times W}$是输入，$H$和$W$是输入的高和宽。</p><script type="math/tex; mode=display">\begin{array}{l}f_{h, w}^{2 d}=\sum_{i=0}^{H-1} \sum_{j=0}^{W-1} x_{i, j}^{2 d} \underbrace{\cos \left(\frac{\pi h}{H}\left(i+\frac{1}{2}\right)\right) \cos \left(\frac{\pi w}{W}\left(j+\frac{1}{2}\right)\right)}_{\text {DCT weights }} \quad  \text { s.t. } h \in\{0,1, \cdots, H-1\}, w \in\{0,1, \cdots, W-1\}\end{array}</script><p>同样，逆DCT变换的公式就如下了。</p><script type="math/tex; mode=display">\begin{array}{l}x_{i, j}^{2 d}=\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} f_{h, w}^{2 d} \underbrace{\cos \left(\frac{\pi h}{H}\left(i+\frac{1}{2}\right)\right) \cos \left(\frac{\pi w}{W}\left(j+\frac{1}{2}\right)\right)}_{\text {DCT weights }} \quad\text { s.t. } i \in\{0,1, \cdots, H-1\}, j \in\{0,1, \cdots, W-1\} \end{array}</script><p>上面两个式子中，为了简单起见，移除了一些常数标准化约束因子。DCT变换属于信号处理领域的知识，是JPEG图像压缩的核心算法，相当于是对重要信息的聚集。其实从这里可以看出来，DCT变换其实也是一种对输入的加权求和，式子中的余弦部分就是权重。因此，GAP这种均值运算可以认为是输入的最简单频谱，这显然是信息不足的，因此作者引出了下面的多谱通道注意力。</p><h3 id="多谱通道注意力"><a href="#多谱通道注意力" class="headerlink" title="多谱通道注意力"></a>多谱通道注意力</h3><p>这里作者首先按证明了GAP其实是二维DCT的特例，其结果和二维DCT的最低分量成比例。这个证明作者是令$h$和$w$都为0得到的，其中$f_{0,0}^{2 d}$表示二维DCT最低频分量，显然，结果来看它与GAP成正比。</p><script type="math/tex; mode=display">\begin{aligned}f_{0,0}^{2 d} &=\sum_{i=0}^{H-1} \sum_{j=0}^{W-1} x_{i, j}^{2 d} \cos \left(\frac{0}{H}\left(i+\frac{1}{2}\right)\right) \cos \left(\frac{\theta}{W}\left(j+\frac{1}{2}\right)\right) \\&=\sum_{i=0}^{H-1} \sum_{j=0}^{W-1} x_{i, j}^{2 d} \\&=g a p\left(x^{2 d}\right) H W\end{aligned}</script><p>通过上面的结论，自然会想到将其他分量引入通道注意力中，首先，为了叙述方便，将二维DCT的基本函数记为$B_{h, w}^{i, j}=\cos \left(\frac{\pi h}{H}\left(i+\frac{1}{2}\right)\right) \cos \left(\frac{\pi w}{W}\left(j+\frac{1}{2}\right)\right)$，继而将逆二维DCT变换改写如下，</p><script type="math/tex; mode=display">\begin{array}{l}x_{i, j}^{2 d}=\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} f_{h, w}^{2 d} \cos \left(\frac{\pi h}{H}\left(i+\frac{1}{2}\right)\right) \cos \left(\frac{\pi w}{W}\left(j+\frac{1}{2}\right)\right) \\\stackrel{简写为B}{=} f_{0,0}^{2 d} B_{0,0}^{i, j}+f_{0,1}^{2 d} B_{0,1}^{i, j}+\cdots+f_{H-1, W-1}^{2 d} B_{H-1, W-1}^{i, j} \\\stackrel{GAP特殊形式结论}{=} g a p\left(x^{2 d}\right) H W B_{0,0}^{i, j}+f_{0,1}^{2 d} B_{0,1}^{i, j}+\cdots+f_{H-1, W-1}^{2 d} B_{H-1, W-1}^{i, j} \\\text { s.t. } i \in\{0,1, \cdots, H-1\}, j \in\{0,1, \cdots, W-1\}\end{array}</script><p>由这个式子其实不难发现，此前的通道注意力只应用了第一项的最低频分量部分，而没有使用下式表示的后面其他部分，这些信息都被忽略了。</p><script type="math/tex; mode=display">X=\underbrace{g a p(X) H W B_{0,0}^{i, j}}_{\text {utilized }}+\underbrace{f_{0,1}^{2 d} B_{0,1}^{i, j}+\cdots+f_{H-1, W-1}^{2 d} B_{H-1, W-1}^{i, j}}_{\text {discarded }}</script><p>基于此，作者设计了多谱注意力模块（Multi-Spectral Attention Module，），该模块通过推广GAP采用更多频率分量从而引入更多的信息。</p><p>首先，输入$X$被沿着通道划分为多块，记为$\left[X^{0}, X^{1}, \cdots, X^{n-1}\right]$，其中每个$X^{i} \in \mathbb{R}^{C^{\prime} \times H \times W}, i \in{0,1, \cdots, n-1}, C^{\prime}=\frac{C}{n}$，每个块分配一个二维DCT分量，那么每一块的输出结果如下式。</p><script type="math/tex; mode=display">\begin{aligned}F r e q^{i} &=2 \mathrm{DDCT}^{u, v}\left(X^{i}\right) \\&=\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} X_{:, h, w}^{i} B_{h, w}^{u, v} \\& \text { s.t. } i \in\{0,1, \cdots, n-1\}\end{aligned}</script><p>上式中的$[u, v]$表示2DDCT的分量下标，这就对每一块采用不同的频率分量了，因此下式得到最终的输出$Freq \in \mathbb{R}^{C}$就是得到的多谱向量，然后再将这个向量送入通道注意力常用的全连接层中进行学习得到注意力图。</p><script type="math/tex; mode=display">\text { Freq }=\operatorname{cat}\left(\left[\text { Fre } q^{0}, \text { Fre } q^{1}, \cdots, \text { Freq }^{n-1}\right]\right)</script><script type="math/tex; mode=display">m s_{-} a t t=\operatorname{sigmoid}(f c(\text { Freq }))</script><p>这就是全部的多谱注意力模块的设计了，现在，下图这个FcaNet整体框架中间的一部分就看得明白了，唯一留下的问题就是对分割得到的每个特征图块，如何选择$[u,v]$呢？事实上，对空间尺寸为$H\times W$的特征图，会有$HW$个频率分量，由此频率分量的组合共有$CHW$种，遍历显然是非常费时的，因此，文中设计了一种启发式的两步准则来选择多谱注意力模块的频率分量，其主要思想是先得到每个频率分量的重要性再确定不同数目频率分量的效果。具体而言，先分别计算通道注意力中采用各个频率分量的结果，然后，根据结果少选出topk个性能最好的分量。</p><p><img src="https://i.loli.net/2021/01/18/6rVxBHFSzWh1y2L.png" alt=""></p><p>到这里，整个FcaNet的方法论就说明完了，下面作者还进行了一些讨论，包括计算复杂度和修改难度。复杂度方面，相比于SENet没有引入额外参数，因为2DDCT权重是预先计算好的，相比于SENet增加的计算量几乎可以忽略不记。此外，2DDCT可以认为是对输入的加权求和，因此它可以通过逐元素乘加实现，在原本通道注意力代码基础上前向计算只需要修改一行，其PyTorch实现如下图。</p><p><img src="https://i.loli.net/2021/01/18/Buk6VcWI94Hsvy8.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者首先经过实验对比了单个频率分量的有效性，具体的实验配置可以查看原论文，不过，实验最终验证当$[u,v]$为[0,0]时效果是最好的，这就是GAP操作，下图同时也验证了深度模型对低频分量更关注的事实。但是，虽然其他分量效果不及GAP，但也是含有大量有效信息的，因此加进去是合理的。</p><p><img src="https://i.loli.net/2021/01/18/pX18sD5VUnTjAP7.png" alt=""><br>接着，在知道了哪些分量更好的前提下，作者对分量的数目进行了实验，结果如下表，Number=1的GAP效果弱于其他各种Number选择的多谱注意力，Number=16时效果最好。</p><p><img src="https://i.loli.net/2021/01/18/GRj4bQWqHakvB6d.png" alt=""></p><p>最后，作者也将本文设计的FcaNet和其他注意力网络进行对比，在分类、检测和分割等benchmark上和其他方法对比，在分类和检测上都有比较明显的提高，下图所示为ImageNet上分类任务结果，其他的我这里就不展示了，可以查看原文。</p><p><img src="https://i.loli.net/2021/01/18/CwsgWMS1rPOKG9b.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文是依据可靠数学进行的理论创新，一改此前通道注意力的结构设计思路，将关注重心放在了GAP是DCT的特例这一推导下，从而设计了多谱通道注意力，取得了相当不错的成果。将思路转到频域还是非常新颖的，虽然一行代码的说法略显牵强，最终的性能也是坐等开源再说，总体来看，FcaNet还是值得了解的注意力文章。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FcaNet解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TransTrack解读</title>
      <link href="2021/01/12/transtrack/"/>
      <url>2021/01/12/transtrack/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Transformer已经在计算机视觉各个任务上获得了不错的效果，当然也不会放过多目标跟踪，香港大学、字节跳动AI实验室等机构在2020年最后一天挂到Arxiv上的这篇文章就是第一个将Transformer用于多目标跟踪的文章，尽管紧随其后就出现了TrackFormer这样类似的作品，不过本人觉得TransTrack效果目前看来好一些，本文就自己的理解谈谈这篇文章。</p><ul><li><p>论文标题</p><p>  TransTrack: Multiple-Object Tracking with Transformer</p></li><li><p>论文地址</p><p>  <a href="http://arxiv.org/abs/2012.15460">http://arxiv.org/abs/2012.15460</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/PeizeSun/TransTrack">https://github.com/PeizeSun/TransTrack</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>回顾MOT的核心思路，如下图所示，目前TBD范式的多目标跟踪方法依然受限于复杂的pipeline，这带来了大量的计算代价，如下图(a)所示，一如当年的DeepSORT，检测和跟踪任务分开进行，这会带来一些比较严重的问题：一方面，这种两个任务分开进行会造成它们不能共享有效的信息带来额外的算力消耗；另一方面，连续两帧间的无序目标对和每帧中不完整的检测都为跟踪算法带来了极大的挑战。<strong>因此，JDE范式的产生其实是MOT领域的发展非常重要的一步。</strong> </p><p><img src="https://i.loli.net/2021/01/12/CZYSFnKIE24GprH.png" alt=""></p><p>上图的(b)其实是SOT中常见的孪生网络，这种结构本质上就是Query-Key机制，目标对象是query而各个图像区域是keys。直接将这个思路引入MOT中也是可行的，前一帧的目标特征作为query，当前帧的图像特征作为key，这就是上图(c)的由来。然而，仅仅将Query-Key机制引入MOT中效果非常差，主要是FN指标上的表现（在检测中的直观含义就是漏检），原因也很简单，因为当前帧出现的新目标的特征肯定不存在于query中，因此当然无法获取key，这也就造成了新目标的缺失。</p><p>因此，回到这篇论文的初衷，能否设计一种基于Query-Key机制的MOT框架，它能输出有序目标集也能检测新目标的出现呢？这就诞生了<strong>TransTrack</strong>，一个联合检测和跟踪（JDE范式）的新框架，它利用Query-Key机制来跟踪当前帧中已存在的目标并且检测新目标。</p><p><img src="https://i.loli.net/2021/01/12/hczZfslYqBxeMyK.png" alt=""></p><p>先简单地看一下这个pipeline的设计，它基于Query-Key目前最火热的Transformer架构构建。最中间的key来自骨干网络对当前帧图像提取的特征图，而query按照两个分支的需求分别来自上一帧的目标特征query集和一个可学习的目标query集。<strong>这两个分支都很有意思，我们先看下面这个检测分支，这里这个learned object query思路来自于DETR，是一种可学习的表示，它能逐渐学会从key中查询到目标的位置从而完成检测，想知道得更明白得可以去看看DETR论文。可以很明显地看明白，这个检测分支完成了当前帧上所有目标的检测得到detection boxes。然后我们看上面这个跟踪框分支，这个object feature query其实就是上一帧的检测分支产生的目标的特征向量，这个object feature query从key中查询目标当前帧中位置，用CenterTrack的思路来理解，这可以认为是一个位移预测分支，它最终得到tracking boxes。最后，由于跟踪框和检测框都在当前帧上了，进行简单的IOU匹配就能完成跟踪了，至此，MOT任务完成。</strong></p><p>回顾上面这个TransTrack的设计，其实很清晰地发现其优势，那就是JDE范式下地同时优化两个子网络，速度很快。不过，这个基于检测框和跟踪框匹配的思路，让人有点梦回DeepSORT啊😂。</p><h2 id="TransTrack"><a href="#TransTrack" class="headerlink" title="TransTrack"></a>TransTrack</h2><p>论文里还回顾了一下Query-Key机制在跟踪中的发展以及TBD范式和JDE范式的创新，我这里就不多赘述了，直接来看TransTrack的一些细节。</p><p>首先，作者认为，一个理想的跟踪模型的输出目标集应该是完整且有序的，因此TransTrack使用learned object query和上一帧的object feature query作为输入query。前者经过decoder之后变为当前帧的检测框，后者经过decoder之后变为跟踪框（上一帧目标在当前帧的位置预测）。因此TransTrack在当前帧完成了数据关联步骤，这就允许其才有简单的metric作为关联指标，文中采用IOU。</p><p><img src="https://i.loli.net/2021/01/12/yVX8JuPELAlKQ6s.png" alt=""></p><p>上图是整个网络的结构图，它和TransFormer很像，由一个产生复合特征图的encoder和两个平行的decoder构成。encoder和decoder的具体结构这里不详细讲解了，可以去阅读Transformer论文了解，简单来说它其实由堆叠的多头注意力和全连接层组成。在Transformer的设计中，encoder生成大量的key，而decoder接收指定任务的query对key进行查询得到想要的输出。</p><p>上图所示，encoder将backbone对当前帧提取的特征图和上一帧处理时保留的特征图组合到一起作为输入，这就是keys。然后两个decoder接收这些keys，他俩分别进行目标检测和目标传播任务。首先来看检测这个decoder，它采用DETR的集合预测思路完成检测，可学习的query从全局特征图上查询目标的位置得到检测框。接着，来看看这个目标传播的decoder，它和检测分支的结构类似，不过输入不同，是前帧目标的特征向量，这个向量包含了历史目标的位置和外观信息，所以能够在当前帧上查询到跟踪框。</p><p>最后的Matching模块就很简单了，既然跟踪框和检测框都在当前帧上，那么相同目标只会有很微小的偏移，基于IOU进行匹配即可（这个思路和CTracker类似），匹配算法采用常用的KM算法。未被匹配上的检测框初始化为新目标。</p><h2 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>训练数据集采用了CrowdHuman和MOT Challenge数据集，损失方面，可以认为检测框和跟踪框的获得都是在当前帧进行目标检测，因此两个decoder可以采用同一个训练损失，这是一个二分匹配的集合预测损失，公式如下。${L}_{c l s}$表示框间的类别损失，使用focal loss实现；L1和GIOU loss用于监督框的定位准确率，具体的训练就是采用DETR类似的方式训练。</p><script type="math/tex; mode=display">\mathcal{L}=\lambda_{c l s} \cdot \mathcal{L}_{c l s}+\lambda_{L 1} \cdot \mathcal{L}_{L 1}+\lambda_{g i o u} \cdot \mathcal{L}_{g i o u}</script><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>推理第一帧的时候没有历史目标因此拷贝一份特征图即可，后面都是相邻帧之间的处理，需要注意的是，这里引入了Track Rebirth策略以增强针对遮挡和短时间目标消失的鲁棒性，具体是指如果一个跟踪框未被匹配上，它暂时不被移除，只有确定连续K帧都没有匹配上时才真正认为该目标消失，文中取K=32。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者进行了大量的消融实验，首先验证了额外训练数据确实对效果有不小的帮助。</p><p><img src="https://i.loli.net/2021/01/12/NybrvUouR53tEYF.png" alt=""></p><p>接着对比了不同的Transformer架构的效果，原始的Transformer训练非常耗时，Deformble DETR训练快效果好。</p><p><img src="https://i.loli.net/2021/01/12/MVznJ5DpZe6U8js.png" alt=""></p><p>此外，还验证了两个解码层的有效性，两种query都是必须的。</p><p><img src="https://i.loli.net/2021/01/12/zCxha43WJ8NSmY9.png" alt=""></p><p>最后，来看看下面和SOTA方法的比较，在MOTA、MOTP、FN等指标上达到了SOTA效果，这主要归功于检测器的强大，然而在IDS这个关乎跟踪效果的指标上表现并不好。</p><p><img src="https://i.loli.net/2021/01/12/PJolZkEpfRFAjKM.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TransTrack第一次将Transformer引入到了MOT领域并获得了不错的效果，这里也推荐一篇最新的视觉Transformer综述<a href="http://arxiv.org/abs/2101.01169">Transformers in Vision: A Survey</a>，其涉及了Transformer在视觉各个任务的应用效果。回到TransTrack上来，这篇文章思路上还是很有创新性的，将一个东西用到一个全新的任务上来要做的远远不是1+1=2这么简单，需要做很多很多的工作，而TransTrack做的都不错，在多个MOT的指标上都达到了SOTA水平，不过在MOT很关键的IDS指标上距离SOTA还有一些距离，期待正式版本的论文效果会更好。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Sun P, Jiang Y, Zhang R, et al. TransTrack: Multiple-Object Tracking with Transformer[J]. arXiv:2012.15460 [cs], 2020.</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TransTrack解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视觉注意力机制(下)</title>
      <link href="2020/12/20/attentions-end/"/>
      <url>2020/12/20/attentions-end/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在<a href="https://zhouchen.blog.csdn.net/article/details/111427558">上篇文章</a>中，我介绍了视觉注意力机制一些比较新的作品，包括$A^2$-Nets、GSoP-Net、GCNet和ECA-Net，本篇文章主要介绍一些近年以来最新的成果，包括SKNet、CCNet、ResNeSt和Triplet Attention。</p><p><img src="https://i.loli.net/2020/12/16/ynLluegbMsiINTj.png" alt=""></p><p>本系列包括的所有文章如下，分为上中下三篇，本文是中篇。</p><ol><li>NL(Non-local Neural Networks)</li><li>SENet(Squeeze-and-Excitation Networks)</li><li>CBAM(Convolutional Block Attention Module)</li><li>BAM(Bottleneck Attention Module)</li><li>$\mathbf{A^2}$-Nets(Double Attention Networks)</li><li>GSoP-Net(Global Second-order Pooling Convolutional Networks)</li><li>GCNet(Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</li><li>ECA-Net(Efficient Channel Attention for Deep Convolutional Neural Networks)</li><li><strong>SKNet(Selective Kernel Networks)</strong></li><li><strong>CCNet(Criss-Cross Attention for Semantic Segmentation)</strong></li><li><strong>ResNeSt(ResNeSt: Split-Attention Networks)</strong></li><li><strong>Triplet Attention(Convolutional Triplet Attention Module)</strong></li></ol><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>我们首先还是来回顾一下卷积神经网络中常用的注意力，主要有两种，即<strong>空间注意力和通道注意力</strong>，当然也有融合两者的混合注意力，下图是画的示意图。、</p><p><img src="https://i.loli.net/2021/01/17/9X3I8w6sahxoe1r.png" alt=""></p><p>我们知道，卷积神经网络输出的是维度为$C\times H \times W$的特征图，其中$C$指的是通道数，它等于作用与输入的卷积核数目，每个卷积核代表提取一种特征，所以每个通道代表一种特征构成的矩阵。$H \times W$这两个维度很好理解，这是一个平面，里面的每个值代表一个位置的信息，尽管经过下采样这个位置已经不是原始输入图像的像素位置了，但是依然是一种位置信息。如果，对每个通道的所有位置的值都乘上一个权重值，那么总共需要$C$个值，构成的就是一个$C$维向量，将这个$C$维向量作用于特征图的通道维度，这就叫<strong>通道注意力</strong>。同样的，如果我学习一个$H\times W$的权重矩阵，这个矩阵每一个元素作用于特征图上所有通道的对应位置元素进行乘法，不就相当于对空间位置加权了吗，这就叫做<strong>空间注意力</strong>。</p><p>上篇文章介绍的GCNet是一种空间注意力机制，而ECA-Net是典型的通道注意力。</p><h2 id="网络详解"><a href="#网络详解" class="headerlink" title="网络详解"></a>网络详解</h2><h3 id="SKNet"><a href="#SKNet" class="headerlink" title="SKNet"></a><strong>SKNet</strong></h3><p>SENet 设计了 SE 模块来提升模型对 channel 特征的敏感性，CVPR2019 的 SKNet 和 SENet 非常相似，它主要是为了提升模型对感受野的自适应能力，这种自适应能力类似 SENet 对各个通道做类似 attention，只不过是对不同尺度的卷积分支做了这种 attention。</p><p><img src="https://i.loli.net/2020/12/20/QKsbW69f8v4djVF.png" alt=""></p><p>上图就是SK卷积的一个基础实现，为了方便描述，作者只采用了两个分支，事实上可以按需增加分支，原理是一样的。可以看到，从左往右分别是三个part：Split、Fuse和Select，下面我就一步步来解释这三个操作是如何获得自适应感受野信息的，解释是完全对照上面这个图来的。</p><p><strong>Split</strong>：对给定的特征图$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$，对其采用两种卷积变换$\widetilde{\mathcal{F}}: \mathbf{X} \rightarrow \tilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$和$\mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$，它们只有卷积核size不同（这里以3和5为例），其余配置一致（卷积采用深度可分离卷积，5x5卷积采用3x3卷进进行膨胀）。这一步，通过两个变换构建了两个感受野的分支，形成了两个特征图$\tilde{\mathbf{U}}$和$\widehat{\mathbf{U}}$，它们的维度都是$H\times W \times C$。</p><p><strong>Fuse</strong>：这一步也就是自适应感受野的核心，这里采用最简单的gates机制控制进入下一层的多尺度信息流。因此，这个gates需要集成来自所有分支的信息，还要有权重的集成。首先，通过逐元素相加获得特征图$\mathbf{U}$（$\mathbf{U}=\tilde{\mathbf{U}}+\widehat{\mathbf{U}}$），然后采用SENet类似的思路，通过GAP生成逐通道的统计信息$\mathbf{s} \in \mathbb{R}^{C}$，计算式如下。</p><script type="math/tex; mode=display">s_{c}=\mathcal{F}_{g p}\left(\mathbf{U}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)</script><p>接着，为了更紧凑的表示，通过一个全连接层对$\mathbf{s}$进行降维，获得$\mathbf{z}=\mathcal{F}_{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))$，这里先是和$\mathbf{W} \in \mathbb{R}^{d \times C}$相乘然后经过BN和ReLU，$d$作为一个超参使用下降比$r$来控制，不过$d$同时通过$d=\max (C / r, L)$约束下界（$L$设置为32）。接着，又一个全连接用于升维度，得到分支数个$C$维向量，论文中这里就是$a$和$b$，然后按照通道维度进行soft attention，也就是说$a_{c}+b_{c}=1$，这样可以反映不同尺度的特征的重要性，然后用$a$和$b$采用类似SE的方式对原始特征图$\tilde{\mathbf{U}}$和$\widehat{\mathbf{U}}$进行逐通道相乘加权，得到有通道区分度的特征图，再相加到一起得到输出特征图$\mathbf{V}$。<strong>这个特征图，就是自适应不同感受野获得的特征图。</strong></p><p>SKNet是尺度层面的注意力，虽然其本质还是通道注意力，但这种自适应卷积核的思路确实为卷积神经网络带来了很大的性能提升，被一些新的论文采用了进去。</p><h3 id="CCNet"><a href="#CCNet" class="headerlink" title="CCNet"></a><strong>CCNet</strong></h3><p>CCNet，翻译应该是十字交叉注意力（Criss-Cross Attention），设计用于语义分割。这不是一篇新文章，2018年就出来了，收录于ICCV2019，后来扩充内容之后发表再TPAMI这个视觉顶级期刊上了。这篇文章主要针对的是Non-local进行改进的，Non-local主要用来捕获不同位置像素之间的长程依赖，具体解析<a href="https://zhouchen.blog.csdn.net/article/details/111302952">我之前的文章</a>已经提到了，这里就不多赘述。</p><p><img src="https://i.loli.net/2020/12/20/q2iPX37GsHTdpmN.png" alt=""></p><p>熟悉Non-local的，上图应该不会陌生，图a表示的就是Non-local的操作，它对输入特征图进行了两个分支处理，其中一个分支计算注意力图，一个分支对特征图进行信息变换$g$。图中的蓝色格子代表待处理像素位置$x_i$，红色格子代表结果$y_i$，绿色网格的分支代表求得的注意力图，每个格子(i,j)代表$x_i$和$x_j$之间的相关性，颜色越深，相关性越强。灰色网格的分支代表信息变换函数$g$处理，将处理后的结果和上面分支得到的注意力图相乘，就得到注意力后的结果。</p><p>图b就是论文提出的CC注意力块，可以发现论文题目的由来，它只计算了当前像素周围十字区域位置与自己的相关性，但是我们需要知道所有像素和它的相关性，所以这个结构需要堆叠，实验表明，循环堆叠两层即可覆盖所有的位置，所以作者设计了循环CC注意力模块RCCA。</p><p>在具体讲解CC模块的细节之前，我们先来比较抽象地解释一下怎么样两次循环就覆盖整个像素空间的，作者做了一个严格的数学证明，我这里直观上解释一下。最极端的情况下，$(u_x, u_y)$想要计算自己和$(\theta_x,\theta_y)$的相关性，但是第一轮循环的时候它是永远也不可能在$(\theta_x,\theta_y)$的十字路径上的，$(\theta_x,\theta_y)$的信息最多传给了$(u_x, \theta_y)$这个左上角的点和$(\theta_x, u_y)$这个右上角的点。但是，第二轮循环的时候，这两个点就在$(u_x,u_y)$的十字路径上了，且这两个点聚集了$(\theta_x,\theta_y)$的信息，因此左下角和右上角间接交互上了。其他点也类似，可以将信息通过两次循环传递给左下角的点，由此，左下角其实遍历了所有的点。</p><p><img src="https://i.loli.net/2020/12/20/wg8aB3AMUbcuTtS.png" alt=""></p><p>CCA模块的设计使得Non-local的计算量从$(H <em> W)^{2}$减少为$(H </em> W) *(H+W-1)$，计算效率相比于Non-local到15%，内存占用仅为1/11。下图就是该模块的具体实现，首先，输入特征图$\mathbf{H} \in \mathbb{R}^{C \times W \times H}$经过两个1x1卷积得到通道降维的特征图${\mathbf{Q}, \mathbf{K}} \in \mathbb{R}^{C^{\prime} \times W \times H}$。</p><p><img src="https://i.loli.net/2020/12/20/J2L1hXmABFO9Wjg.png" alt=""></p><p>现在，按照设计我们需要得到一个注意力图$\mathbf{A} \in \mathbb{R}^{(H+W-1) \times(W \times H)}$（每个位置，都有$H+W-1$个位置要计算相关性，原始的Non-local中，这里的特征图维度为$(H * W)^{2}$），所以设计了一个下式表示的亲和度操作来获取注意力图。在$Q$的空间维度上，每个位置$\mathbf{u}$都有一个特征向量$\mathrm{Q}_{\mathrm{u}} \in \mathbb{R}^{C^{\prime}}$，同时，在$\mathbf{K}$上选择和$\mathbf{u}$同行同列的位置的特征向量，它们组成集合$\Omega_{\mathrm{u}} \in \mathbb{R}^{(H+W-1) \times C^{\prime}}$，$\boldsymbol{\Omega}_{i, \mathbf{u}} \in \mathbb{R}^{C^{\prime}}$是$\Omega_{\mathrm{u}}$的第$i$个元素。</p><script type="math/tex; mode=display">d_{i, \mathbf{u}}=\mathbf{Q}_{\mathbf{u}} \boldsymbol{\Omega}_{i, \mathbf{u}}^{\top}</script><p>上式所示的亲和度操作中，$d_{i, \mathrm{u}} \in \mathbf{D}$表示$\mathrm{Q}_{\mathrm{u}}$和$\boldsymbol{\Omega}_{i, \mathbf{u}}, i=[1, \ldots, H+W-1]$之间的相关性程度，其中$\mathbf{D} \in \mathbb{R}^{(H+W-1) \times(W \times H)}$。此时，对$\mathbf{D}$沿着通道维度进行softmax就得到了注意力图$\mathbf{A}$。</p><p>接着，对$\mathbf{V}$进行类似的操作，在$\mathbf{V}$的每个位置$\mathbf{u}$都可以得到$\mathbf{V}_{\mathbf{u}} \in \mathbb{R}^{C}$和$\boldsymbol{\Phi}_{\mathbf{u}} \in \mathbb{R}^{(H+W-1) \times C}$，这个$\boldsymbol{\Phi}_{\mathbf{u}}$同样是$\mathbf{V}$中和$\mathbf{u}$同行同列的所有位置的特征向量集合。</p><p>至此，上下文信息通过下面的式子聚合，得到一个CCA模块的输出，重复堆叠两次即可。</p><script type="math/tex; mode=display">\mathbf{H}_{\mathbf{u}}^{\prime}=\sum_{i=0}^{H+W-1} \mathbf{A}_{i, \mathbf{u}} \boldsymbol{\Phi}_{\mathbf{i}, \mathbf{u}}+\mathbf{H}_{\mathbf{u}}</script><p>此外，作者还设计了三维的CCA模块，这里我就不多赘述了。总之，CCNet通过独有的十字交叉注意力对Non-local进行了改进，并获得了更好的效果，在当时，是很有突破的成果。</p><h3 id="ResNeSt"><a href="#ResNeSt" class="headerlink" title="ResNeSt"></a><strong>ResNeSt</strong></h3><p>ResNeSt，又叫Split-Attention Networks（分割注意力网络），是今年争议比较多的一篇文章，文章的质量其实还是不错的。这篇文章是基于ResNeXt的一个工作，在ResNeXt中，其对深度和宽度外增加了一个维度—-基数（cardinality），对同一个特征图采用不同的卷积核进行卷积，最后将结果融合。</p><p><img src="https://i.loli.net/2020/12/20/CoMX7FKWxEHDVaJ.png" alt=""></p><p>上图是SE模块、SK模块以及文章提出的ResNeSt的模块设计图，其将输入特征图切分为多路（共$k$路），每路的拓扑结构一致，又将每一路划分为$r$个组，特征图再次被切分，每组的拓扑结构也是一样的。</p><p>在每一路中，每个组会对自己切分得到的通道数为$c’/k/r$的特征图进行1x1卷积核3x3卷积，再将这一路的$r$个结果送入一个分割注意力模块（Split Attention，SA）中，每个路将自己注意力后的结果concat到一起，再经过1x1卷积恢复维度后和原始输入相加。</p><p>现在，这个Split Attention如何实现的呢，如下图所示，$r$个输入逐元素求和后通过全局平均池化得到$r$个$c$维度的向量，经过全连接层学习到通道注意力向量，再和各自的输入做乘法进行通道注意力，$r$个通道注意力的结果，最后相加。</p><p><img src="https://i.loli.net/2020/12/20/HUnzjN3yQvmOBA4.png" alt=""></p><p>整体来看，每一路的处理都是一个SK卷积，所以ResNeSt其实是对ResNeXt、SENet和SKNet的结构的融合，但是融合并用好本身也是挺不错的工作了。</p><h3 id="Triplet-Attention"><a href="#Triplet-Attention" class="headerlink" title="Triplet Attention"></a><strong>Triplet Attention</strong></h3><p>最后，聊聊最近的一个注意力成果，Triplet Attention。这篇文章的目标是研究如何在不涉及任何维数降低的情况下建立廉价但有效的通道注意力模型。Triplet Attention不像CBAM和SENet需要一定数量的可学习参数来建立通道间的依赖关系，它提出了一个几乎无参数的注意机制来建模通道注意力和空间注意力。</p><p><img src="https://i.loli.net/2020/12/20/XPpYw1E75eGDUuj.png" alt=""></p><p>如上图，顾名思义，Triplet Attention由3个平行的分支组成，其中两个负责捕获通道C和空间H或W之间的跨维交互。最后一个分支类似于CBAM，用于构建空间注意力。最终3个分支的输出通过求和平均进行聚合。</p><p>首先，为了解决此前注意力方法通道注意力和空间注意力分离的问题，论文提出跨维交互这个概念，下图是Triplet Attention的概念图，通过旋转输入张量实现不同维度之间的交互捕获。</p><p><img src="https://i.loli.net/2020/12/20/FEV5ohUAvuasSzR.png" alt=""></p><p>为了实现一个维度上关键信息的捕获，Z-pool通过取最大值和平均值并将其连接的方式，将任意维度特征调整为2维，这样既能保留丰富的表示又能降维减少计算量。<strong>这里之所以采用这个Z-pool降维而不是1x1卷积就是为了减少计算量。</strong></p><script type="math/tex; mode=display">Z \text { -pool }(\chi)=\left[\operatorname{MaxPool}_{0 d}(\chi), \text { AvgPool }_{0 d}(\chi)\right]</script><p>给定输入张量$\chi \in R^{C \times H \times W}$，它被分配给三个分支进行处理。</p><p>首先，看Triplet Attention的第一个分支，其目的是对$H$维度和$C$维度之间建立交互。输入张量$\chi$先沿着$H$轴进行旋转90度，得到$\hat{\chi}_{1} \in R^{W \times H \times C}$。这个$\hat{\chi}$先是通过Z-pool降维到$(2 \times H \times C)$然后经过卷积和BN层，得到$(1 \times H \times C)$的特征图再经过Sigmoid输出注意力图，这个注意力图和$\hat{\chi}$相乘，得到$H$和$C$的交互注意力结果。然后，再反转90度回去，得到和输入维度匹配的注意力图。</p><p><img src="https://i.loli.net/2020/12/20/cAfUlkRnIKqGzpw.png" alt=""></p><p>同样的，第二个分支类似，只是旋转方式的不同，因而得到的是$C$和$W$的交互注意力结果。<strong>其实，这个文章的创新之处就在这个旋转操作上，这个所谓的跨维度交互就是旋转后最后两个维度上的空间注意力而已。</strong></p><p><img src="https://i.loli.net/2020/12/20/PEOUoCKc2Srf4sD.png" alt=""></p><p>最后，第三个分支其实一模一样，只是没有旋转，那自然捕获的就是原本的$H$和$W$的空间注意力了。然后，简单的求和平均就得到了最终的特征图了。</p><p><img src="https://i.loli.net/2020/12/20/1aYmjIls83nMvpJ.png" alt=""></p><p>上面说的这些，也就是Triplet Attention的全部了。它抓住了张量中各个维度特征的重要性并使用了一种有效的注意力计算方法，不存在任何信息瓶颈。实验证明，Triplet Attention提高了ResNet和MobileNet等标准神经网络架构在图像分类和目标检测等任务性能，而只引入了最小的计算开销。是一个非常不错的即插即用的注意力模块。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文简单介绍了计算机视觉中几种最新的采用注意力机制的卷积神经网络，它们都是基于前人的成果进行优化，获得了相当亮眼的表现，值得借鉴。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视觉注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视觉注意力机制(中)</title>
      <link href="2020/12/20/attentions-medium/"/>
      <url>2020/12/20/attentions-medium/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在<a href="https://zhouchen.blog.csdn.net/article/details/111302952">上篇文章</a>中，我介绍了视觉注意力机制比较早期的作品，包括Non-local、SENet、BAM和CBAM，本篇文章主要介绍一些后来的成果，包括$A^2$-Nets、GSoP-Net、GCNet和ECA-Net，它们都是对之前的注意力模型进行了一些改进，获得了更好的效果。</p><p><img src="https://i.loli.net/2020/12/16/ynLluegbMsiINTj.png" alt=""></p><p>本系列包括的所有文章如下，分为上中下三篇，本文是中篇。</p><ol><li>NL(Non-local Neural Networks)</li><li>SENet(Squeeze-and-Excitation Networks)</li><li>CBAM(Convolutional Block Attention Module)</li><li>BAM(Bottleneck Attention Module)</li><li><strong>$\mathbf{A^2}$-Nets(Double Attention Networks)</strong></li><li><strong>GSoP-Net(Global Second-order Pooling Convolutional Networks)</strong></li><li><strong>GCNet(Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</strong></li><li><strong>ECA-Net(Efficient Channel Attention for Deep Convolutional Neural Networks)</strong></li><li>SKNet(Selective Kernel Networks)</li><li>CCNet(Criss-Cross Attention for Semantic Segmentation)</li><li>ResNeSt(ResNeSt: Split-Attention Networks)</li><li>Triplet Attention(Convolutional Triplet Attention Module)</li></ol><h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><p>我们首先还是来回顾一下卷积神经网络中常用的注意力，主要有两种，即<strong>空间注意力和通道注意力</strong>，当然也有融合两者的混合注意力，下图是画的示意图。</p><p><img src="https://i.loli.net/2021/01/17/9X3I8w6sahxoe1r.png" alt=""></p><p>我们知道，卷积神经网络输出的是维度为$C\times H \times W$的特征图，其中$C$指的是通道数，它等于作用与输入的卷积核数目，每个卷积核代表提取一种特征，所以每个通道代表一种特征构成的矩阵。$H \times W$这两个维度很好理解，这是一个平面，里面的每个值代表一个位置的信息，尽管经过下采样这个位置已经不是原始输入图像的像素位置了，但是依然是一种位置信息。如果，对每个通道的所有位置的值都乘上一个权重值，那么总共需要$C$个值，构成的就是一个$C$维向量，将这个$C$维向量作用于特征图的通道维度，这就叫<strong>通道注意力</strong>。同样的，如果我学习一个$H\times W$的权重矩阵，这个矩阵每一个元素作用于特征图上所有通道的对应位置元素进行乘法，不就相当于对空间位置加权了吗，这就叫做<strong>空间注意力</strong>。</p><p>上篇文章介绍的Non-local是一种空间注意力机制，而SENet是典型的通道注意力，BAM和CBAM则是混合注意力的代表。</p><h2 id="网络详解"><a href="#网络详解" class="headerlink" title="网络详解"></a>网络详解</h2><h3 id="A-2-Nets"><a href="#A-2-Nets" class="headerlink" title="$A^2$-Nets"></a><strong>$A^2$-Nets</strong></h3><p>$A^2$-Nets，也叫AA-Nets，指的就是Double Attention Networks，可以认为是Non-local的拓展工作，作为<strong>长程交互的信息交互捕获</strong>方法而言，相比于Non-local，论文中的方法精度更高、参数量更少。作者文中也直言，这篇文章基于了很多注意力的共工作，包括SENet、Non-local、Transformer。</p><p>论文的核心思想是首先将整个空间的关键特征收集到一个紧凑的集合中，然后自适应地将其分布到每个位置，这样后续的卷积层即使没有很大的接收域也可以感知整个空间的特征。第一次的注意力操作有选择地从整个空间中收集关键特征，而第二次的注意力操作采用另一种注意力机制，自适应地分配关键特征，这些特征有助于补充高级任务的每个时空位置。因为这两次注意力的存在，因此称为Double Attention Networks。</p><p>下面就来看看这篇文章的方法论，不过，在此提醒，这篇文章的数学要求很高，对注意力的建模也很抽象，需要多看几次论文才能理解，不过，结构上而言，这些花里胡哨的公式都是靠1x1卷积实现的。下图就是总体的pipeline设计，这张图分为左图、右上图和右下图来看。先看左图，这就是Double Attention的整体思路，一个输入图像（或者特征图）进来，先计算一堆Global Descriptors，然后每个位置会根据自己的特征计算对每个Global Descriptor的权重（称为Attention Vectors），从而是对自己特征的补充，如图中的红框中的球，所以它对球棒等周围物体的依赖很高，对球本身的依赖就很小。Global descriptors和Attention Vectors相乘就恢复输入的维度从而得到注意力结果。Global Descriptors和输出的计算细节就是右边两图，我们下面会具体数学分析。</p><p><img src="https://i.loli.net/2020/12/20/olTdiOg1P7yIQEX.png" alt=""></p><p>上面的整个过程，数学表达如下式，显然，这是个两步计算，其计算流图如下图。</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\mathbf{F}_{\text {distr }}\left(\mathbf{G}_{\text {gather }}(X), \mathbf{v}_{i}\right)</script><p><img src="https://i.loli.net/2020/12/20/1c2LN3BzGjifoYF.png" alt=""></p><p>上图有先后两个步骤，分别是Feature Gathering和Feature Distribution。先来看Feature Gathering，这里面用了一个双线性池化，这是个啥呢，如下图所示，由双线性CNN这篇文章提出来的，$A^2-Nets$只用到了最核心的双线性池化的思路，不同于平均池化和最大池化值计算一阶统计信息，双线性池化可以捕获更复杂的二阶统计特征，而方式就是对两个特征图$A=\left[\mathbf{a}_{1}, \cdots, \mathbf{a}_{ h w}\right] \in \mathbb{R}^{m \times  h w}$和$B=\left[\mathbf{b}_{1}, \cdots, \mathbf{b}_{ h w}\right] \in \mathbb{R}^{n \times h w}$计算外积，每个特征图有$h \times w$个位置的特征，每个特征是通道数$m$和$n$维度。下面这个公式其实就是对两个特征图的同样位置的两个向量进行矩乘然后求和，这里当然得到的是一个$m \times n$维的矩阵，将其记为$G=\left[\mathbf{g}_{1}, \cdots, \mathbf{g}_{n}\right] \in \mathbb{R}^{m \times n}$。 <strong>有人问，这个$A$和$B$是怎么来的，是通过1x1卷积对$X$变换得到的，其中$A$发生了降维，而$B$没有。</strong></p><script type="math/tex; mode=display">\mathbf{G}_{\text {bilinear }}(A, B)=A B^{\top}=\sum_{\forall i} \mathbf{a}_{i} \mathbf{b}_{i}^{\top}</script><p><img src="https://i.loli.net/2020/12/20/uwf7sS5aidrDY8b.png" alt=""></p><p>作者这里给了一个解释，如果将特征图$B$改写为$B=\left[\overline{\mathbf{b}}_{1} ; \cdots ; \overline{\mathbf{b}}_{n}\right]$，那么$\mathbf{G}$的计算式可以写成下面的形式，此时每个$b_i$都是一个$h \times w$的行向量，$A$又是$m\times hw$的，所以$g_i$等同于$A$乘上一个注意力向量$b_i$得到$m$维的向量。从这个角度来看，$G$实际上是一个图片上视觉基元的集合，每个基元$g_i$都通过$\overline{\mathbf{b}}_{i}^{\top}$加权的局部特征聚合得到。</p><script type="math/tex; mode=display">\mathbf{g}_{i}=A \overline{\mathbf{b}}_{i}^{\top}=\sum_{\forall j} \overline{\mathbf{b}}_{i j} \mathbf{a}_{j}</script><p>因此，自然诞生了一个新的注意力设计想法用来聚合信息，对$\overline{\mathbf{b}}_{i}^{\top}$应用softmax保证所有元素和为1（这就是上面提到的Attention Vectors），从而$G\in \mathbb{R}^{m \times n}$的计算如下式，$n$个$g$组成的就叫做Global Descriptors。</p><script type="math/tex; mode=display">\mathbf{g}_{i}=A \operatorname{softmax}\left(\overline{\mathbf{b}}_{i}\right)^{\top}</script><p><strong>这第一步Feature Gathering我就解释到这里，具体哪里代表哪一个张量我在之前的图上进行了标注。</strong> 下面，来看Feature Distribution这一步，将从整幅图得到的紧凑的Global Descriptors分发给各个位置，因此，后续处理使用小卷积核也能获得全局信息。受到SENet的启发，不过它是对每个位置分发同一个通道注意力值的，作者认为应该根据该位置的值的需求去有选择地分发视觉基元，因此对一个通道上地所有空间的$hw$个元素进行softmax然后和$G$相乘，就得到那个位置的注意力结果，显然$G$是$m\times n$的，而$v$是$n \times hw$维向量，他俩按照下式的计算结果为一个$m \times hw$维度的结果，再经过1x1卷积升维和原始的$X$相加都得到注意力后的结果。</p><script type="math/tex; mode=display">\mathbf{z}_{i}=\sum_{\forall j} \mathbf{v}_{i j} \mathbf{g}_{j}=\mathbf{G}_{\text {gather }}(X) \mathbf{v}_{i}, \text { where } \sum_{\forall j} \mathbf{v}_{i j}=1</script><p>至此，完成了Double Attention Block的构建，它分两步进行，总的计算式如下（$A$、$B$和$V$都是1x1卷积实现的），它也可以改写为下面第二个式子，这两个式子数学上等价，但是复杂度不同，而且后面的计算方法空间消耗很高，一个32帧28x28的视频，第二个式子需要超过2GB的内存，而第一个式子只需要约1MB，所以在一般的$(dhw)^2 &gt; nm$的情况下，建议采用式1。</p><script type="math/tex; mode=display">\begin{aligned}Z &=\mathbf{F}_{\text {distr }}\left(\mathbf{G}_{\text {gather }}(X), V\right) \\&=\mathbf{G}_{\text {gather }}(X) \operatorname{softmax}\left(\rho\left(X ; W_{\rho}\right)\right) \\&=\left[\phi\left(X ; W_{\phi}\right) \operatorname{softmax}\left(\theta\left(X ; W_{\theta}\right)\right)^{\top}\right] \operatorname{softmax}\left(\rho\left(X ; W_{\rho}\right)\right)\end{aligned}</script><script type="math/tex; mode=display">Z=\phi\left(X ; W_{\phi}\right)\left[\operatorname{softmax}\left(\theta\left(X ; W_{\theta}\right)\right)^{\top} \operatorname{softmax}\left(\rho\left(X ; W_{\rho}\right)\right)\right]</script><p>这篇文章算的上很会讲故事了，把注意力解释的非常抽象、层次很高，最后竟然全靠1x1卷积实现，这就是大繁至简吗（手动狗头）？Double Attention Block的优势应该是能用更少的层数达到与更多的层数带来的接近的大感受野的效果，这适用于轻量级网络。</p><h3 id="GSoP-Net"><a href="#GSoP-Net" class="headerlink" title="GSoP-Net"></a><strong>GSoP-Net</strong></h3><p>GSoP指的是Global Second-order Pooling，它是相对于GAP这种常用在网络末端的全局一阶池化的下采样方式，不过之前的GSoP都是放在网络末端，这篇论文将其放到了网络中间层用于注意力的学习，这也在网络的早期阶段就可以获得整体图像的二阶统计信息。按照这个思路设计的GSoP模块可以以微量的参数嵌入到现有网络中，构建的GSoP-Net如下图。它紧跟在GAP后面生成紧凑的全局表示时，称为GSoP-Net1（下图右侧），或者在网络中间层通过协方差矩阵表示依赖关系然后实现通道注意力或者空间注意力，这称为GSoP-Net2（下图左侧），本文介绍主要关注GSoP-Net2。</p><p><img src="https://i.loli.net/2020/12/20/VrlvxB2FWhqpscC.png" alt=""></p><p>GSoP这种池化策略，它已经被证明在在网络末端使用会带来较好的性能提升，因为二阶统计信息相比一阶线性卷积核获取到的信息更加丰富。由此，作者从SENet开始思考改进，SENet中采用GAP来获取全局位置表示，然后通过全连接或者非线性卷积层来捕获逐通道依赖。CBAM则不仅仅沿着channel维度进行GAP，它也在空间上进行注意力实现了一种类似自注意力的机制。不过，SENet和CBAM都采用一阶池化，GSoP想到将二阶池化用过来，这就诞生了这篇文章。</p><p><img src="https://i.loli.net/2020/12/20/g8rVIzNuRe2Zmaw.png" alt=""></p><p>上图就是GSoP模块的结构，它类似于SE模块，采用了压缩激励两个步骤。压缩操作是为了沿着输入张量的通道维度建模二阶统计信息。首先，输入的$h’\times w’ \times c’$的张量（其实就是特征图）首先通过1x1卷积降维到$h’\times w’ \times c$，然后通道之间两两之间计算相关性，得到$c \times c$的协方差矩阵，这个协方差矩阵意义鲜明，第$i$行元素表明第$i$个通道和其他通道的统计层面的依赖。由于二次运算涉及到改变数据的顺序，因此对协方差矩阵执行逐行归一化，保留固有的结构信息。SENet使用GAP只获得了每个通道的均值，限制了统计建模能力。</p><p>然后，激励模块，对上面的协方差特征图进行非线性逐行卷积得到$4c$的结构信息，再用卷积调整到输入的通道数$c’$维度，和输入进行逐通道相乘，完成通道注意力。</p><p><strong>如果，只是对SENet进行了这样的改进，其实创新度不高，因此作者进一步进行空间注意力的推广，用来捕获逐位置的依赖。</strong></p><p>空间GSoP模块和上面的通道GSoP思路类似，首先特征图降维到$h’ \times w’ \times c$，然后下采样到$h \times w \times c$，然后计算逐位置协方差矩阵，得到$hw \times hw$，它的含义和之前通道上类似。然后同样是两个非线性卷积，获得一个$h \times w$的注意力图，再被上采样到$h’ \times w’ \times c’$，再在空间位置上进行注意力乘法即可实现空间注意力。</p><p>具体如何将GSoP模块嵌入到网络中其实很简单，就不多赘述了。GSoP实现了一种二阶统计信息层面的通道和空间注意力，但其本质上其实和之前的一些自注意力结构类似，捕获了一种全位置的长程交互。</p><h3 id="GCNet"><a href="#GCNet" class="headerlink" title="GCNet"></a><strong>GCNet</strong></h3><p>GCNet全名Non-local Networks Meet Squeeze-Excitation Networks and Beyond，听名字也看得出来，是Non-loca和SENet的结合成果，它深入分析了Non-local和SENet的优缺点，结合其优势提出了GCNet。</p><p>为了捕获长程依赖，主要产生两类方法：一种是采用自注意力策略进行逐对建模；另一种是不依赖query的全局上下文建模。Non-local就是采用的自注意力策略来建模像素对之间的关系，它对每个位置学习不受位置依赖的注意力图，存在大量的资源浪费；SENet则采用全局上下文对不同通道加权来调整通道依赖，但是这种利用加权进行的特征融合无法充分利用全局上下文。</p><p>作者通过大量实验分析发现Non-local的全局上下文在不同位置几乎是相同的，这表明学习到了无位置依赖的全局上下文，其实这算是自注意力的通病了。下图是可视化不同位置的注意力图，几乎是相同的。</p><p><img src="https://i.loli.net/2020/12/20/EAjp9r6MeKu12zf.png" alt=""></p><p>既然如此，作者就想干脆全局共享一个注意力图，因此Non-local修改为如下结构。</p><p><img src="https://i.loli.net/2020/12/20/aUcgF19z5Mtvp2P.png" alt=""></p><script type="math/tex; mode=display">\mathbf{z}_{i}=\mathbf{x}_{i}+W_{v} \sum_{j=1}^{N_{p}} \frac{\exp \left(W_{k} \mathbf{x}_{j}\right)}{\sum_{m=1}^{N_{p}} \exp \left(W_{k} \mathbf{x}_{m}\right)} \mathbf{x}_{j}</script><p>简化版Non-local的第二项是不受位置依赖的，所有位置共享这一项。因此，作者直接将全局上下文建模为所有位置特征的加权平均值，然后聚集全局上下文特征到每个位置的特征上。它抽象为下面三个阶段：</p><ol><li>全局注意力池化，通过1x1卷积核softmax获取注意力权重，然后通过注意力池化捕获上下文特征；</li><li>特征转换，通过1x1卷积进行特征变换；</li><li>特征聚合，采用加法将全局上下文特征聚合到每个位置的特征上。</li></ol><p>上述步骤可以抽象为下式所述的全局上下文建模框架，里外三层计算对应上面三个步骤，SENet也有类似的三个步骤：压缩、激励和聚合，SE模块的优势就是计算量很少，非常轻量。</p><script type="math/tex; mode=display">\mathbf{z}_{i}=F\left(\mathbf{x}_{i}, \delta\left(\sum_{j=1}^{N_{p}} \alpha_{j} \mathbf{x}_{j}\right)\right)</script><p>为了对非常消耗算力的简化版Non-local进一步优化，对第二步的1x1卷积替换为SENet中先降维再升维的bottleneck transform模块，从而形成了下图的GC模块。</p><p><img src="https://i.loli.net/2020/12/20/Tu2p14jnwVHMhAO.png" alt=""></p><p>最后，做个简单总结，Non-local和SENet本质上还是实现了一种全局上下文建模，这对于感受野局限的卷积神经网络是有效的信息补充，GCNet实现了Non-local的全局上下文建模能力和SENet的轻量，获得了相当不错的效果。</p><h3 id="ECA-Net"><a href="#ECA-Net" class="headerlink" title="ECA-Net"></a><strong>ECA-Net</strong></h3><p>上面说了这么多种注意力方法，他们的出发点虽然不尽相同，但结果都是设计了更加复杂的注意力模块以获得更好的性能，但是即使精心设计，还是不可避免带来了不少的计算量，ECA-Net则为了克服性能和复杂度互相制约不得不做出权衡的悖论，提出了一种超轻量的注意力模块（Efficient Channel Attention，ECA），最终被收录于CVPR2020。下图是其和之前比较著名的注意力模块对比的结果图，其在精度和参数量上都实现了新的突破，同等层数，ECA-Net精度超越了之前的所有注意力模型且参数量最少。</p><p><img src="https://i.loli.net/2020/12/20/3FpWd6vuQNtkPXY.png" alt=""></p><p>这篇文章如果直接看最后设计出来的ECA模块，可能不会感觉多么惊艳，但是其对之前很多注意力方法做的理论分析是非常具有开创性的，这也是我本人非常喜欢这篇论文的原因。</p><p>首先，回顾一下SENet，可以参考<a href="https://zhouchen.blog.csdn.net/article/details/110826497">我之前的文章</a>，在SE模块中，GAP之后的特征经过降维后进行学习，这个操作其实破坏了通道之间的信息直接交互，因为经过投影降维，信息已经变化了。为了验证这个想法，作者设计了三个SE模块的变种并进行了实验对比，结果如下表，其中SE-Var1是不经过学习直接用GAP结果加权，SE-Var2是每个通道单独学习一个权重，这就已经超越了SE模块，SE-Var3是使用一层全连接层进行学习，相当于所有通道之间进行交互，也超过了之前的思路，计算式也相应改变。</p><p><img src="https://i.loli.net/2020/12/20/IfrGtYURECSbuWg.png" alt=""></p><p>那么，第一个思路来了，不需要对原始的GAP后的通道特征进行降维，那么使用单层全连接层学习是否有必要呢？我们知道，全连接其实是捕获的每个通道特征之间的全局交互，也就是每个通道特征都和其他通道的存在一个权重，这个跨通道交互前人已经证明是必要的，但是这种全局交互并没有必要，局部范围的通道交互实际上效果更好。如下图所示，每个通道直接捕获自己邻域内通道的交互信息即可，这个操作采用一维卷积就能实现，卷积核采用通道数自适应获取。</p><p><img src="https://i.loli.net/2020/12/20/aHNEhJXevkRsg6i.png" alt=""></p><p>下图就是ECA模块的PyTorch实现，是非常简单高效的。</p><p><img src="https://i.loli.net/2020/12/20/q8EyvPiXwJlmu7N.png" alt=""></p><p>总的来说，这篇文章的工作还是和充分的，研究了SENet以及后面的通道注意力的问题，提出了降维有害理论并设计了局部通道交互的ECA模块，实验证明，ECA-Net是切实有效的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文简单介绍了计算机视觉中几种比较新的采用注意力机制的卷积神经网络，它们都是基于前人的成果进行优化，获得了相当亮眼的表现，尤其是ECA-Net，后面我会介绍视觉注意力中最新的几个成果。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视觉注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>视觉注意力机制(上)</title>
      <link href="2020/12/16/attentions-first/"/>
      <url>2020/12/16/attentions-first/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，起源于自然语言处理（NLP）领域，后来在计算机视觉中广泛应用。注意力机制本质上与人类对事物的观察机制相似：一般而言，我们在观察事物的时候，首先会倾向于观察事物一些重要的局部信息（如下图所示，我们会首先将注意力集中在目标而不是背景的身上），然后再去关心一些其他部分的信息，最后组合起来形成整体印象。</p><p>注意力机制能够使得深度学习在提取目标的特征时更加具有针对性，使得相关任务的精度有所提升。注意力机制应用于深度学习通常是对输入每个部分赋予不同的权重，抽取出更加关键及重要的信息使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销，这也是注意力机制广泛使用的原因。</p><p><img src="https://i.loli.net/2020/12/16/ynLluegbMsiINTj.png" alt=""></p><p>在计算机视觉中，注意力机制主要和卷积神经网络进行结合，按照传统注意力的划分，大部分属于软注意力，实现手段常常是通过掩码（mask）来生成注意力结果。掩码的原理在于通过另一层新的权重，将输入特征图中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。说的更简单一些，网络除了原本的特征图学习之外，还要学会通过特征图提取权重分布，对原本的特征图不同通道或者空间位置加权。因此，按照加权的位置或者维度不同，将注意力分为<strong>空间域、通道域和混合域</strong>。</p><h2 id="典型方法"><a href="#典型方法" class="headerlink" title="典型方法"></a>典型方法</h2><p>卷积神经网络中常用的注意力有两种，即<strong>空间注意力和通道注意力</strong>，当然也有融合两者的混合注意力，下图是画的一个示意图。</p><p><img src="https://i.loli.net/2021/01/17/9X3I8w6sahxoe1r.png" alt=""></p><p>首先，我们知道，卷积神经网络输出的是维度为$C\times H \times W$的特征图，其中$C$指的是通道数，它等于作用与输入的卷积核数目，每个卷积核代表提取一种特征，所以每个通道代表一种特征构成的矩阵。$H \times W$这两个维度很好理解，这是一个平面，里面的每个值代表一个位置的信息，尽管经过下采样这个位置已经不是原始输入图像的像素位置了，但是依然是一种位置信息。如果，对每个通道的所有位置的值都乘上一个权重值，那么总共需要$C$个值，构成的就是一个$C$维向量，将这个$C$维向量作用于特征图的通道维度，这就叫<strong>通道注意力</strong>。同样的，如果我学习一个$H\times W$的权重矩阵，这个矩阵每一个元素作用于特征图上所有通道的对应位置元素进行乘法，不就相当于对空间位置加权了吗，这就叫做<strong>空间注意力</strong>。</p><p>下面我列举一些常见的使用了注意力机制的卷积神经网络，我在下面一节会详细介绍它们。</p><ol><li><strong>NL(Non-local Neural Networks)</strong></li><li><strong>SENet(Squeeze-and-Excitation Networks)</strong></li><li><strong>BAM(Bottleneck Attention Module)</strong></li><li><strong>CBAM(Convolutional Block Attention Module)</strong></li><li>$A^2$-Nets(Double Attention Networks)</li><li>GSoP-Net(Global Second-order Pooling Convolutional Networks)</li><li>GCNet(Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</li><li>ECA-Net(Efficient Channel Attention for Deep Convolutional Neural Networks)</li><li>SKNet(Selective Kernel Networks)</li><li>CCNet(Criss-Cross Attention for Semantic Segmentation)</li><li>ResNeSt(ResNeSt: Split-Attention Networks)</li><li>Triplet Attention(Convolutional Triplet Attention Module)</li></ol><h2 id="网络详解"><a href="#网络详解" class="headerlink" title="网络详解"></a>网络详解</h2><h3 id="NL"><a href="#NL" class="headerlink" title="NL"></a><strong>NL</strong></h3><p>Non-local Neural Networks 应该算是引入自注意力机制比较早期的工作，后来的语义分割里各种自注意力机制都可以认为是 Non-local 的特例，这篇文章作者中同样有熟悉的何恺明大神 😂。</p><p>首先聊聊 Non-local 的动机，我们知道，CV 和 NLP 任务都需要捕获长程依赖（远距离信息交互），卷积本身是一种局部算子，CNN 中一般通过堆叠多层卷积层获得更大感受野来捕获这种长程依赖的，这存在一些严重的问题：效率低；深层网络的设计比较困难；较远位置的消息传递，局部操作是很困难的。所以，收到非局部均值滤波的启发，作者设计了一个泛化、简单、可直接嵌入主流网络的 non-local 算子，它可以捕获时间（一维时序数据）、空间（图像）和时空（视频）的长程依赖。</p><p><img src="https://i.loli.net/2020/12/16/pGaDETk4FHqx3oK.png" alt=""></p><p>首先，Non-local 操作早在图像处理中已经存在，典型代表就是非局部均值滤波，到了深度学习时代，在计算机视觉中，这种通过关注特征图中所有位置并在嵌入空间中取其加权平均值来计算某位置处的响应的方法，就叫做<strong>自注意力</strong>。</p><p>然后来看看深度神经网络中的 non-local 操作如何定义，也就是下面这个式子，这是一个通式，其中$x$是输入，$y$是输出，$i$和$j$代表输入的某个位置，可以是序列、图像或者视频上的位置，不过因为我比较熟悉图像，所以后文的叙述都以图像为例。因此$x_i$是一个向量，维数和通道一样；$f$是一个计算任意两点的相似性函数，$g$是一个一元函数，用于信息变换；$\mathcal{C}$是归一化函数，保证变换前后整体信息不变。所以，<strong>下面的式子，其实就是为了计算某个位置的值，需要考虑当前这个位置的值和所有位置的值的关系，然后利用这种获得的类似 attention 的关系对所有位置加权求和得到当前位置的值。</strong></p><script type="math/tex; mode=display">\mathbf{y}_{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) g\left(\mathbf{x}_{j}\right)</script><p>那么，确定了这个通式，在图像上应用，只需要确定$f$、$g$和$\mathcal{C}$即可，首先，由于$g$的输入是一元的，可以简单将$g$设置为 1x1 卷积，代表线性嵌入，算式为$g\left(\mathbf{x}_{j}\right)=W_{g} \mathbf{x}_{j}$。关于$f$和$\mathcal{C}$需要配对使用，作用其实就是计算两个位置的相关性，可选的函数有很多，具体如下。</p><ul><li><strong>Gaussian</strong><br>高斯函数，两个位置矩阵乘法然后指数映射，放大差异。<script type="math/tex; mode=display">f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}</script><script type="math/tex; mode=display">\mathcal{C}(x)=\sum_{\forall j} f\left(\mathrm{x}_{i}, \mathrm{x}_{j}\right)</script></li><li><strong>Embedded Gaussian</strong><br>嵌入空间的高斯高斯形式，$\mathcal{C}(x)$同上。<script type="math/tex; mode=display">\theta\left(\mathbf{x}_{i}\right)=W_{\theta} \mathbf{x}_{i} \text { and } \phi\left(\mathbf{x}_{j}\right)=W_{\phi} \mathbf{x}_{j}</script><script type="math/tex; mode=display">f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)}</script>论文中这里还特别提了一下，如果将$\mathcal{C}(x)$考虑进去，对$\frac{1}{\mathcal{C}(\mathbf{x})} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)$而言，这其实是一个 softmax 计算，因此有$\mathbf{y}=\operatorname{softmax}\left(\mathbf{x}^{T} W_{\theta}^{T} W_{\phi} \mathbf{x}\right) g(\mathbf{x})$，这个其实就是 NLP 中常用的自注意力，因此这里说，自注意力是 Non-local 的特殊形式，Non-local 将自注意力拓展到了图像和视频等高维数据上。但是，softmax 这种注意力形式不是必要的，因此作者设计了下面的两个 non-local 操作。</li><li><strong>Dot product</strong><br>这里去掉了指数函数形式，$\mathcal{C}(x)$的形式也相应改变为$x$上的像素数目。<script type="math/tex; mode=display">f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)</script><script type="math/tex; mode=display">\mathcal{C}(\mathbf{x})=N</script></li><li><strong>Concatenation</strong><br>最后，是一种 concat 的形式，$[\cdot, \cdot]$表示 concat 操作，$\mathcal{C}(x)$同上。<script type="math/tex; mode=display">f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\operatorname{ReLU}\left(\mathbf{w}_{f}^{T}\left[\theta\left(\mathbf{x}_{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)</script></li></ul><p>有了上面定义的 non-local 算子，就可以定义<strong>non-local 模块</strong>了，其定义如下，其中的$+\mathbf{x}_{i}$表示残差连接，这种残差形式可以保证该模块可以嵌入预训练模型中，只要将$W_z$初始化为 0 即可。其实这中间的实现都是通过 1x1 卷积完成的，因此输出和输入可以控制为同等通道数。</p><script type="math/tex; mode=display">\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i}</script><p>上面是数学上的定义，具体来看，一个时空格式的 non-local 模块如下图，我们从二维图像的角度来看，可以忽略那个$T$，直接将其置为 1 即可。所以，输入是$(h,w,1024)$，经过两个权重变换$W_\theta$和$W_\phi$得到降维后的两个特征图，都为$(h, w, 512)$，它们两者 reshape 后变为$(h\times w, 512)$，之后再其中一个转置后矩阵乘法另一个，得到相似性矩阵$(h\times w, h \times w)$，然后再最后一个维度上进行 softmax 操作。上述这个操作得到一种自注意力图，表示每个像素与其他位置像素的关系。然后将原始输入通过变换$g$得到一个$(h\times w, 512)$的输入矩阵，它和刚刚的注意力图矩阵乘法，输出为$(h\times w, 512)$，这时，每个位置的输出值其实就是其他位置的加权求和的结果。最后，通过 1x1 卷积来升维恢复通道，保证输入输出同维。</p><p><img src="https://i.loli.net/2020/12/16/V5RKiDI7bPEJydG.png" alt=""></p><p>这篇文章有很不错的<a href="https://github.com/AlexHex7/Non-local_pytorch">第三方 Pytorch 实现</a>，想了解更多细节的可以去看看源码和论文，其实实现是很简单的。</p><p>最后，简单总结一下这篇文章。提出了 non-local 模块，这是一种自注意力的泛化表达形式，Transformer 的成功已经证明自注意力的长程交互信息捕获能力很强，对网络尤其是处理视频的网络如 I3D 是有参考意义的。当然，其本质还是空间层面的注意力，如果考虑通道注意力或许会获得更好的效果，而且 non-local 模块的矩阵计算开销其实不低，这也制约了 non-local 的广泛应用，略微有点遗憾。</p><h3 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a><strong>SENet</strong></h3><p>SENet 应该算是 Non-local 的同期成果，我在<a href="https://zhouchen.blog.csdn.net/article/details/110826497">之前的文章</a>中专门解读过，这里就大概的说一下。</p><p>卷积操作是卷积神经网络的核心，卷积可以理解为在一个局部感受野范围内将空间维度信息和特征维度信息进行聚合，聚合的方式是加和操作。然而想要提高卷积神经网络的性能其实是很难的，需要克服很多的难点。为了获得更加丰富的空间信息，很多工作被提出，如下图使用多尺度信息聚合的Inception。</p><p><img src="https://i.loli.net/2020/12/16/QXprjOhZCV3ldFc.png" alt=""></p><p>那么，自然会想到，能否在通道维度上进行特征融合呢？其实卷积操作默认是有隐式的通道信息融合的，它对所有通道的特征图进行融合得到输出特征图（这就默认每个通道的特征是同权的），这就是为什么一个32通道的输入特征图，要求输出64通道特征图，需要$32\times64$个卷积核（这个32也可以理解为卷积核的深度）。通道方面的注意力也不是没人进行尝试，一些轻量级网络使用分组卷积和深度可分离卷积对通道进行分组操作，但这本质上只是为了减少参数，并没有什么特征融合上的贡献。</p><p>所以，SENet出现了，它从从每个通道的特征图应该不同权的角度出发，更加关注通道之间的关系，让模型学习到不同通道的重要程度从而对其加权，即显式建模不同通道之间的关系。为此，设计了SE（Squeeze-and-Excitation）模块，如下图所示。SE模块的思路很简单，先通过全局平均池化获得每个通道的全局空间表示，再利用这个表示学习到每个通道的重要性权重，这些权重作用与原始特征图的各个通道，得到通道注意力后的特征图。由于轻量简洁，SE模块可以嵌入任何主流的卷积神经网络模型中，因为其可以保证输入输出同维。</p><p><img src="https://i.loli.net/2020/12/16/mBIL5fi3kG8197K.png" alt=""></p><h3 id="BAM"><a href="#BAM" class="headerlink" title="BAM"></a><strong>BAM</strong></h3><p>这篇文章和下面的CBAM是同一个团队的成果，非常类似，CBAM收录于ECCV2018，BAM收录于BMVC2018，两篇文章挂到Arxiv上的时间也就差了几分钟而已，这里先简单地说一下区别，BAM其实是通道注意力和空间注意力的并联，CBAM是两者的串联。</p><p>BAM，全名Bottleneck Attention Module，是注意力机制在卷积神经网络中的一次伟大尝试，它提出的BAM模块可以集成到任意的卷积神经网络中，通过channel和spatial两个分支得到注意力图，通道注意力关注语义信息回答what问题，空间注意力关注位置信息，回答where问题，因此结合起来是最好的选择。下图是BAM集成到一个卷积神经网络的示意图，显然，BAM存在于池化层之前，这也是bottleneck的由来，作者说这样的多个BAM模块构建的注意力图层类似人类的感知过程。</p><p><img src="https://i.loli.net/2020/12/16/tcZXoqDmR4MUuBK.png" alt=""></p><p><img src="https://i.loli.net/2020/12/16/WYcVdF36JHbTys8.png" alt=""></p><p>上图就是核心的BAM模块结构图，我们来一步步看它是如何实现通道空间混合注意力的。整体来看，对于输入特征图$\mathbf{F} \in \mathbb{R}^{C \times H \times W}$，BAM模块最终会得到一个注意力图$\mathbf{M}(\mathbf{F}) \in \mathbb{R}^{C \times H \times W}$，<strong>这里注意到，这是一个和输入同维的张量，此前，通道注意力学习到的是个$C$维向量，空间注意力学到的是个$H\times W$维的矩阵，BAM这种格式表明其混合了通道和空间的注意力信息。</strong> 调整后输出的特征图$\mathbf{F}^{\prime}=\mathbf{F}+\mathbf{F} \otimes \mathbf{M}(\mathbf{F})$，显然，这是一个张量点乘后进行加法的运算，加法是明显的残差结构，点乘发生在学到的注意力图和输入特征图之间，因此输出和输入同样shape。为了计算上高效，通道和空间注意力采用并行的分支结构获得。因此，整体计算上要先获得通道注意力图$\mathbf{M}_{\mathbf{c}}(\mathbf{F}) \in \mathbb{R}^{C}$和空间注意力图$\mathbf{M}_{\mathbf{s}}(\mathbf{F}) \in \mathbb{R}^{H \times W}$，上面的最终注意力图通过下式计算得到，其中$\sigma$表示Sigmoid激活函数，两个注意力图的加法需要broadcast，得到的就是$C\times H \times W$维度了。</p><script type="math/tex; mode=display">\mathbf{M}(\mathbf{F})=\sigma\left(\mathbf{M}_{\mathbf{c}}(\mathbf{F})+\mathbf{M}_{\mathbf{s}}(\mathbf{F})\right)</script><p>然后，我们再来看看具体的两个分支内发生了什么。首先，看通道注意力分支，首先，对输入特征图$\mathbf{F}$进行全集平均池化得到$\mathbf{F}_{\mathbf{c}} \in \mathbb{R}^{C \times 1 \times 1}$，这相当于在每个通道的空间上获得了全局信息。然后，这个向量送入全连接层进行学习，这里进行了一个先降维再升维的操作，所以学到的向量$\mathbf{M}_{\mathbf{c}}(\mathbf{F})$依然是$C \times 1 \times 1$维度的，这个就是通道注意力图。</p><p>接着，我们看看空间注意力分支。作者这里先用1x1卷积对输入特征图降维，然后使用两层膨胀卷积以更大的感受野获得更丰富的信息$\mathbf{F_{temp}} \in \mathbb{R}^{C / r \times H \times W}$，最后再用1x1卷积将特征图降维到通道数为1，得到空间注意力图$\mathbf{M}_{\mathbf{s}}(\mathbf{F}) \in \mathbb{R}^{H\times W}$。</p><p><strong>至此，我们理解了BAM模块的结构，它可以嵌入到主流网络中获得一些性能提升，不过后来并没有在各种任务中获得较好的表现，因此不是很广泛，但它的混合注意力思路是值得借鉴的。</strong></p><h3 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a><strong>CBAM</strong></h3><p>CBAM，全名Convolutional Block Attention Module，相对于BAM，在CV中受到了更多的关注，下图就是CBAM的整体结构图，不难发现，它和BAM区别就是通道注意力和空间注意力是串联进行的，实践证明，这样的效果更好一些。</p><p><img src="https://i.loli.net/2020/12/16/xvHuDgG4o2cbhWd.png" alt=""></p><p>我们先从上图整体上看看CBAM怎么进行注意力的，首先，输入特征图$\mathbf{F} \in \mathbb{R}^{C \times H \times W}$和通道注意力图$\mathbf{M}_{\mathbf{c}} \in \mathbb{R}^{C \times 1 \times 1}$逐通道相乘得到$\mathbf{F’}$，接着，$\mathbf{F’}$会和空间注意力图$\mathbf{M}_{\mathbf{s}} \in \mathbb{R}^{1 \times H \times W}$逐位置相乘得到$\mathbf{F’’}$，这就是CBAM的输出，它依然是$C \times H \times W$维度的。</p><p><img src="https://i.loli.net/2020/12/16/JMIqi3S6vuPg7LE.png" alt=""></p><p>上图就是两个注意力模块的具体实现，我们先看通道注意力，它很类似于SENet，先是利用全局池化获得每个通道的位置全局信息，不过这里采用全局平均池化和全局最大池化分别得到$\mathbf{F}_{\mathrm{avg}}^{\mathrm{c}}$和$\mathbf{F}_{\mathrm{max}}^{\mathrm{c}}$，均得到一个$C\times 1\times 1$维的向量，经过共同的全连接层的降维再升维学习两个通道注意力（降维的维度用一个缩放比例控制），加到一起，获得的注意力图仍然是$\mathbf{M}_{\mathbf{c}}(\mathbf{F}) \in \mathbb{R}^{C\times 1 \times 1}$。</p><p>再来看空间注意力，也是采用全局平均池化和全局最大池化，不过是沿着通道进行的，所以得到两个特征图$\mathbf{F}_{\text {avg }}^{\mathbf{s}} \in \mathbb{R}^{1 \times H \times W}$ 和 $\mathbf{F}_{\max }^{\mathbf{s}} \in \mathbb{R}^{1 \times H \times W}$，然后将它们concat一起后使用一个7x7的卷积进行处理，得到$\mathbf{M}_{\mathbf{s}}(\mathbf{F}) \in \mathbb{1}^{C\times H \times W}$。</p><p>将上述通道注意力图和空间注意力图按照下面的公式先后作用于输入特征图，就得到混合注意力的结果。<strong>至此，我们理解了CBAM模块的运行过程，其中的激活函数和BN等细节我没有提到，可以查看原论文，这里相比于BAM都采用了两个全局池化混合的方式，这在今天的网络中已经很常见了，属于捕获更加丰富信息的手段。</strong></p><script type="math/tex; mode=display">\begin{aligned}\mathbf{F}^{\prime} &=\mathbf{M}_{\mathbf{c}}(\mathbf{F}) \otimes \mathbf{F} \\\mathbf{F}^{\prime \prime} &=\mathbf{M}_{\mathbf{s}}\left(\mathbf{F}^{\prime}\right) \otimes \mathbf{F}^{\prime}\end{aligned}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文简单介绍了计算机视觉中几种比较早期的采用注意力机制的卷积神经网络，它们的一些设计理念今天还被活跃用于各类任务中，是很值得了解的。后面的文章会介绍一些相对比较新的成果，欢迎关注。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视觉注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SENet&amp;SKNet解读</title>
      <link href="2020/12/07/senet-sknet/"/>
      <url>2020/12/07/senet-sknet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>今年有很多基于 ResNet 的新成果诞生，包括由于代码实现错误等问题引起广泛关注却屠榜各个榜单的 ResNeSt，关于 ResNeSt 的好坏这里不多做评论，不过它基于的前人工作 SENet 和 SKNet 其实是很值得回顾的，本文就聊聊这两个卷积神经网络历史上标杆式的作品。</p><ul><li>SENet: <a href="http://arxiv.org/abs/1709.01507">Squeeze-and-Excitation Networks</a>（CVPR2018）</li><li>SKNet: <a href="http://arxiv.org/abs/1903.06586">Selective Kernel Networks</a>（CVPR2019）</li></ul><h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><p>SENet 获得了 ImageNet2017 大赛分类任务的冠军，这也是最后一届 ImageNet 比赛，论文同时获得了 CVPR2018 的 oral。而且，SENet 思路简单，实现方便，计算量小，模块化涉及，可以无缝嵌入主流的网络结构中，实践不断证明其可以使得网络获得更好的任务效果。</p><h3 id="动机和思路"><a href="#动机和思路" class="headerlink" title="动机和思路"></a><strong>动机和思路</strong></h3><p>我们知道，卷积操作是卷积神经网络的核心，卷积可以理解为在一个局部感受野范围内将空间维度信息和特征维度信息进行聚合，聚合的方式是加和（sum）。然后想要提高卷积神经网络的性能其实是很难的，需要克服很多的难点。为了获得更加丰富的空间信息，很多工作被提出，如使用多尺度信息聚合的 Inception（下面左图，图来自官方分享）、考虑空间上下文的 Inside-outside Network（下面右图）以及一些注意力机制。</p><p><img src="https://i.loli.net/2020/12/07/jbxznFHyrITCYlR.jpg" alt=""></p><p>那么，自然会想到，能否在通道（channel）维度上进行特征融合呢？首先，其实卷积操作默认是有隐式的通道信息融合的，它对所有通道的特征图进行融合得到输出特征图（这就默认每个通道的特征是同权的），这就是为什么一个 32 通道的输入特征图，要求输出 64 通道特征图，需要 32x64 个卷积核。这方面也不是没人进行尝试，一些轻量级网络使用分组卷积和深度可分离卷积对 channel 进行分组操作，而这本质上只是为了减少参数，并没有什么特征融合上的贡献。</p><p>SENet 则从每个通道的特征图应该不同权角度出发，更加关注 channel 之间的关系，让模型学习到不同 channel 的重要程度从而对其加权，即显式建模不同 channel 之间的关系。为此，设计了 SE（Squeeze-and-Excitation）模块，我这边译作压缩激励模块，不过后文还是采用 SE 进行阐述。</p><h3 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a><strong>结构设计</strong></h3><p>下图就是 SE 模块的结构图，对输入$X$进行一系列卷积变换$\mathbf{F}_{t r}$后得到$U$，维度也从$(C’,H’,W’)$变为$(C,H,W)$。这里假定$\mathbf{F}_{t r}$就是一个卷积操作，并且卷积核为$\mathbf{V}=\left[\mathbf{v}_{1}, \mathbf{v}_{2}, \ldots, \mathbf{v}_{C}\right]$，$\mathbf{v}_{c}$表示第$c$个卷积核的参数，那么输出$\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{C}\right]$，这种表示源于下式，这个不难理解，其中$*$表示卷积运算。</p><script type="math/tex; mode=display">\mathbf{u}_{c}=\mathbf{v}_{c} * \mathbf{X}=\sum_{s=1}^{C^{\prime}} \mathbf{v}_{c}^{s} * \mathbf{x}^{s}</script><p>上面的式子说明了一个什么问题呢？一个卷积核到输入 feature map 的每一个 channel 上进行了操作然后加和在一起，channel 特征和卷积核学到的空间特征混杂纠缠在了一起。这就是说，默认卷积操作建模的通道之间的关系是隐式和局部的。作者希望显示建模通道之间的依赖关系来增强卷积特征的学习。因此，论文希望提供全局信息捕获的途径并且<strong>重标定</strong>卷积结果，所以设计了两个操作<strong>压缩（squeeze）</strong>和<strong>激励（excitation）</strong>。</p><p><img src="https://i.loli.net/2020/12/07/OSTCu8DWXP5wyZm.png" alt=""></p><p>上图就是 SE 模块的结构，我们先来看看压缩操作。卷积是在一个局部感受野范围内操作，因此$U$很难获得足够的信息来捕获全局的通道之间的关系。所以这里通过压缩全局空间信息为一个通道特征的操作，论文中采用全局平均池化（GAP）来实现这个目的，具体的$\mathbf{z} \in \mathbb{R}^{C}$通过下式计算，通过在$H\times W$维度上压缩$U$得到。</p><script type="math/tex; mode=display">z_{c}=\mathbf{F}_{s q}\left(\mathbf{u}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_{c}(i, j)</script><p>激励操作需要利用压缩操作得到的全局描述来抓取 channel 之间的关系，所以激励操作必须满足两个特性：灵活，它要能捕获通道间的非线性关系；不互斥，我们希望多个通道都被增强或者多个通道被抑制而不是要得到 one-hot 那样的结果。所以采用下述的简单 gating 机制，其中$W_{1} \in R^{\frac{C}{r} \times C}, W_{2} \in R^{C \times \frac{C}{r}}$。</p><script type="math/tex; mode=display">\mathbf{s}=\mathbf{F}_{e x}(\mathbf{z}, \mathbf{W})=\sigma(g(\mathbf{z}, \mathbf{W}))=\sigma\left(\mathbf{W}_{2} \delta\left(\mathbf{W}_{1} \mathbf{z}\right)\right)</script><p>为了降低模型复杂度以及提升泛化能力，这里采用两个全连接层构成 bottleneck 的结构，其中第一个全连接层起到降维的作用，降维比例$r$是个超参数，然后采用 ReLU 激活，然后第二个全连接层用于恢复特征维度最后 Sigmoid 激活输出，显然输出$\mathbf{s}$是一个$C$维的向量，它表征了各个 channel 的重要性且值在$(0,1)$之间，将这个$\mathbf{s}$逐通道乘上输入$U$即可，这相当于对每个 channel 加权，这种操作类似什么呢？Attention。</p><script type="math/tex; mode=display">\widetilde{\mathbf{x}}_{c}=\mathbf{F}_{\text {scale}}\left(\mathbf{u}_{c}, s_{c}\right)=s_{c} \mathbf{u}_{c}</script><h3 id="结构应用"><a href="#结构应用" class="headerlink" title="结构应用"></a><strong>结构应用</strong></h3><p>SE 模块可以无缝嵌入到主流的网络结构中，以 Inception 和 ResNet 为例，改造前后的结构如下图，Inception 比较直白，右边的残差网络则将 SE 模块直接嵌入残差单元中。当然，其在 ResNetXt，Inception-ResNet，MobileNet 和 ShuffleNet 等结构基础上也可以嵌入。</p><p>而且，经过分析可以得知，SE 模块算力增加并不大，在 ResNet50 上，虽然增加了约 10%的参数量，但计算量（GFLOPS）却增加不到 1%。</p><p><img src="https://i.loli.net/2020/12/07/Esir4h87akFveRZ.png" alt=""></p><p>作者也在主流任务上测试了 SE 模块的适用性，基本上在各个任务上都有涨点，感兴趣的可以查看原论文，不过其能获得 ImageNet2017 的冠军已经说明了 SE 模块的强大。而且其实现也是非常简单的，基本的 SE 模块的 PyTorch 实现如下（参考<a href="https://github.com/moskomule/senet.pytorch">开源链接</a>）。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SELayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SELayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channel<span class="token punctuation">,</span> channel <span class="token operator">//</span> reduction<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channel <span class="token operator">//</span> reduction<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> y<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="SKNet"><a href="#SKNet" class="headerlink" title="SKNet"></a>SKNet</h2><p>SENet 设计了 SE 模块来提升模型对 channel 特征的敏感性，CVPR2019 的 SKNet 和 SENet 非常相似，它主要是为了提升模型对感受野的自适应能力，这种自适应能力类似 SENet 对各个通道做类似 attention，只不过是对不同尺度的卷积分支做了这种 attention。</p><h3 id="动机和思路-1"><a href="#动机和思路-1" class="headerlink" title="动机和思路"></a><strong>动机和思路</strong></h3><p>我们设计卷积神经网络的时候参考了动物的视觉机制，但是一个重要的思路并没有在设计卷积神经网络时被过多关注，那就是感受野的尺寸是根据刺激自动调节的，SKNet就从这个思路出发，自适应选择更重要的卷积核尺寸（这个实现其实就是对不同尺度的特征图分别加权，这个权重由网络学习得到，这就是我为什么说和SENet很类似的原因）。</p><p>首先，我们考虑如何获得不同尺寸的感受野信息呢，一个非常直接的想法就是使用不同size的卷积核，然后把他们融合起来就行了，这个思路诞生了下图的Inception网络，不过，Inception将所有分支的多尺度信息线性聚合到了一起，这也许是不合适的，因为不同的尺度的信息有着不同的重要程度，网络应该自己学到这种重要程度（看到这是不是觉得和SENet针对的不同通道信息应该有不同重要程度有类似之处）。</p><p><img src="https://i.loli.net/2020/12/07/hFZLVTjguEdtyqO.png" alt=""></p><p>所以，这篇论文，作者设计了一种非线性聚合不同尺度的方法来实现自适应感受野，引入的这种操作称为SK卷积（Selective Kernel Convolution），它包含三个操作：Split、Fuse和Select。Split操作很简单，通过不同的卷积核size产生不同尺度的特征图；Fuse操作则通过聚合不同尺度的信息产生全局的选择权重；最后的Select操作通过这个选择权重来聚合不同的特征图。SK卷积可以嵌入主流的卷积神经网络之中且只会带来很少的计算量，其在大规模的ImageNet和小规模的CIFAR-10上都超越了SOTA方法。</p><h3 id="结构设计-1"><a href="#结构设计-1" class="headerlink" title="结构设计"></a><strong>结构设计</strong></h3><p>在具体了解SKNet之前，必须知道不少的前置知识，比如多分支卷积神经网络（不同size的卷积核特征融合）、分组卷积及深度可分离卷积及膨胀卷积（减少运算量的高效卷积方式）、注意力机制（增强网络的重点关注能力），这些我这边就不多做解释了。</p><p><img src="https://i.loli.net/2020/12/07/gZNdSo9ImtO1HjE.png" alt=""><br>上图就是SK卷积的一个基础实现，为了方便描述，作者只采用了两个分支，事实上可以按需增加分支，原理是一样的。可以看到，从左往右分别是三个part：Split、Fuse和Select，下面我就一步步来解释这三个操作是如何获得自适应感受野信息的，解释是完全对照上面这个图来的。</p><p><strong>Split</strong>：对给定的特征图$\mathbf{X} \in \mathbb{R}^{H^{\prime} \times W^{\prime} \times C^{\prime}}$，对其采用两种卷积变换$\widetilde{\mathcal{F}}: \mathbf{X} \rightarrow \tilde{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$和$\mathbf{X} \rightarrow \widehat{\mathbf{U}} \in \mathbb{R}^{H \times W \times C}$，它们只有卷积核size不同（这里以3和5为例），其余配置一致（卷积采用深度可分离卷积，5x5卷积采用3x3卷进进行膨胀）。这一步，通过两个变换构建了两个感受野的分支，形成了两个特征图$\tilde{\mathbf{U}}$和$\widehat{\mathbf{U}}$，它们的维度都是$H\times W \times C$。</p><p><strong>Fuse</strong>：这一步也就是自适应感受野的核心，这里采用最简单的gates机制控制进入下一层的多尺度信息流。因此，这个gates需要集成来自所有分支的信息，还要有权重的集成。首先，通过逐元素相加获得特征图$\mathbf{U}$（$\mathbf{U}=\tilde{\mathbf{U}}+\widehat{\mathbf{U}}$），然后采用SENet类似的思路，通过GAP生成逐通道的统计信息$\mathbf{s} \in \mathbb{R}^{C}$，计算式如下。</p><script type="math/tex; mode=display">s_{c}=\mathcal{F}_{g p}\left(\mathbf{U}_{c}\right)=\frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{U}_{c}(i, j)</script><p>接着，为了更紧凑的表示，通过一个全连接层对$\mathbf{s}$进行降维，获得$\mathbf{z}=\mathcal{F}_{f c}(\mathbf{s})=\delta(\mathcal{B}(\mathbf{W} \mathbf{s}))$，这里先是和$\mathbf{W} \in \mathbb{R}^{d \times C}$相乘然后经过BN和ReLU，$d$作为一个超参使用下降比$r$来控制，不过$d$同时通过下面的式子约束下界（$L$设置为32）。接着，又一个全连接用于升维度，得到分支数个$C$维向量，论文中这里就是$a$和$b$，然后按照通道维度进行soft attention，也就是说$a_{c}+b_{c}=1$，这样可以反映不同尺度的特征的重要性，然后用$a$和$b$采用类似SE的方式对原始特征图$\tilde{\mathbf{U}}$和$\widehat{\mathbf{U}}$进行逐通道相乘加权，得到有通道区分度的特征图，再相加到一起得到输出特征图$\mathbf{V}$。<strong>这个特征图，就是自适应不同感受野获得的特征图。</strong></p><script type="math/tex; mode=display">d=\max (C / r, L)</script><h3 id="结构应用-1"><a href="#结构应用-1" class="headerlink" title="结构应用"></a><strong>结构应用</strong></h3><p>将SK卷积应用到ResNeXt50中，得到SKNet50，具体配置如下图，在主流任务上都有突破。SK卷积也适用于轻量级网络，因为其不会带来多少算力增加。</p><p><img src="https://i.loli.net/2020/12/07/r9qWGCHwPD5Uueb.png" alt=""></p><p>SK卷积的PyTorch实现如下（参考<a href="https://github.com/ResearchingDexter/SKNet_pytorch">开源链接</a>）。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SKConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span>stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>M<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>r<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>L<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SKConv<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        d<span class="token operator">=</span><span class="token builtin">max</span><span class="token punctuation">(</span>in_channels<span class="token operator">//</span>r<span class="token punctuation">,</span>L<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>M<span class="token operator">=</span>M        self<span class="token punctuation">.</span>out_channels<span class="token operator">=</span>out_channels        self<span class="token punctuation">.</span>conv<span class="token operator">=</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>M<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>out_channels<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>stride<span class="token punctuation">,</span>padding<span class="token operator">=</span><span class="token number">1</span><span class="token operator">+</span>i<span class="token punctuation">,</span>dilation<span class="token operator">=</span><span class="token number">1</span><span class="token operator">+</span>i<span class="token punctuation">,</span>groups<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                           nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>                                           nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>global_pool<span class="token operator">=</span>nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span>d<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                               nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token punctuation">,</span>                               nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2<span class="token operator">=</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>d<span class="token punctuation">,</span>out_channels<span class="token operator">*</span>M<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>softmax<span class="token operator">=</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        batch_size<span class="token operator">=</span><span class="token builtin">input</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>        output<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token comment"># the part of split</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span>conv <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">)</span><span class="token punctuation">:</span>            output<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conv<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># the part of fuse</span>        U<span class="token operator">=</span><span class="token builtin">reduce</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>x<span class="token operator">+</span>y<span class="token punctuation">,</span>output<span class="token punctuation">)</span>        s<span class="token operator">=</span>self<span class="token punctuation">.</span>global_pool<span class="token punctuation">(</span>U<span class="token punctuation">)</span>        z<span class="token operator">=</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>s<span class="token punctuation">)</span>        a_b<span class="token operator">=</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        a_b<span class="token operator">=</span>a_b<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>self<span class="token punctuation">.</span>M<span class="token punctuation">,</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        a_b<span class="token operator">=</span>self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>a_b<span class="token punctuation">)</span>        <span class="token comment"># the part of select</span>        a_b<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>a_b<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>M<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#split to a and b</span>        a_b<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span>self<span class="token punctuation">.</span>out_channels<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>a_b<span class="token punctuation">)</span><span class="token punctuation">)</span>        V<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>x<span class="token operator">*</span>y<span class="token punctuation">,</span>output<span class="token punctuation">,</span>a_b<span class="token punctuation">)</span><span class="token punctuation">)</span>        V<span class="token operator">=</span><span class="token builtin">reduce</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span>y<span class="token punctuation">:</span>x<span class="token operator">+</span>y<span class="token punctuation">,</span>V<span class="token punctuation">)</span>        <span class="token keyword">return</span> V<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SENet和SKNet分别从通道信息和感受野自适应角度出发，设计了一个新的网络结构，获得了比较有突破的成果，SKNet是SENet基础上的工作，还集成了近几年卷积神经网络的一些主流技巧，可以说集众家之长，也可以说是人工设计卷积神经网络的集大成者了，SKNet后来的很多效果更好的卷积神经网络或多或少带有NAS技术的影子，不过，自动搜索也是不可阻挡的未来趋势。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SENet </tag>
            
            <tag> SKNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TPAGT解读</title>
      <link href="2020/12/04/tpagt/"/>
      <url>2020/12/04/tpagt/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>浙江大学和达摩院前不久提出的一个 MOT 新方法，目前在 MOT Challenge 常用的几个数据集上名列前茅。论文标题 Tracklets Predicting Based Adaptive Graph Tracking 其实已经表明本文最大的两个创新点，基于轨迹预测的特征提取以及基于自适应图网络的特征聚合。大多数现存的多目标跟踪方法将当前帧的检测结果链接到历史轨迹段都是采用基于特征余弦距离和目标边界框 IOU 的线性组合作为度量的，这其实有两个问题：<strong>一是两个不同帧（当前帧和上一帧）上同一个目标提取到的特征往往会出现不一致的问题；二是特征提取只考虑外观而不考虑位置关系、轨迹段信息是不合理的。</strong></p><p>因此，论文提出了一种新的高精度端到端多目标跟踪框架 TPAGT（上一个版本叫 FGAGT，感觉 TPAGT 更加贴合论文的工作），该方法解决了上述的两个问题，在多个数据集上实现了新的 SOTA。</p><ul><li>论文标题<br><br>Tracklets Predicting Based Adaptive Graph Tracking</li><li>论文地址<br><br><a href="http://arxiv.org/abs/2010.09015">http://arxiv.org/abs/2010.09015</a></li><li>论文源码<br><br>暂未开源</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>首先说明的是，TPAGT 按照一般 MOT 的方法划分是一个<strong>二阶段框架</strong>，也就是先完成检测，再按照检测结果到相应的位置提取目标特征，最后利用关联算法得到结果，关联一般采用匈牙利算法。单阶段方法融合了检测和特征提取，是为了速度做出的精度妥协，所以精度相比二阶段有些低。所以，作为一个二阶段方法，TPAGT 的精度应该有所创新，但是相应的速度比较慢，具体推理速度，论文没有提及，只能等源码开放后测试了。</p><p>先来说一说 MOT 现有方法没解决的几个问题。</p><ol><li>特征不一致问题<br><br>这个问题怎么来的呢，其实是因为轨迹段（tracklet）上目标的特征都是来自于之前帧，而不是当前帧（这很容易理解，当前帧只有当前帧的检测结果确定目标位置来提取特征嘛），但是呢，其实在移动过程中，目标的姿态、光强度、视角都可能发生变化，这导致来自不同图像的同一目标的特征即使检测准确也会不太一致，这种不一致对数据关联来说负面影响比较大。</li><li>特征融合问题<br><br>事实上，从 DeepSORT 开始，特征提取器主要关注的就是外观信息，因为这对忽略了运动建模的一些 MOT 方法至关重要，因此特征提取分支也成为 ReID 分支，主要就是因为重识别模型关注的就是外观信息。但是，目标之间的位置关系、tracklet 的历史信息对 MOT 任务也是很重要的。</li><li>样本不平衡问题<br><br>一个 tracklet 只能匹配一个检测框，那这个 tracklet 就是个连续的正例，没有匹配上的 tracklet 就是连续的负例。显然，正例数量是远远少于负例的，而且由于少量的新目标的产生和旧目标的消失，进一步加剧了不同类型的样本的不均衡问题。</li></ol><p>上述的问题 TPAGT 都逐一解决了，其中最主要的一个问题就是 traklets 中的特征和当前帧是不一致的，那么如何解决呢，到当前帧上<strong>重提取</strong>特征就行，但是显然不能直接把上一帧的 bbox（边界框，包含目标的位置区域等信息）用于当前帧，因为目标在图像上不可能静止，使用上一时刻的位置很不合理，所以需要对上一帧进行运动估计得到目标在当前帧预测的 bbox 位置然后提取特征。然后是特征融合的问题，考虑到目标之间的联系近似一个图表示，作者采用了<strong>GNN</strong>（图神经网络）来进行信息的聚合，为了更好获取全局时空信息，GNN 的边权自适应学习。最后，样本不平衡的问题采用了<strong>Balanced MSE Loss</strong>，这是一个加权 MSE，属于常用思路。</p><h2 id="框架设计"><a href="#框架设计" class="headerlink" title="框架设计"></a>框架设计</h2><h3 id="Tracklets-predicting-based-feature-re-extracting"><a href="#Tracklets-predicting-based-feature-re-extracting" class="headerlink" title="Tracklets predicting based feature re-extracting"></a><strong>Tracklets predicting based feature re-extracting</strong></h3><p><img src="https://i.loli.net/2020/12/04/tfmMKJpOq5BgcnR.png" alt=""></p><p>上面这个图就是整体框架的设计，我先大体介绍一下网络的 pipeline。首先，网络的输入有当前帧图像、当前帧检测结果、历史帧检测结果；接着，图像被送入 backbone 中获得特征图（这里 backbone 最终采用 ResNet101+FPN 效果最好），然后将 bbox（这里当前帧用的是检测的 bbox，上一帧用的光流预测的 bbox）映射到特征图上通过 RoI Align 获得 region 外观特征继而送入全连接（这个操作类似 Faster R-CNN 的 proposal 提取特征，不理解的可以查阅我的<a href="https://zhouchen.blog.csdn.net/article/details/110404238">博客</a>），然后结合当前帧的位置信息、历史帧信息，让图网络自适应学习进行特征融合从而计算相似度，有了相似度矩阵匈牙利就能计算匹配结果了。</p><p><strong>上面的叙述有个容易误解的地方，它将过去一帧预测的 bbox 和历史帧的非预测的 bbox 都在当前特征图上提取了特征，事实上，不是的，一来实际上，$t-2$帧的特征在处理$t-1$帧的时候已经重提取过了，在当前帧上用当时的 bbox 提取肯定存在严重的不对齐问题；二来，这样会大大加大网络计算的复杂性，完全没有必要。论文这个图画的稍微有些让人误解，等开源后可以再细细研究。</strong></p><p>我们知道，此前的 MOT 方法对运动的建模主要采用卡尔曼滤波为代表的状态估计方法、光流法和位移预测法，这篇论文使用稀疏光流法预测 bbox 的中心点运动，由于目标的运动有时候是高速的，为了应对这种运动模式，必须采用合适的光流方法，文章采用金字塔光流，该方法鲁棒性很强，具体想了解的可以参考<a href="https://blog.csdn.net/gh_home/article/details/51502933">这篇博客</a>，下图是金字塔光流预测的目标当前帧位置（b 图），c 图是 GT 的框，可以看到，预测还是很准的。</p><p><img src="https://i.loli.net/2020/12/04/cg1P6rNB49bGSyH.png" alt=""></p><h3 id="Adapted-Graph-Neural-Network"><a href="#Adapted-Graph-Neural-Network" class="headerlink" title="Adapted Graph Neural Network"></a><strong>Adapted Graph Neural Network</strong></h3><p><img src="https://i.loli.net/2020/12/04/NtzZBPnAXgE6l7U.png" alt=""></p><p>下面聊一聊这个自适应图神经网络。将 tracklets 和 detections 作为二分图处理不是什么新鲜的事情，但是用来聚合特征 TPAGT 应该是为数不多的工作，<strong>要知道此前我们聚合运动和外观特征只是人工设计的组合，作者这种借助图网络自适应聚合特征是很超前的思路。</strong> 每个检测目标和每个 tracklet 都是节点，如上图所示，detection 之间没有联系，tracklet 之间也没有联系，但是每个 tracklet 和每个 detection 之间都有连接。图网络的学习目的就是每个节点的状态嵌入$\mathbf{h}_{v}$，或者说聚合其他信息后的特征向量。最终，这个$\mathbf{h}_{v}$包含了邻居节点的信息。</p><p>需要学习的状态嵌入通过下面的公式更新，第一行表示 detections 的节点更新，第二行表示 tracklets 的节点更新，共有$N$个 detection 和$M$个 tracklet。下面讲解第一行的几个符号含义，第二行类似。$f$表示神经网络运算，可以理解为网络拟合函数；$h_{t, c}^{j}$表示第$c$层第$i$个 detection 的状态嵌入。在一开始，$c=0, h_{d, 0}^{i}=f_{d}^{i}, h_{t, 0}^{i}=f_{t}^{j}$，$e_{d, c}^{i, j}$则表示第$i$个检测和第$j$个 tracklet 在第$c$层的图上的边权。本文作者只使用添加自适应的单层 GNN，所以下面具体阐述单层学习的情况。</p><script type="math/tex; mode=display">\begin{aligned}h_{d, c+1}^{i} &=f\left(h_{d, c}^{i},\left\{h_{t, c}^{j}, e_{d, c}^{i, j}\right\}_{j=1}^{N}\right), i=1,2, \cdots, M \\h_{t, c+1}^{j} &=f\left(h_{t, c}^{j},\left\{h_{d, c}^{i}, e_{t, c}^{j, i}\right\}_{i=1}^{M}\right) j=1,2, \cdots, N\end{aligned}</script><p>首先，边权的初始化不采用随机初始化，而是采用节点的特征和位置先验信息，具体如下，主要是计算每个节点特征向量之间的归一化距离相似度。具体图信息聚合步骤如下。<br></p><ol><li>计算初始相似度<script type="math/tex; mode=display">\begin{array}{c}s_{i, j}=\frac{1}{\left\|f_{d}^{i}-f_{t}^{j}\right\|_{2}+1 \times 10^{-16}} \\s_{i, j}=\frac{s_{i, j}}{\sqrt{s_{i, 1}^{2}+s_{i, 2}^{2}+\cdots s_{i, j}^{2}+\cdots+s_{i, N}^{2}}}\\\mathbf{S}_{\mathrm{ft}}=\left[s_{i, j}\right]_{M \times N}, i=1, \cdots M, j=1, \cdots N\end{array}</script></li><li>通过 IOU 和上面的初始相似度组成边权（$w$可学习，表示位置和外观信息的相对重要性）</li></ol><script type="math/tex; mode=display">\mathrm{E}=w \times \mathrm{IOU}+(1-w) \times \mathrm{S}_{\mathrm{ft}}</script><ol><li>根据上述的自适应权重聚合节点特征（$\odot$表示点积）</li></ol><script type="math/tex; mode=display">\mathbf{F}_{\mathrm{t}}^{\mathrm{ag}}=\mathrm{EF}_{t}=\mathrm{E}\left[f_{t}^{1}, f_{t}^{2}, \cdots, f_{t}^{N}\right]^{T}</script><script type="math/tex; mode=display">\mathbf{H}_{\mathrm{d}}=\sigma\left(\mathbf{F}_{d} W_{1}+\operatorname{Sigmoid}\left(\mathbf{F}_{d} W_{a}\right) \odot \mathbf{F}_{\mathrm{t}}^{\mathbf{a g}} W_{2}\right)</script><script type="math/tex; mode=display">\mathbf{H}_{\mathrm{t}}=\sigma\left(\mathbf{F}_{t} W_{1}+\operatorname{Sigmoid}\left(\mathbf{F}_{t} W_{a}\right) \odot \mathbf{F}_{\mathrm{d}}^{\mathbf{a g}} W_{2}\right)</script><p>现有的图跟踪方法需要额外的全连接层降维特征向量，然后通过欧式距离计算相似度。TPAGT 的方法只要标准化来自单隐层图网络的特征，然后矩乘它们即可得到相似度决战，如下式。最终得到的相似度矩阵值介于 0 和 1 之间，越大代表两个目标越相似。学习的目的是使得同一个目标的特征向量尽量接近，不同目标的特征向量尽量垂直，这等价于三元组损失，但是更加简单。</p><p>$h_{d}^{i}=\frac{h_{d}^{i}}{\left|h_{d}^{i}\right|_{2}}, h_{t}^{j}=\frac{h_{t}^{j}}{\left|h_{t}^{j}\right|_{2}}, \mathbf{S}_{\mathrm{out}=\mathbf{H}_{\mathrm{d}} \mathbf{H}_{\mathbf{t}}^{\mathrm{T}}}$</p><h3 id="Blanced-MSE-Loss"><a href="#Blanced-MSE-Loss" class="headerlink" title="Blanced MSE Loss"></a><strong>Blanced MSE Loss</strong></h3><p>得到最终的相似度矩阵就可以进行监督训练了，不过 GT 的标签为相同目标为 1，不同的目标为 0，下图是作者做的可视化，每行代表一个 detection，每列代表一个 tracklet，绿行表示 detection 没有匹配上任何 tracklet，所以是新目标；相对的，红列表示消失的目标。1 表示正例，0 表示负例，显然正负例严重不均衡，所以这里对 MSE 按照目标类型进行了加权（超参），如下式。</p><p><img src="https://i.loli.net/2020/12/04/NGSUCpjo8mQa7xl.png" alt=""></p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L} &=\alpha E_{c 0}+\beta E_{c 1}+\gamma E_{n e}+\delta E_{d}+\varepsilon E_{w} \\&=\sum_{i=1}^{M} \sum_{j=1}^{N}\left[\begin{array}{c}\alpha\left(\hat{S}_{i, j}-S_{i, j}\right)^{2} \cdot \mathbb{I}_{\text {continue }} \cdot \mathbb{I}_{S_{i, j}=0}+\beta\left(\hat{S}_{i, j}-S_{i, j}\right)^{2} \cdot \mathbb{I}_{\text {continue }} \cdot \mathbb{I}_{S_{i, j}=1} \\+\gamma\left(\hat{S}_{i, j}-S_{i, j}\right)^{2} \cdot \mathbb{I}_{n e w}+\delta\left(\hat{S}_{i, j}-S_{i, j}\right)^{2} \cdot \mathbb{I}_{\text {disap }}+\varepsilon\|W\|_{2}^{2}\end{array}\right]\end{aligned}</script><h2 id="推理设计"><a href="#推理设计" class="headerlink" title="推理设计"></a>推理设计</h2><p>推理时，我们会得到相似度矩阵，那么如何利用这个矩阵呢？假设有$N$个 detection 和$M$个 tracklet，矩阵就是$M\times N$的，此时在后面补充一个$M\times M$的增广矩阵，矩阵中每个值都是一个阈值，如下图，匈牙利算法就成了带筛选的匹配方法，下图由于第 3 行和第 8 行没有高于阈值（0.2）的相似度，所以成为了新目标。</p><p><img src="https://i.loli.net/2020/12/04/ZQHhlGDUcg2o9XC.png" alt=""></p><h2 id="实验及分析"><a href="#实验及分析" class="headerlink" title="实验及分析"></a>实验及分析</h2><p>检测部分采用 FairMOT 的检测结果，也就是采用 CenterNet 作为检测器。特征提取部分，文章使用 ResNet101-FPN 作为 backbone，在 COCO 上预训练过，然后在 MOT 数据集上 fine tune 30 轮。其他训练细节可以自行查阅论文，我这里就不多说了，在 Public 和 private 两个赛道进行了测试，结果分别如下，超越了之前的 SOTA 方法如 FairMOT 等，精度突破很大，速度比较慢。</p><p><img src="https://i.loli.net/2020/12/04/cE86gYS5t3oKBDj.png" alt=""></p><p><img src="https://i.loli.net/2020/12/04/QbR6ngGePwxSYWh.png" alt=""></p><p>此外，作者还进行了丰富的消融实验，证明了 TPAGT 的鲁棒性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>开创性地提出了特征重提取策略，并引入 AGNN 进行特征融合，从而构建了 TPAGT 框架，这是一个端到端的学习框架，可以直接输出相似度矩阵。在 MOT Challenge 两个赛道都获得了 SOTA 表现。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Shan C, Wei C, Deng B, et al. Tracklets Predicting Based Adaptive Graph Tracking[J]. arXiv:2010.09015 [cs], 2020.</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TPAGT解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R-CNN系列解读</title>
      <link href="2020/11/30/r-cnns/"/>
      <url>2020/11/30/r-cnns/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>最近香港大学和加州伯克利开放了一篇新的基于 R-CNN 的工作 Sparse R-CNN，这是一个类似于 DETR 的稀疏端到端目标检测框架，使用少量可学习的 proposal 即可达到 SOTA 性能。这为稀疏端到端目标检测开辟了一条新的路，也将更多研究者的目标吸引了过来。本文从 R-CNN 开始，逐步讲解 R-CNN 系列目标检测算法的发展优化之路。，它们都是沿着 region proposal 这个思路的，只是处理方式大不相同。下面的图是我非常喜欢的目标检测综述《Object detection in 20 years: A survey》中归纳的目标检测里程碑式作品，可以看到，R-CNN 系的三个算法是目标检测发展史上避不开的话题。</p><ul><li><a href="https://arxiv.org/abs/1311.2524">R-CNN</a>（CVPR2014）</li><li><a href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a> （ICCV2015）</li><li><a href="https://arxiv.org/abs/1506.01497">Faster R-CNN</a>（NIPS2015）</li><li><a href="http://arxiv.org/abs/2011.12450">Sparse R-CNN</a>（暂未收录）</li></ul><p><img src="https://i.loli.net/2020/11/30/I6oZrcKxTzlfQ8q.png" alt=""></p><h2 id="目标检测思路"><a href="#目标检测思路" class="headerlink" title="目标检测思路"></a>目标检测思路</h2><p>在聊具体的算法之前，我们首先要知道目标检测永不过时的核心思路：<strong>目标定位</strong>+<strong>目标分类</strong>，这是任务本身决定的，后来所谓的 two-stage 方法和 one-stage 方法其实都还是这个思路，只是处理的技巧不同罢了。</p><p>我们知道，<strong>目标分类</strong>这个任务已经基本上被 CNN 所解决，所以只要在 pipeline 中引入卷积分类模型就能确保不错的分类精度。留给目标检测的核心问题其实就是<strong>目标定位</strong>。我们当然会想到很直接很粗暴的想法：遍历图片中所有可能的位置，搜索不同大小、宽高比的所有区域，逐个检测其中是否存在某个目标，以概率较大的结果作为输出。这个就是传统的滑窗方法，这个方法显然是一种密集采样的方式，类似后来的 anchor 策略，R-CNN 系列则采用了一种候选框提名的方式减少滑窗的复杂性形成了 Dense-to-Sparse 方法，Sparse R-CNN 则完全采用 Sparse 策略，获得了低维的 proposal 输入。</p><p><img src="https://i.loli.net/2020/11/30/cQYRqUzH1SvgA5I.png" alt=""></p><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a><strong>R-CNN</strong></h2><p>R-CNN 是 2014 年出现的一篇目标检测方法，为后来目标检测的研究奠定了基础，R-CNN 结合了传统的滑窗法和边框回归思路，开创性地提出了候选区的概念（Region proposals），先从输入图像中找到一些可能存在对象的候选区，这个过程称为 Selective Search（根据输入图像的特征如颜色等提取出候选区）。在 R-CNN 中这些候选区大概会有 2000 个，对这些区域进行分类和微调即可完成目标检测。候选区的提出大大减少了目标定位的时间，提高了目标检测的效率。然而由于深度学习分类器的存在，非常耗费算力，且 proposal 的生成都是在 CPU 上完成的，因此 R-CNN 处理一张图片大概需要 49 秒，这和实时检测差的还很远。</p><p><img src="https://i.loli.net/2020/11/30/c4A15kyVlfmbi9v.png" alt=""></p><p>下面我们来理解一下 R-CNN 的具体训练流程，它的整体思路如上图，具体细节如下。</p><ul><li><strong>Region proposals</strong>：使用 Selective Search 方法生成大约 2000 个候选框（proposals），与 ground truth box（以下简称 GT box）的 IOU（交并比）大于 0.5 则认为这个 proposal 为值得学习的 positive（正例），否则为 negative（负例，即 background）。</li><li><strong>Supervised pre-training and Domain-specific fine-tuning</strong>：在 ImageNet 上预训练 VGG16 这样的视觉深度模型用于提取图像特征，然后在 VOC 数据集上 fine tune 为 21 类（包括背景），这样就得到了一个优质的特征提取器，只要将 proposal 区域 resize 到这个卷积模型的需要尺寸就能提取特征，提取的特征用于目标的分类和边框回归。</li><li><strong>Object category classifiers and Bounding box regression</strong>：分类器使用 SVM 进行分类。分类完成后，边框回归模型用于精调边框的位置，论文对每一类目标使用了简单的线性回归模型。具体训练的细节如优化器等配置这里就不细细展开了。</li></ul><p>测试时（推理时）思路类似，先对图像产生大量的 proposal，然后进行分类和边框回归，然后使用 NMS 算法（非极大值抑制）去除冗余的边框得到最终的检测结果。</p><p><strong>至此，我们理解了 R-CNN 的思路，尽管它不像传统方法那样穷举可能的目标位置，然而 Selective Search 方法提取的候选框多达 2000 个，这 2000 个框都需要 resize 后送入 CNN 和 SVM 中，这就是说，对一幅图像的检测需要对 2000 个图像进行 CNN 特征提取，这在这个算力爆炸的时代都是很费时的，何况在当时，这就导致 R-CNN 平均处理一张图片需要 49 秒。</strong></p><p>那么，有没有办法提高检测的速度呢？其实很容易发现，这 2000 个 proposal 都是图像的一部分，完全可以对图像提取一次特征，然后只需要将 proposal 在原图的位置映射到特征图上，就得到该 proposal 区域的特征了。这样，对一幅图像只需要一次得到卷积特征，然后将每个 proposal 特征送入全连接进行后续任务的端到端学习。</p><h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a><strong>Fast R-CNN</strong></h2><p>在上面一节的最后，得到一个优化 R-CNN 的思路，SPPNet 沿着这个思路走了下去，其中最关键的一个问题就是 crop/warp 操作使得 proposal 尺寸固定送入了 CNN 中，这是为了全连接层固定输入的要求，但是导致了图像的失真，因此作者设计了金字塔池化来使得任意输入的图像获得同维的特征。有了这个方法，就能对 R-CNN 进行优化了，只对原图进行一次卷积计算，便得到整张图的卷积特征 feature map，然后找到每个候选框在 feature map 上的映射 patch，将此 patch 作为每个候选框的卷积特征输入到 SPP layer（空间金字塔池化层） 和之后的层，完成特征提取工作。如此，对一幅图，SPPNet 只需要提取一次卷积特征，提速 R-CNN 百倍左右。</p><p>Fast R-CNN 吸取了 SPPNet 的思路，对 R-CNN 进行了改进，获得了进一步的性能提升。它采用了单层空间金字塔池化（文中为 RoI Pooling）代替原来的池化操作使得不同的 proposal 在最后一层 feature map 上映射得到的区域上经过池化后得到同维特征。原本的 proposal 位置经过下采样倍率除法就可以得到其在特征图上的位置从而获得特征，这个不难理解，遇到除不尽的就取整即可。<strong>问题是，RoI Pooling 是如何保证任意尺度的特征图都能得到同维的输出特征呢 2？RoI Pooling 采用了一个简单的策略，那就是网格划分。</strong></p><p>假设我有个$w=7,h=5$的 RoI 特征矩阵，我需要得到$W=2,H=2$的输出特征，那么其实只需要将$7\times 5$的矩阵划分为四个部分，每个部分取最大元素值就行，不过，由于非方阵，，难以等分，所以 RoI Pooling 会先进行取整，这样$w$被划分为 3 和 4，$h$被划分为 2 和 3。这样就完成了 RoI 区域的固定维度特征提取，有意思的是，<strong>上述过程其实发生了两次取整，一次计算 proposal 在 feature map 上的坐标，一次计算 feature map 上的 RoI 区域的网格划分，这个过程是有问题的，所以后来提出了 RoI Align。</strong></p><p><img src="https://i.loli.net/2020/11/30/wVenWRkUbPrCfY3.png" alt=""></p><p>上图所示的就是 Fast R-CNN 的 pipeline 设计，现在一次卷积特征提取就能得到 2000 个 proposal 的特征了，后续的任务作者也做了改变。彼时，<strong>多任务学习</strong>已经初有起色，原先在 R-CNN 中是先提 proposal，然后 CNN 提取特征，之后用 SVM 分类器，最后再做 bbox regression 的，在 Fast R-CNN 中则将对 proposal 的分类和边框回归同时通过全连接层完成，利用一个上图所示的多任务学习网络。实验也证明，这两个任务能够共享卷积特征，并相互促进。</p><p>其他方面，和 R-CNN 处理思路类似，其实，除了 proposal 的生成，整个 Fast R-CNN 已经是端到端训练的了，Fast-RCNN 很重要的一个贡献是为 Region Proposal + CNN 这种实时检测框架提供了一丝可能性，这也为后来的 Faster R-CNN 埋下伏笔。</p><p>最后提一下，Fast R-CNN 单图平均处理时间缩短到了 2.3 秒，这是一个巨大的突破，不过，对实时检测而言还是有点慢，而下一节的 Faster R-CNN 则将实时检测的可能性化为了现实。</p><h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a><strong>Faster R-CNN</strong></h2><p>在 Fast R-CNN 中留下了一个遗憾，也是制约网络速度和端到端训练的部件，那就是 Selective Search 求得候选框，这个方法其实非常耗时，Faster R-CNN 就设计了一个 Region Proposal Network（RPN，区域推荐网络）代替 Selective Search，同时也引入了 anchor box 应对目标形状的变化问题（anchor 可以理解为位置和大小固定的 bbox，或者说事先设置好的固定的 proposal）。</p><p><img src="https://i.loli.net/2020/11/30/LUgclEvd8bXsipn.png" alt=""></p><p>具体而言，就像上图一样，RPN 网络会在最后一层特征图上对大量的预定义的 anchor 进行正例挑选（这是个分类任务）以及边界框回归（这是个回归任务），通过这两个任务会得到较为准确的 proposal，这就完成了 Fast R-CNN 中 Selective Search 得到的 proposal 了。接着，这些 proposal 会被用于和 Fast R-CNN 中一样的目标分类和边框回归来得到精确的检测结果。</p><p><img src="https://i.loli.net/2020/11/30/KWs7YZC2EbJtmFv.png" alt=""></p><p>RPN 的引入使得端到端目标检测框架得以实现，这大大提高了目标检测的速度，它单图检测时间只需要 0.2 秒。直到今天，Faster R-CNN 这种基于候选框的二阶段目标检测框架仍然是目标检测的一个重要研究领域，Faster R-CNN 也成为目标检测领域绕不过的里程碑。</p><h2 id="Sparse-R-CNN"><a href="#Sparse-R-CNN" class="headerlink" title="Sparse R-CNN"></a><strong>Sparse R-CNN</strong></h2><p>距离 Faster R-CNN 的出现已经过去了 5 年的时间，这 5 年是目标检测高速发展的 5 年，在 Faster R-CNN 之后出现了 anchor-based 的单阶段目标跟踪范式，它主张去掉 RPN 直接对 anchor 进行处理，在速度上获得了卓越的表现，如 SSD、RetinaNet 等，它们成为了目标检测的新 SOTA。再后来，受到工业界普遍关注的高效<a href="https://zhouchen.blog.csdn.net/article/details/108032597">anchor-free 方法</a>为目标检测开拓了全新的方向，如 FCOS、CenterNet 等。今年，FaceBook 发布了使用 Transformer 构建的 DETR 框架，它只需要少量的低维输入即可获得 SOTA 检测表现，这为稀疏检测带来了一丝契机。</p><p>最近，沿着目标检测中 Dense 和 Dense-to-Sparse 的框架的设计思路，Sparse R-CNN 建立了一种彻底的稀疏框架，它完全脱离了 anchor、RPN 和 NMS 后处理等设计。</p><p>首先，我们来回顾一下目标检测的范式。最著名的为<strong>Dense范式</strong>，密集检测器的思路从深度学习时代之前沿用至今，这种范式最典型的特征是会设置大量的候选（candidates），无论是anchor、参考点还是其他的形式，最后，网络会对这些候选进行过滤以及分类和边框微调。接着，取得过突出成果的<strong>Dense-to-Sparse范式</strong>最典型的代表就是上文所说的R-CNN系列，这种范式的典型特点是对一组较为稀疏的候选进行分类和回归，这组稀疏的候选其实还是来自密集检测中那种。</p><p><img src="https://i.loli.net/2020/11/30/cQYRqUzH1SvgA5I.png" alt=""></p><p>很多人认为目标检测已经是个几乎solved problem，然而Dense范式总有一些问题难以忽略：较慢的NMS后处理、多对一的正负样本分类、candidates的设计等。这些问题可以通过Sparse范式解决，DETR提出了一个解决方案：DETR中，candidates是一组sparse的可学习的object queries，正负样本分配是一对一的二分图匹配，而且，不需要NMS就能获得检测结果。然而，DETR使用的Transformer无形之中使得每个object query与全局特征图进行了注意力交互，这还是Dense的交互。</p><p>Sparse R-CNN作者认为，Sparse的检测框架不仅仅是稀疏的候选，也应该是稀疏的特征交互，因此提出了Sparse R-CNN。</p><p><img src="https://i.loli.net/2020/11/30/bRAIPkMeNhSv7L9.png" alt=""></p><p>Sparse R-CNN的出发点是通过少量的（如100个）proposal代替RPN产生的动辄上千的proposal。不妨看一下上面的pipeline设计，Sparse R-CNN的proposal是一组可学习参数，就是上面绿色框中的$N*4$向量，其中$N$表示proposal的数目，一般几百个就够了，4代表物体边框的4个属性，为归一化后的中心点坐标$(x,y)$以及宽高（$w$和$h$），这个向量作为可学习参数和其他参数一起被网络优化，这就是整体的设计。</p><p>但是，如果这也就可以了那么前人早就实现了，这个学习到的proposal可以理解为什么呢？其实就是根据图像信息推理得到的可能出现物体的统计值，这种“肤浅”的proposal确定的RoI特征显然不足以用来精确定位和恶分类目标，事实上作者在Faster R-CNN基础上换成这种可学习的proposal后精度下降了20个点。于是，，作者提出了上图蓝色框中的特征层面的proposal，它和proposal box数目一致，是一个高维特征向量（256维左右）。<strong>proposal box和proposal feature一一对应，proposal feature和proposal box提取出来的RoI feature做一对一的交互，使得RoI特征更利于分类和回归。这种交互如下图所示，类似于注意力机制，这种交互模块可以多个拼接以进一步精炼特征，所以该模块为Dynamic Instance Interactive Module，堆叠多个这样的模块构成了该框架的Dynamic Instance Interactive Head。</strong> 由于交互是一对一的，所以特征的交互和proposal一样是稀疏的。</p><p><img src="https://i.loli.net/2020/11/30/HopeQ7qyCt5wNul.png" alt=""></p><p>关于整体框架：backbone采用基于ResNet的FPN，Head是一组Dynamic Instance Interactive Head，上一个head的output features和output boxes作为下一个head的proposal features和proposal boxes。proposal features在与RoI features交互之前做self-attention。整体训练的损失函数是基于二分图匹配的集合预测损失。</p><p><img src="https://i.loli.net/2020/11/30/n7b5JhgqB9PLIjy.png" alt=""></p><p>实验结果上看，在COCO数据集达到了SOTA表现。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>R-CNN和Fast R-CNN出现后的一段时期内，目标检测领域的一个重要研究方向是提出更高效的候选框，其中Faster R-CNN开创性提出RPN，产生深远影响。Sparse R-CNN以一组稀疏输入即可获得比肩SOTA的检测性能，为真正的端到端检测开拓了一条路。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R-CNN系列解读 </tag>
            
            <tag> R-CNN解读 </tag>
            
            <tag> Fast R-CNN解读 </tag>
            
            <tag> Faster R-CNN解读 </tag>
            
            <tag> Sparse R-CNN解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EfficientDet 解读</title>
      <link href="2020/11/22/efficientdet/"/>
      <url>2020/11/22/efficientdet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇发表于 CVPR2020 的检测论文不同于大火的 anchor-free，还是基于 one-stage 的范式做的设计，是 ICML2019 的 EfficientNet 的拓展，将分类模型引入到了目标检测任务中。近些年目标检测发展迅猛，精度提升的同时也使得模型越来越大、算力需求越来越高，这制约了算法的落地。近些年出现了很多高效的目标检测思路，如 one-stage、anchor-free 以及模型压缩策略，它们基本上都是以牺牲精度为代价获得效率的。EfficientDet 直指当前目标检测的痛点：有没有可能在大量的资源约束前提下，实现高效且高精度的目标检测框架？<strong>这就是 EfficientDet 的由来。</strong></p><ul><li><p>论文标题</p><p>EfficientDet: Scalable and Efficient Object Detection</p></li><li><p>论文地址</p><p><a href="http://arxiv.org/abs/1911.09070">http://arxiv.org/abs/1911.09070</a></p></li><li><p>论文源码</p><p><a href="https://github.com/google/automl/tree/master/efficientdet">https://github.com/google/automl/tree/master/efficientdet</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>之前提到，EfficientDet 是 EfficientNet 的拓展，我们首先来简单聊一聊 EfficientNet，感兴趣的请阅读<a href="https://arxiv.org/abs/1905.11946">原文</a>。在 EfficientNet 中提到了一个很重要的概念 Compound Scaling（符合缩放），这是基于一个公认的事实：调整模型的深度、宽度以及输入的分辨率在一定范围内会对模型性能有影响，但是过大的深度、宽度和分辨率对性能改善不大还会严重影响模型前向效率，所以 EfficientNet 提出复合系数$\phi$统一缩放网络的宽度、深度和分辨率，具体如下。</p><p><img src="https://i.loli.net/2020/11/22/rQsZnAzVWE986He.png" alt=""></p><p>这里的$\alpha, \beta, \gamma$都是由一个很小范围的网络搜索得到的常量，直观上来讲，$\phi$是一个特定的系数，可以用来控制资源的使用量，$\alpha, \beta, \gamma$决定了具体是如何分配资源的。值得注意的是，常规卷积操作的计算量是和$d, w^{2}, r^{2}$成正比的，加倍深度会使得 FLOPS 加倍，但是加倍宽度和分辨率会使得 FLOPS 加 4 倍。由于卷积 ops 经常在 CNN 中占据了大部分计算量，使用等式上式缩放卷积网络将会使得整体计算量近似增加$\left(\alpha \cdot \beta^{2} \cdot \gamma^{2}\right)^{\phi}$倍。由于 EfficientNet 对任意$\phi$增加了约束$\alpha \cdot \beta^{2} \cdot \gamma^{2} \approx 2$，整体的计算量近似增加了$2^{\phi}$倍。</p><p><strong>对比 EfficientNet 从 B0 到 B7 的提升，不难知道，这种复合缩放可以较大的提升模型性能，所以 EfficientDet 也将其引入了进来。</strong></p><p>论文首先分析了目前 OD（object detection，目标检测）的两个挑战：<strong>高效多尺度特征融合</strong>和<strong>模型缩放</strong>。</p><p><strong>多尺度特征融合</strong>：FPN 如今被广泛用于多尺度特征融合，最近 PANet、NAS-FPN 等方法研究了更多跨尺度特征融合的结构。不过，这些方法融合不同尺度特征的方式都是简单加和，这默认了不同尺度特征的贡献是同等的，然而往往不是这样的。为了解决这个问题，论文提出了一种简单但是高效的加权双向特征金字塔网络（<strong>BiFPN</strong>），它对不同的输入特征学习权重。</p><p><strong>模型缩放</strong>：之前的方法依赖于更大的 backbone 或者更大分辨率的输入，论文发现放大特征网络和预测网络对精度和速度的考量是很重要的。基于 EfficientNet 的基础，论文为目标检测设计了一种复合缩放（<strong>Compound Scaling</strong>）方法，联合调整 backbone、特征网络、预测网络的深度、宽度和分辨率。</p><p>和 EfficientNet 一样，EfficientDet 指的是一系列网络，如下图包含 D1 到 D7，速度逐渐变慢，精度逐渐提升。在理解 EfficientDet 两个核心工作（<strong>BiFPN</strong>和<strong>Compound Scaling</strong>）之前，可以先看看下图的 SOTA 方法比较，可以看到 EfficientDet-D7 的效果非常惊人，在 FLOPs 仅为 AmoebaNet+NAS-FPN+AA 的十分之一的前提下，COCO2017 验证集上的 AP 到达了 55.1，超越了 SOTA5 个点。而且单尺度训练的 EfficientD7 现在依然霸榜 PaperWithCode 上。</p><p><img src="https://i.loli.net/2020/11/22/NU35fMiRe6Fo2BT.png" alt=""></p><p><img src="https://i.loli.net/2020/11/22/LI5h3EBkVtWQzjO.png" alt=""></p><p>此外，查看官方仓库提供的模型，其参数量其实是不大的（当然，这不绝对意味着计算量小）。</p><p><img src="https://i.loli.net/2020/11/22/giL6hNfOYMVA2I1.png" alt=""></p><h2 id="BiFPN"><a href="#BiFPN" class="headerlink" title="BiFPN"></a>BiFPN</h2><p>CVPR2017 的 FPN 指出了不同层之间特征融合的重要性如下图 a，不过它采用的是自上而下的特征图融合，融合方式也是很简单的高层特征加倍后和低层特征相加的方式。此后，下图 b 所示的 PANet 在 FPN 的基础上又添加了自下而上的信息流。再后来出现了不少其他的融合方式，直到 NAS-FPN 采用了 NAS 策略搜索最佳 FPN 结构，得到的是下图 c 的版本，不过 NAS-FPN 虽然简单高效，但是精度和 PANet 还是有所差距的，并且 NAS-FPN 这种结构是很怪异的，难以理解的。所以，EfficientDet 在 PANet 的基础上进行了优化如下图的 d：移除只有一个输入的节点；同一个 level 的输入和输出节点进行连接，类似 skip connection；PANet 这种一次自上而下再自下而上的特征融合可以作为一个单元重复多次从而获得更加丰富的特征，不过重复多少次是速度和精度的权衡选择，这在后面的复合 22 缩放部分讲到。</p><p><img src="https://i.loli.net/2020/11/22/iuAG38EdxKI9DqF.png" alt=""></p><p>上述是 FPN 特征流动的结构，如何数学上组合这些特征也是一个重要的方面。此前的方法都是上一层特征 resize 之后和当前层特征相加。这种方式存在诸多不合理之处，因为这样其实默认融合的两层特征是同权重的，事实上不同尺度的特征对输出特征的贡献是不平等的，应当对每个输入特征加权，这个权重需要网络自己学习。当然，学习到的权重需要归一化到和为 1，采用 softmax 是一个选择，但是 softmax 指数运算开销大，所以作者这里简化为快速标准化融合的方式（Fast normalized fusion），它的计算方法如下，其实就是去掉了 softmax 的指数运算，这种方式在 GPU 上快了很多，精度略微下降，可以接受。</p><script type="math/tex; mode=display">O=\sum_{i} \frac{w_{i}}{\epsilon+\sum_{j} w_{j}} \cdot I_{i}</script><h2 id="Compound-Scaling"><a href="#Compound-Scaling" class="headerlink" title="Compound Scaling"></a>Compound Scaling</h2><p>在看复合缩放之前，我们先要知道，有了 BiFPN 又有了 EfficientNet 再加上 head 部分，其实网络框架已经确定了，如下图所示，左边是 backbone（EfficientDet），中间是多层 BiFPN，右边是 prediction head 部分。</p><p><img src="https://i.loli.net/2020/11/22/P28ZjJ4sgwYSXGh.png" alt=""></p><p>结合 EfficientNet 的联合调整策略，论文提出目标检测的联合调整策略，用复合系数$\phi$统一调整.调整的内容包括 backbone（EfficientNet 版本，B0 到 B6）、neck 部分的 BiFPN（通道数、layer 数）以及 head 部分（包括层数）还要输入图像分辨率。不过，和 EfficientNet 不同，由于参数太多采用网格搜索计算量很大，论文采用启发式的调整策略。其中 backbone 的选择系数控制，BiFPN 的配置用下面第一个式子计算，head 的层数和输入分辨率是下面 2、3 式的计算方式。</p><script type="math/tex; mode=display">W_{b i f p n}=64 \cdot\left(1.35^{\phi}\right), \quad D_{b i f p n}=3+\phi</script><script type="math/tex; mode=display">D_{b o x}=D_{c l a s s}=3+\lfloor\phi / 3\rfloor</script><script type="math/tex; mode=display">R_{\text {input}}=512+\phi \cdot 128</script><p>最后得到的 8 种结构的配置表如下图。</p><p><img src="https://i.loli.net/2020/11/22/fw7sLOZj8KNtUm2.png" alt=""></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在简介里我已经提到很多这个检测框架的过人之处了，这里就简单看一下在 COCO 验证集的效果，可以说无论是速度还是精度都是吊打其他 SOTA 方法的，至今依然在 COCO 验证集榜首的位置。</p><p><img src="https://i.loli.net/2020/11/22/KUOb2RJcBCQt5fu.png" alt=""></p><p>此外作者也将其拓展到语义分割，潜力也是比较大的。还做了不少消融实验，感兴趣的可以自行查看论文原文。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文最大的亮点在于提出了目标检测网络联合调整复杂度的策略，从而刷新了 SOTA 结果。这个思路来自 EfficientDet，同样 backbone 的高效也源自该网络。文中另一个突出的成果在于设计了 BiFPN 以及堆叠它，可以看到效果还是很显著的。此外，除了官方的 TF 实现外，这里也推荐一个目前公认最好的 PyTorch 实现（由国内大佬完成），<a href="https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch">Github 地址</a>给出，这也是唯一一个达到论文效果的 PyTorch 复现（作者复现时官方还没有开源）。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Tan M, Pang R, Le Q V. EfficientDet: Scalable and Efficient Object Detection[J]. arXiv:1911.09070 [cs, eess], 2020.</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EfficientDet解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CSTrack 解读</title>
      <link href="2020/11/20/cstrack/"/>
      <url>2020/11/20/cstrack/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>自从 FairMOT 的公开以来，MOT 似乎进入了一个高速发展阶段，先是 CenterTrack 紧随其后发布并开源 ，然后是后来的 RetinaTrack、MAT、FGAGT 等 SOTA 方法出现，它们不断刷新着 MOT Challenge 的榜单。最近，CSTrack 这篇文章则在 JDE 范式的基础上进行了改进，获得了相当不错的跟踪表现（基本上暂时稳定在榜单前 5），本文就简单解读一下这篇短文（目前在 Arxiv 上开放的是一个 4 页短文的版本）。</p><ul><li><p>论文标题</p><p>Rethinking the competition between detection and ReID in Multi-Object Tracking</p></li><li><p>论文地址</p><p><a href="http://arxiv.org/abs/2010.12138">http://arxiv.org/abs/2010.12138</a></p></li><li><p>论文源码</p><p><a href="https://github.com/JudasDie/SOTS">https://github.com/JudasDie/SOTS</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>为了追求速度和精度的平衡，联合训练检测模型和 ReID 模型的 JDE 范式（如下图，具体提出参考 JDE 原论文 Towards Real-Time Multi-Object Tracking）受到了学术界和工业界越来越多的关注。这主要是针对之前的 two-stage 方法先是使用现有的检测器检测出行人然后再根据检测框提取对应行人的外观特征进行关联的思路，这种方法在精度上的表现不错，然而由于检测模型不小且 ReID 模型需要在每个检测框上进行推理，计算量非常之大，所以 JDE 这种 one-shot 的思路的诞生是一种必然。</p><p><img src="https://i.loli.net/2020/11/20/XSvleZbIjwDo1Yd.png" alt=""></p><p>然而，就像之前 FairMOT 分析的那样，检测和 ReID 模型是<strong>存在不公平的过度竞争</strong>的，这种竞争制约了两个任务（检测任务和 ReID 任务 ）的表示学习，导致 了学习的混淆。具体而言，检测任务需要的是同类的不同目标拥有相似的语义信息（类间距离最大），而 ReID 要求的是同类目标有不同的语义信息（类内距离最大）。此外，<strong>目标较大的尺度变化</strong>依然是 MOT 的痛点。在 ReID 中图像被调整到统一的固定尺寸来进行查询 ，而在 MOT 中，提供在 ReID 网络的特征需要拥有尺度感知能力，这是因为沿着帧目标可能会有巨大的 size 变化。</p><p>为了解决上述的过度竞争问题，论文提出了一种新的互相关网络（CCN）来改进单阶段跟踪框架下 detection 和 ReID 任务之间的协作学习。作者首先将 detection 和 ReID 解耦为两个分支，分别学习。然后两个任务的特征通过自注意力方式获得自注意力权重图和互相关性权重图。自注意力图是促进各自任务的学习，互相关图是为了提高两个任务的协同学习。而且，为了解决上述的尺度问题，设计了尺度感知注意力网络（SAAN）用于 ReID 特征的进一步优化，SAAN 使用了空间和通道注意力，该网络能够获得目标 不同尺度的外观信息，最后 不同尺度外观特征融合输出即可。</p><h2 id="框架设计"><a href="#框架设计" class="headerlink" title="框架设计"></a>框架设计</h2><p>整体的思路还是采用 JDE 的框架，下图的右图是整体 pipeline 的设计，和左侧的 JDE 相比，中间增加了一个 CCN 网络（互相相关网络）用于构建 detection 和 ReID 两个分支不同的特征图（这里解释一下为什么要解耦成两个特征图送入后续的两个任务中，其实原始的 JDE 就是一个特征图送入检测和 ReID 分支中，作者这边认为这会造成后续的混淆，所以在这里分开了，FairMOT 也发现了这个问题，只是用另一种思路解决的而已）。构建的两个特征图分别送入 Detection head 和 SAAN（多尺度+注意力+ReID）中，Detection head 将 JDE 的 YOLO3 换为了更快更准的 YOLO5，其他没什么变动，下文检测这边我就不多提了。检测完成的同时，SAAN 也输出了多尺度融合后的 ReID 特征，至此也就完成了 JDE 联合检测和 ReID 的任务，后续就是关联问题了。</p><p><img src="https://i.loli.net/2020/11/20/KIpS3jV1ezicCk5.png" alt=""></p><p>所以，从整个框架来看，CSTrack 对 JDE 的改动主要集中在上图的 CCN 和 SAAN，而这两部分都是在特征优化上做了文章，且主要是基于注意力手段的特征优化（不知道是好是坏呢）。</p><h2 id="CCN"><a href="#CCN" class="headerlink" title="CCN"></a>CCN</h2><p>CCN（Cross-correlation Network）用于提取更适合 detection 和 ReID 任务的一般特征和特定特征。在特定性学习方面，通过学习反映不同特征通道之间相互关系的自联系，增强了每个任务的特征表示。对于一般性学习，可以通过精心设计的相互关系机制来学习两个任务之间的共享信息。</p><p><img src="https://i.loli.net/2020/11/20/XoOUnxREIqfCy3K.png" alt=""></p><p>CCN 的结构如上图，我们对着这个图来理解 CCN 的思路。从检测器的 backbone 得到的特征图为$\mathbf{F} \in R^{C \times H \times W}$，首先，这个特征经过平均池化降维获得统计信息（更精炼的特征图）$\mathbf{F}^{\prime} \in R^{C \times H^{\prime} \times W^{\prime}}$。然后，两个不同的卷积层作用于$\mathbf{F}^{\prime}$生成两个特征图$\mathbf{T_1}$和$\mathbf{T_2}$，这两个特征图被 reshape 为特征$\left{\mathbf{M}_{\mathbf{1}}, \mathbf{M}_{\mathbf{2}}\right} \in R^{C \times N^{\prime}}$（$N^{\prime}=H^{\prime} \times W^{\prime}$）。下面的上下两个分支操作是一致的，先用矩阵$\mathbf{M_1}$或者$\mathbf{M_2}$和自己的转置矩阵相乘获得各自的自注意力图$\left{\mathbf{W}_{\mathrm{T}_{1}}, \mathbf{W}_{\mathrm{T}_{2}}\right} \in R^{\mathrm{C} \times \mathrm{C}}$，然后$\mathbf{M_1}$和$\mathbf{M_2}$的转置进行矩阵乘法获得互注意力图$\left{\mathbf{W}_{\mathrm{S}_{1}}, \mathbf{W}_{\mathrm{S}_{2}}\right} \in R^{\mathrm{C} \times \mathrm{C}}$（这是$\mathbf{M_1}$的，转置之后 softmax 就是$\mathbf{M_2}$的）。然后，对每个分支，自注意力图和互注意力图相加获得通道级别的注意力图，和原始的输入特征图$\mathbf{F}$相乘再和$\mathbf{F}$相加得到输出特征图$\mathrm{F}_{\mathrm{T} 1}$和$\mathrm{F}_{\mathrm{T} 2}$。</p><p>上述学到的$\mathrm{F}_{\mathrm{T} 1}$用于 Detection head 的检测处理，后者则用于下面的 SAAN 中 ReID 特征的处理。</p><p><img src="https://i.loli.net/2020/11/20/KhYU9rfR3g7VnFe.png" alt=""></p><p>上图就是作者设计的 ReID 分支，用于对 ReID 特征进行多尺度融合，这个设计挺简单的，不同分支采用不同的下采样倍率获得不同尺度的特征图（其中通过空间注意力进行特征优化），然后融合产生的特征通过空间注意力加强，最终输出不同目标的 embedding$\mathbf{E} \in R^{512 \times W \times H}$（特征图每个通道对应不同的 anchor 的 embedding）。</p><p><strong>这样，整个 JDE 框架就完成了。</strong></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在 MOT16 和 MOT17 上实验结果如下图，比较的方法都比较新，MOTA 也是刷到了 70 以上，不过速度稍许有点慢了，总的精度还是很不错的。</p><p><img src="https://i.loli.net/2020/11/20/Tcv3j8x1ruYHsQh.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>CSTrack 在 JDE 的基础上使用了更强的检测器也对 ReID 特征进行了优化，获得了相当不错的表现。不过，从结果上看这种暴力解耦还是会对整个跟踪的速度有影响的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Liang C, Zhang Z, Lu Y, et al. Rethinking the competition between detection and ReID in Multi-Object Tracking[J]. arXiv:2010.12138 [cs], 2020.</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CSTrack解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCN &amp; RepPoints解读</title>
      <link href="2020/11/13/dcn-reppoints/"/>
      <url>2020/11/13/dcn-reppoints/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>近几年，Anchor-free的目标检测方法受到了很大的关注，究其原因，该类方法不需要像Anchor-base方法那样受限于anchor的配置（anchor的设置要求开发者对数据很了解）就可以获得不错的检测结果，大大减少了数据分析的复杂过程。Anchor-free方法中有一类方法是基于关键点的，它通过检测目标的边界点（如角点）来配对组合成边界框，RepPoints系列就是其代表之作，这包括了RepPoints、Dense RepPoints和RepPoints v2。不过，回顾更久远的历史，从模型的几何形变建模能力的角度来看，RepPoints其实也是对可变形卷积（Deformable Convolutional Networks，DCN）系列的改进，所以本文会从DCN开始讲起，简单回顾这几个工作对几何建模的贡献，其中，DCN系列包括DCN和DCN v2。</p><ul><li><a href="http://arxiv.org/abs/1703.06211">DCN</a>（ICCV2017）</li><li><a href="http://arxiv.org/abs/1811.11168">DCN v2</a>（CVPR2019）</li><li><a href="http://arxiv.org/abs/1904.11490">RepPoints</a>（ICCV2019）</li><li><a href="http://arxiv.org/abs/1912.11473">Dense RepPoints</a>（ECCV2020）</li><li><a href="http://arxiv.org/abs/2007.08508">RepPoints v2</a>（暂未收录）</li></ul><h2 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h2><p>首先，我们来看DCN v1。在计算机视觉中，同一物体在不同的场景或者视角中未知的几何变化是识别和检测任务的一大挑战。为了解决这类问题，通常可以在数据和算法两个方面做文章。从数据的角度看来，通过充分的<strong>数据增强</strong>来构建各种几何变化的样本来增强模型的尺度变换适应能力；从算法的角度来看，设计一些<strong>几何变换不变的特征</strong>即可，比如SIFT特征。</p><p>上述的两种方法都很难做到，前者是因为样本的限制必然无法构建充分数据以保证模型的泛化能力，后者则是因为手工特征设计对于复杂几何变换是几乎不可能实现的。所以作者设计了Deformable Conv（可变形卷积）和Deformable Pooling（可变形池化）来解决这类问题。</p><h3 id="可变形卷积"><a href="#可变形卷积" class="headerlink" title="可变形卷积"></a><strong>可变形卷积</strong></h3><p>顾名思义，可变形卷积的含义就是进行卷积运算的位置是可变的，不是传统的矩形网格，以原论文里的一个可视化图所示，左边的传统卷积的感受野是固定的，在最上层的特征图上其作用的区域显然不是完整贴合目标，而右边的可变形卷积在顶层特征图上自适应的感受野很好的捕获了目标的信息（这可以直观感受得到）。</p><p><img src="https://i.loli.net/2020/11/13/19Arvci4ldpDfna.png" alt=""></p><p>那么可变形卷积是如何实现的呢，其实是通过针对每个卷积采样点的偏移量来实现的。如下图所示，其中淡绿色的表示常规采样点，深蓝色的表示可变卷积的采样点，它其实是在正常的采样坐标的基础上加上了一个偏移量（图中的箭头）。</p><p><img src="https://i.loli.net/2020/11/13/HMiJ4mwNU9fPzjS.png" alt=""></p><p>我们先来看普通的卷积的实现。使用常规的网格$\mathcal{R}$在输入特征图$x$上进行采样，采样点的值和权重$w$相乘加和得到输出值。举个例子，一个3x3的卷积核定义的网格$\mathcal{R}$表示如下式，中心点为$(0,0)$，其余为相对位置，共9个点。</p><script type="math/tex; mode=display">\mathcal{R}=\{(-1,-1),(-1,0), \ldots,(0,1),(1,1)\}</script><p>那么，对输出特征图$y$上的任意一个位置$p_0$都可以以下式进行计算，其中$\mathbf{p}_n$表示就是网格$\mathcal{R}$中的第$n$个点。</p><script type="math/tex; mode=display">\mathbf{y}\left(\mathbf{p}_{0}\right)=\sum_{\mathbf{p}_{n} \in \mathcal{R}} \mathbf{w}\left(\mathbf{p}_{n}\right) \cdot \mathbf{x}\left(\mathbf{p}_{0}+\mathbf{p}_{n}\right)</script><p>而可变形卷积干了啥呢，它对原本的卷积操作加了一个偏移量$\left{\Delta \mathbf{p}_{n} \mid n=1, \ldots, N\right}$，也就是这个偏移量使得卷积可以不规则进行，所以上面的计算式变为了下式。不过要注意的是，这个偏移量可以是小数，所以偏移后的位置特征需要通过双线性插值得到，计算式如下面第二个式子。</p><script type="math/tex; mode=display">\mathbf{y}\left(\mathbf{p}_{0}\right)=\sum_{\mathbf{p}_{n} \in \mathcal{R}} \mathbf{w}\left(\mathbf{p}_{n}\right) \cdot \mathbf{x}\left(\mathbf{p}_{0}+\mathbf{p}_{n}+\Delta \mathbf{p}_{n}\right)</script><script type="math/tex; mode=display">\mathbf{x}(\mathbf{p})=\sum_{\mathbf{q}} G(\mathbf{q}, \mathbf{p}) \cdot \mathbf{x}(\mathbf{q})</script><p>至此，可变卷积的实现基本上理清楚了，现在的问题就是，这个偏移量如何获得？不妨看一下论文中一个3x3可变卷积的解释图（下图），图中可以发现，上面绿色的分支其实学习了一个和输入特征图同尺寸且通道数为$2N$的特征图（$N$为卷积核数目），这就是偏移量，之所以两倍是因为网格上偏移有x和y两个方向。</p><p><img src="https://i.loli.net/2020/11/13/uemUOJDXLHWwcEi.png" alt=""></p><h3 id="可变形RoI池化"><a href="#可变形RoI池化" class="headerlink" title="可变形RoI池化"></a><strong>可变形RoI池化</strong></h3><p><img src="https://i.loli.net/2020/11/13/V1DoF9PZqim2jwd.png" alt=""></p><p>理解了可变形卷积，理解可变形RoI就没有太大的难度了。原始的RoI pooling在操作时将输入RoI划分为$k\times k = K$个区域，这些区域叫做bin，偏移就是针对这些bin做的。针对每个bin学习偏移量，这里通过全连接层进行学习，因此deformable RoI pooling的输出如下式（含义参考上面的可变卷积即可）。</p><script type="math/tex; mode=display">\mathbf{y}(i, j)=\sum_{\mathbf{p} \in \operatorname{bin}(i, j)} \mathbf{x}\left(\mathbf{p}_{0}+\mathbf{p}+\Delta \mathbf{p}_{i j}\right) / n_{i j}</script><p><strong>至此，关于DCN的解读就完成了，下图是一个来自原论文对的DCN效果的可视化，可以看到绿点标识的目标基本上被可变形卷积感受野覆盖，且这种覆盖能够针对不同尺度的目标。这说明，可变形卷积确实能够提取出感兴趣目标的完整特征，这对目标检测大有好处。</strong></p><p><img src="https://i.loli.net/2020/11/13/xS8pHOFe72Wf3Jq.png" alt=""></p><h2 id="DCN-v2"><a href="#DCN-v2" class="headerlink" title="DCN v2"></a>DCN v2</h2><p>DCNv1尽管获得了不错的效果，然而还是存在着不少的问题，DCNv2中进行了一个可视化，对比了普通卷积、DCNv1和DCNv2的区别，下图中每个图片都有从上到下三个可视化，分别是采样点、有效感受野、有效分割区域。可以看出来，DCNv1虽然能覆盖整个目标，但是这种覆盖不够“精细”，会带来不少的背景信息的干扰。</p><p><img src="https://i.loli.net/2020/11/13/kHfNovIC9XZcwy1.png" alt=""></p><p>为此，DCNv2提出了一些改进，总结下来就是：</p><ol><li>使用更多的可变卷积层</li><li>Modulated Deformable Modules（调制变形模块）</li><li>RCNN特征模仿指导训练</li></ol><h3 id="更多的可变卷积层"><a href="#更多的可变卷积层" class="headerlink" title="更多的可变卷积层"></a><strong>更多的可变卷积层</strong></h3><p>DCNv1中，只在ResNet50的conv5中使用了3个可变形卷积，DCNv2认为更多的可变形卷积会有更强的几何变化建模效果，所以将conv3到conv5都换为了可变形卷积。之前之所以没有采用更多的可变形卷积，是因为当时没有在大型检测数据集上进行验证，致使精度提升不高。</p><h3 id="调制可变卷积模块"><a href="#调制可变卷积模块" class="headerlink" title="调制可变卷积模块"></a><strong>调制可变卷积模块</strong></h3><p>这才是本文比较大的突破之一，设计了调制可变卷积来控制采样点权重，这样就可以忽略掉不重要甚至有负效果的背景信息。下式是引入偏移量的可变卷积输出特征图的计算式，这是DCNv1的思路。</p><script type="math/tex; mode=display">\mathbf{y}\left(\mathbf{p}_{0}\right)=\sum_{\mathbf{p}_{n} \in \mathcal{R}} \mathbf{w}\left(\mathbf{p}_{n}\right) \cdot \mathbf{x}\left(\mathbf{p}_{0}+\mathbf{p}_{n}+\Delta \mathbf{p}_{n}\right)</script><p>上面我们不是发现DCNv1采样了很多无效区域吗，DCNv2则认为，从输入特征图上不仅仅需要学习偏移量，还需要学习一个权重来表示采样点区域是否感兴趣，不感兴趣的区域，权重为0即可。所以原来的计算式变为下式，其中我们所说的权重$\Delta m_{k} \in [0,1]$称为调制因子。<strong>结构上的实现就是原来学习的offset特征图由2N个通道变为3N个通道，这N个通道的就是调制因子。</strong></p><script type="math/tex; mode=display">y(p)=\sum_{k=1}^{K} w_{k} \cdot x\left(p+p_{k}+\Delta p_{k}\right) \cdot \Delta m_{k}</script><p>相应的，可变形池化也引入这样的调制因子，计算式变为下式，结构上的实现类似上面的调制可变卷积，这里就不详细展开了。</p><script type="math/tex; mode=display">y(k)=\sum_{j=1}^{n_{k}} x\left(p_{k j}+\Delta p_{k}\right) \cdot \Delta m_{k} / n_{k}</script><h3 id="RCNN特征模仿"><a href="#RCNN特征模仿" class="headerlink" title="RCNN特征模仿"></a><strong>RCNN特征模仿</strong></h3><p>作者发现，RCNN和Faster RCNN的分类score结合起来，模型的表现会有提升。这说明，RCNN学到的关注在物体上的特征可以解决无关上下文的问题。但是将RCNN融入整个网络会降大大降低推理速度，DCNv2这里就采用了类似知识蒸馏的做法，把RCNN当作teacher network，让DCNv2主干的Faster RCNN获得的特征去模拟RCNN的特征。</p><p><img src="https://i.loli.net/2020/11/13/mxRNAUnickufZOt.png" alt=""></p><p>整个训练设计如上图，左边的网络为主网络（Faster RCNN），右边的网络为子网络（RCNN）。用主网络训练过程中得到的RoI去裁剪原图，然后将裁剪到的图resize到224×224作为子网络的输入，最后将子网络提取的特征和主网络输出的特征计算feature mimicking loss，用来约束这2个特征的差异（实现上就是余弦相似度）。同时子网络通过一个分类损失（如下式）进行监督学习，因为并不需要回归坐标，所以没有回归损失。推理阶段因为没有子网络，所以速度不会有缺失。</p><script type="math/tex; mode=display">L_{\operatorname{mimic}}=\sum_{b \in \Omega}\left[1-\cos \left(f_{\mathrm{RCNN}}(b), f_{\mathrm{FRCNN}}(b)\right)\right]</script><p>很多人不理解RCNN的有效性，其实RCNN这个子网络的输入就是RoI在原输入图像上裁剪出来的图像，这就导致不存在RoI以外区域信息（背景）的干扰，这就使得RCNN这个网络训练得到的分类结果是非常可靠的，以此通过一个损失函数监督主网络Faster RCNN的分类路线训练就能够使网络提取到更多RoI内部特征，而不是自己引入的外部特征。<strong>这是一种很有效的训练思路，但这并不能算创新，所以DCNv2的创新也就集中在调制因子上了。而且，从这个训练指导来看，DCNv2完全侧重于分类信息，对采样点没有监督，因此只能学习分类特征而没有几何特征。</strong></p><p>DCNv2对DCNv1进行了一些粗暴的改进并获得了卓有成效的效果，它能将有效感受野更稳定地聚集在目标有效区域，尽管留下了一些遗憾，不过前人的坑总会被后人解决，那就是RepPoints的故事了。</p><h2 id="RepPoints"><a href="#RepPoints" class="headerlink" title="RepPoints"></a>RepPoints</h2><p>这篇文章发表于ICCV2019，新颖地提出使用点集的方式来表示目标，这种方法在不使用anchor的前提下取得了非常好的效果。很多人认为RepPoints是DCNv3，这是由于RepPoints也采用了可变形卷积提取特征，不过它对采用点有相应的损失函数（追求更高的可解释性），可以对DCN的采样点进行监督，让其学习有效的RoI内部信息。此外，它也弥补了之前DCNv2的遗憾，可以学习到目标的几何特征，，而不仅仅是分类特征。</p><p>总结一下，个人觉得，RepPoints主要针对了之前DCNv2的offset（偏移量）学习过于black box，难以解释，所以采用了定位和分类损失直接监督偏移量的学习，这样定位和识别任务会更加“精致”一些。从这个角度来看（如果仅仅是将RepPoints看作一种新的目标表示法未免太低估其价值了），它其实是对DCNv2的改进，至于作者是不是这个出发点，，可以参考作者的<a href="https://www.zhihu.com/question/322372759/answer/798327725">知乎回答</a>，，总之，其对DCNv2的改进可以总结如下：</p><ol><li>通过定位和分类的损失直接监督可形变卷积的偏移量的学习，使得偏移量具有可解释性；</li><li>通过采样点来直接生成伪框 (pseudo box)，不需要另外学习边界框，这样分类和定位建立起了联系。</li></ol><p>当然，上面这些看法是从DCNv2的角度来看的，下面我们回归作者论文的思路来看看RepPoints是如何实现的。</p><p><img src="https://i.loli.net/2020/11/13/s5pudHOwIBvNKYL.png" alt=""></p><p>首先，RepPoints（representative points，表示点）具体是怎样表示目标的呢，其实就像下图这样。显然，这和传统的bbox表示法（bounding box，边界框）不同。在目标检测任务中，bbox作为描述检测器各阶段的目标位置的标准形式。然而，其虽然容易计算，但它们仅提供目标的粗略定位，并不完全拟合目标的形状和姿态（因为bbox是个矩形，而目标往往不可能如此规则，所以bbox中必然存在冗余信息，这会导致特征的质量下降从而降低了检测器性能）。对此，提出了一种更贴合目标的细粒度目标表示RepPoints，其形式是一组通过学习自适应地置于目标之上的点，这种表示既限制了目标的空间位置又捕获了精确的语义信息。</p><p>RepPoints是一种如下的二维表示（我们一般用2D表示和4D表示来区分anchor-free和anchor-base方法），一个目标由$n$个点确定，在该论文中$n$设置为9。</p><script type="math/tex; mode=display">\mathcal{R}=\left\{\left(x_{k}, y_{k}\right)\right\}_{k=1}^{n}</script><p>这种点表示可以通过转换函数$\mathcal{T}$转换为伪框（pseudo box），三种转换函数具体可以参考原论文3.2节。然后就是RepPoints的监督学习了，这里也不难实现，通过pseudo box就可以和GT的bbox进行定位监督了，而分类监督和之前的方法没有太大的区别。</p><p><img src="https://i.loli.net/2020/11/13/d8LgoFJzXxHNGvk.png" alt=""></p><p>借此，作者设计了一个anchor-free检测框架RPDet，该框架在检测的各个阶段均使用RepPoints表示。通过下图的Pipeline其实就能理解这个网络架构：首先，通过FPN获得特征图；不同于其他单阶段方法一次分类一次回归得到最终结果，RPDet通过两次回归一次分类得到结果（作者自称1.5阶段），分类和第二次回归均采用可变形卷积，可变形卷积可以很好的和RepPoints结合。因为它是在不规则点上进行的。可形变卷积的偏移量是通过第一次回归得到的，也就意味着偏移量在训练过程中是有监督的，而第一次回归的偏移量通过对角点监督得到。采用这种方式，后续的分类和回归特征均是沿着目标选取的，特征质量更高。</p><p><img src="https://i.loli.net/2020/11/13/QDGWAlZou4HNOCn.png" alt=""></p><h2 id="Dense-RepPoints"><a href="#Dense-RepPoints" class="headerlink" title="Dense RepPoints"></a>Dense RepPoints</h2><p>Dense RepPoints是RepPoints之后的一个成果，它将RepPoints拓展至实例分割任务，而方法就是采用更加密集的点集表示目标，如下图。</p><p><img src="https://i.loli.net/2020/11/13/GhBfVzujlgDYObw.png" alt=""></p><p>Dense RepPoints在RepPoints的基础上进行了如下拓展，使用了更多的点并赋予了点属性。而在表示物体方面，Dense RepPoints采用上图第四个边缘掩码的方式表示目标，它综合了轮廓表示（表示边缘）和网格掩码（前后景区分，利于学习）的优点。</p><script type="math/tex; mode=display">\mathcal{R}=\left\{\left(x_{i}+\Delta x_{i}, y_{i}+\Delta y_{i}, \mathbf{a}_{i}\right)\right\}_{i=1}^{n}</script><p><img src="https://i.loli.net/2020/11/13/X7esbqK1hxBDYtQ.png" alt=""></p><p>最后pipeline和常规的思路很类似，唯一的问题就是采用点集在目标检测中尚可，用于分割会因为点数太多导致计算量大增，所以设计Group pooling;、Shared offset fields;、Shared attribute map来减少计算量，此外还发现点集损失比点损失更加合适等问题，具体可以查阅论文。</p><h2 id="RepPoints-v2"><a href="#RepPoints-v2" class="headerlink" title="RepPoints v2"></a>RepPoints v2</h2><p>最后，我们来看看最新的RepPoints v2，这是对原本RepPoints目标检测任务上的改进。验证（这里指的是分割任务）和回归是神经网络两类通用的任务。验证更容易学习并且准确，回归通常很高效并且能预测连续变化。因此，采用一定的方式将两者组合起来能充分利用它们的优势。RepPoints v2就是在RepPoints的基础上增加了验证模块，提升了检测器性能。</p><p><img src="https://i.loli.net/2020/11/13/uyBAhK13zkHdGSb.png" alt=""></p><p>这篇文章细节不少，这里不展开了，直接对着上面的pipeline讲讲整体的思路。RepPoints V2在RepPoints方法基础上添加了一个验证（分割）分支，该分支主要包含两部分，一部分是角点预测，另一部分是目标前景分割。</p><p>如上图所示，训练阶段得到分割heatmap后，这个分割图和原始特征相加，作为回归特征的补充。不过，在推理阶段，在回归分支获取到目标位置后，利用该分割heatmap来对结果进行进一步修正。</p><p>总的来说，这个分支的添加对任务提升不小：一方面多任务往往会带来更好的效果；另一方面，利用分割图增强特征确实可以增强回归效果。</p><p>RepPoints v2工作有一定的可拓展性，它证明了在基于回归的方法上加上这个模块确实可以提升性能，后人要做的就是权衡精度和速度了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从DCN到RepPoints，本质上其实都是更精细的特征提取演变的过程。点集（RepPoints）方式只是显式的表现了出来而已，不过其确实能在精度和速度上取得非常好的平衡。以RepPoints或者类似的思路如今已经活跃在目标检测和实例分割任务中，推动着计算机视觉基础任务的发展，，这是难能可贵的。而且，跳出bbox的范式也诠释着，有时候跳出固有的范式做研究，会获得意想不到的效果。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DCN解读 </tag>
            
            <tag> RepPoints解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LambdaNetworks 论文解读</title>
      <link href="2020/11/04/lambdanetworks/"/>
      <url>2020/11/04/lambdanetworks/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近有不少人和我提到 ViT 以及 DETR 以及商汤提出的 Deformable DETR，仿若看到了 Transformer 在计算机视觉中大放异彩的未来，甚至谷歌对其在自注意力机制上进行了调整并提出 Performer。但是，由于 Transformer 的自注意力机制对内存的需求是输入的平方倍，这在图像任务上计算效率过低，当输入序列很长的时候，自注意力对长程交互建模计算量更是庞大无比。而且，Transformer 是出了名的难训练。所以，想要看到其在视觉任务上有更好的表现，还需要面临不小的挑战，不过，LambdaNetworks倒是提出了一种新的长程交互信息捕获的新范式，而且在视觉任务中效果很不错。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>文章对于捕获输入和结构化上下文之间的长程交互提出了一种新的通用框架，该方法名为Lambda Layer。它通过将可用上下文转化为名为lambdas的线性函数，并将这些函数分别应用于每个输入。Lambda层是通用的，它可以建模全局或者局部的内容和位置上的信息交互。并且，由于其避开了使用“昂贵”的注意力图，使得其可以适用于超长序列或者高分辨率图像。由Lambda构成的LambdaNetworks在计算上是高效的，并且可以通过主流计算库实现。实验证明，LambdaNetworks在图像分类、目标检测、实例分割等任务上达到sota水准且计算更加高效。同时，作者也基于ResNet改进设计了LambdaResNet并且获得和EfficientNet相当的效果，快了4.5倍。</p><ul><li><p>论文地址</p><p>  <a href="https://openreview.net/forum?id=xTJEN-ggl1b">https://openreview.net/forum?id=xTJEN-ggl1b</a></p></li><li><p>论文源码</p><p>  <a href="https://github.com/lucidrains/lambda-networks">https://github.com/lucidrains/lambda-networks</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>建模长程信息交互是机器学习领域很重要的课题，注意力机制是当前最主流的范式，然而，自注意力的二次内存占用不利于处理超长序列或者多维输入，比如包含数万像素的图像。论文中这里举了个例子，一批256个64x64的图像使用8head的多头注意力就需要32G的内存。</p><p>考虑到自注意力的局限性，论文提出了Lambda层，该层为捕获输入和结构化的上下文之间的长程信息交互提供了一种新的通用框架。Lambda层捕获信息交互的方式也很简单，它将可用上下文转化为线性函数，并将这些线性函数分别应用于每个输入，这些线性函数就是lambda。Lambda层可以成为注意力机制的替代品，注意力在输入和上下文之间定义了一个相似性核，而Lambda层将上下文信息总结为一个固定size的线性函数，这样就避开了很耗内存的注意力图。他俩的对比，可以通过下面的图看出来（左图是一个包含三个query的局部上下文，它们同处一个全局上下文中；中图是attention机制产生的注意力图；右图则是lambda层线性函数作用于query的结果）。</p><p><img src="https://i.loli.net/2020/11/04/eDh6RuJ7BsMjgZx.png" alt=""></p><p>Lambda层用途广泛，可以实现为在全局、局部或masked上下文中对内容和基于位置的交互进行建模。由此产生的神经网络结构LambdaNetworks具有高效的计算能力，并且可以以较小的内存开销建模长程依赖，因此非常适用于超大结构化输入，如高分辨率图像。</p><p>后文也用实验证明，在注意力表现很好的任务中，LambdaNetworks表现相当，且计算更为高效且更快。</p><h2 id="长程信息交互建模"><a href="#长程信息交互建模" class="headerlink" title="长程信息交互建模"></a>长程信息交互建模</h2><p>论文在第二部分主要对一些Lambda的术语进行了定义，引入keys作为捕获queries和它们的上下文之间信息交互的需求，而且，作者也说明，Lambda layer采用了很多自注意力的术语来减少阅读差异，这就是为什么很多人觉得两者在很多名称定义上差异不大的原因。</p><h3 id="queries、contexts和interactions"><a href="#queries、contexts和interactions" class="headerlink" title="queries、contexts和interactions"></a><strong>queries、contexts和interactions</strong></h3><p>$\mathcal{Q}=\left{\left(\boldsymbol{q}_{n}, n\right)\right}$和$\mathcal{C}=\left{\left(\boldsymbol{c}_{m}, m\right)\right}$分别表示queries和contexts，每个$\left(\boldsymbol{q}_{n}, n\right)$都包含内容$\boldsymbol{q}_{n} \in \mathbb{R}^{|k|}$和位置$n$，同样的，每个上下文元素$\left(\boldsymbol{c}_{m}, m\right)$都包含内容$\boldsymbol{c}_{m}$和位置$m$，而$(n, m)$指的是任意结构化元素之间的成对关系。举个例子，这个(n,m)对可以指被固定在二维栅格上的两个像素的相对距离，也可以指图（Graph）上俩node之间的关系。</p><p>下面，作者介绍了Lambda layer的工作过程。先是考虑给定的上下文$\mathcal{C}$的情况下通过函数$\boldsymbol{F}:\left(\left(\boldsymbol{q}_{n}, n\right), \mathcal{C}\right) \mapsto \boldsymbol{y}_{n}$将query映射到输出向量$\boldsymbol{y}_{n}$。显然，如果处理的是结构化输入，那么这个函数可以作为神经网络中的一个层来看待。将$\left(\boldsymbol{q}_{n}, \boldsymbol{c}_{m}\right)$称为基于内容的交互，$\left(\boldsymbol{q}_{n},(n, m)\right)$则为基于位置的交互。此外，若$\boldsymbol{y}_{n}$依赖于所有的$\left(\boldsymbol{q}_{n}, \boldsymbol{c}_{m}\right)$或者$\left(\boldsymbol{q}_{n},(n, m)\right)$，则称$\boldsymbol{F}$捕获了全局信息交互，如果只是围绕$n$的一个较小的受限上下文用于映射，则称$\boldsymbol{F}$捕获了局部信息交互。最后，若这些交互包含了上下文中所有$|m|$个元素则称为密集交互（dense interaction），否则为稀疏交互（sparse interaction）。</p><h3 id="引入key来捕获长程信息交互"><a href="#引入key来捕获长程信息交互" class="headerlink" title="引入key来捕获长程信息交互"></a><strong>引入key来捕获长程信息交互</strong></h3><p>在深度学习这种依赖GPU计算的场景下，我们优先考虑快速的线性操作并且通过点积操作来捕获信息交互。这就促使了引入可以和query通过点击进行交互的向量，该向量和query同维。特别是基于内容的交互$\left(\boldsymbol{q}_{n}, \boldsymbol{c}_{m}\right)$需要一个依赖$\boldsymbol{c}_{m}$的$k$维向量，这个向量就是key（键）。相反，基于位置的交互$\left(\boldsymbol{q}_{n},(n, m)\right)$则需要位置编码$\boldsymbol{e}_{n m} \in \mathbb{R}^{|k|}$，有时也称为相对key。query和key的深度$|k|$以及上下文空间维度$|m|$不在输出$\boldsymbol{y}_{n} \in \mathbb{R}^{|v|}$，因此需要将这些维度收缩为layer计算的一部分。因此，捕获长程交互的每一层都可以根据它是收缩查询深度还是首先收缩上下文位置来表征。</p><h3 id="注意力交互"><a href="#注意力交互" class="headerlink" title="注意力交互"></a><strong>注意力交互</strong></h3><p>收缩query的深度首先会在query和上下文元素之间创建一个相似性核，这就是attention操作。随着上下文位置$|m|$的增大而输入输出维度$|k|$和$|v|$不变，考虑到层输出是一个很小维度的向量$|v| \ll|m|$，注意力图（attention map）的计算会变得很浪费资源。</p><h3 id="Lambda交互"><a href="#Lambda交互" class="headerlink" title="Lambda交互"></a><strong>Lambda交互</strong></h3><p>相反，通过一个线性函数$\boldsymbol{\lambda}(\mathcal{C}, n)$获得输出$\boldsymbol{y}_{n}=F\left(\left(\boldsymbol{q}_{n}, n\right), \mathcal{C}\right)=\boldsymbol{\lambda}(\mathcal{C}, n)\left(\boldsymbol{q}_{n}\right)$会更高效地简化映射过程（map）。在这个场景中，上下文被聚合为一个固定size的线性函数$\boldsymbol{\lambda}_{n}=\boldsymbol{\lambda}(\mathcal{C}, n)$。每个$\boldsymbol{\lambda}_{n}$作为一个小的线性函数独立于上下文并且被用到相关的query$\boldsymbol{q}_n$后丢弃。这个机制很容易联想到影响比较大的函数式编程和lambda积分，所以称为lambda层。</p><h2 id="Lambda层"><a href="#Lambda层" class="headerlink" title="Lambda层"></a>Lambda层</h2><p>一个lambda层将输入$\boldsymbol{X} \in \mathbb{R}^{|n| \times d_{i n}}$和上下文$\boldsymbol{C} \in \mathbb{R}^{|m| \times d_{c}}$作为输入并产生线性函数lambdas分别作用于query，返回输出$\boldsymbol{Y} \in \mathbb{R}^{|n| \times d_{o u t}}$。显然，在自注意力中，$\boldsymbol{C} = \boldsymbol{X}$。为了不失一般性，我们假定$d_{i n}=d_{c}=d_{o u t}=d$。在接下来的论文里，作者将重点放在了lambda层的一个具体实例上，并且证明lambda层可以获得密集的长程内容和位置的信息交互而不需要构建注意力图。</p><h3 id="将上下文转化为线性函数"><a href="#将上下文转化为线性函数" class="headerlink" title="将上下文转化为线性函数"></a><strong>将上下文转化为线性函数</strong></h3><p>首先，假定上下文只有一个query$\left(\boldsymbol{q}_{n}, n\right)$。我们希望产生一个线性函数lambda$\mathbb{R}^{|k|} \rightarrow \mathbb{R}^{|v|}$，我们将$\mathbb{R}^{|k| \times|v|}$称为函数。下表所示的就是lambda层的超参、参数以及其他相关的配置。</p><p><img src="https://i.loli.net/2020/11/04/rRBmAzS2dYHeniW.png" alt=""></p><p><strong>生成上下文lambda函数</strong>：lambda层首先通过线性投影上下文来计算keys和values，并且使用softmax操作跨上下文对keys进行标准化从而得到标准化后的$\bar{K}$。它的实现可以看作是一种函数式消息传递，每个上下文元素贡献一个内容function$\boldsymbol{\mu}_{m}^{c}=\overline{\boldsymbol{K}}_{m} \boldsymbol{V}_{\boldsymbol{m}}^{T}$和位置function$\boldsymbol{\mu}_{n m}^{p}=\boldsymbol{E}_{n m} \boldsymbol{V}_{\boldsymbol{m}}^{T}$，最终的lambda函数其实是两者的和，具体如下，式子中的$\boldsymbol{\lambda}^{c}$为内容lambda，而$\boldsymbol{\lambda}^p_n$为位置lambda。内容$\boldsymbol{\lambda}^{c}$对上下文元素的排列是不变的，在所有的query位置$n$之间共享，并仅基于上下文内容对$\boldsymbol{q}_{n}$进行编码转换。不同的是，位置$\lambda_{n}^{p}$基于内容$\boldsymbol{c}_{m}$和位置$(n, m)$对查询query进行编码转换，从而支持对结构化输入建模如图像。</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{\lambda}^{c} &=\sum_{m} \boldsymbol{\mu}_{m}^{c}=\sum_{m} \overline{\boldsymbol{K}}_{m} \boldsymbol{V}_{\boldsymbol{m}}^{T} \\\boldsymbol{\lambda}_{n}^{p} &=\sum_{m} \boldsymbol{\mu}_{n m}^{p}=\sum_{m} \boldsymbol{E}_{n m} \boldsymbol{V}_{\boldsymbol{m}}^{T} \\\boldsymbol{\lambda}_{n} &=\boldsymbol{\lambda}^{c}+\boldsymbol{\lambda}_{n}^{p} \in \mathbb{R}^{|k| \times|v|}\end{aligned}</script><p><strong>应用lambda到query</strong>：输入被转化为query$\boldsymbol{q}_{n}=\boldsymbol{W}_{Q} \boldsymbol{x}_{n}$，然后lambda层获得如下输出。</p><script type="math/tex; mode=display">\boldsymbol{y}_{n}=\boldsymbol{\lambda}_{n} \boldsymbol{q}_{n}=\left(\boldsymbol{\lambda}^{c}+\boldsymbol{\lambda}_{n}^{p}\right) \boldsymbol{q}_{n} \in \mathbb{R}^{|v|}</script><p><strong>Lambda的解释</strong>：$\boldsymbol{\lambda}_{n} \in \mathbb{R}^{|k| \times|v|}$矩阵的列可以看作$|k| |v|$维上下文特征的固定size的集合。这些上下文特征从上下文内容和结构聚合而来。应用lambda线性函数动态地分布这些上下文特征来产生输出$\boldsymbol{y}_{n}=\sum_{k} q_{n k} \boldsymbol{\lambda}_{n k}$。这个过程捕获密集地内容和位置的长程信息交互，而不需要产生注意力图。</p><p><strong>标准化</strong>： 实验表明，非线性或者标准化操作对计算是有帮助的，作者在计算的query和value之后应用batch normalization发现是有效的。</p><h3 id="对结构化上下文应用Lambda函数"><a href="#对结构化上下文应用Lambda函数" class="headerlink" title="对结构化上下文应用Lambda函数"></a><strong>对结构化上下文应用Lambda函数</strong></h3><p>在这一节，作者主要介绍如何将lambda层应用于结构化上下文。</p><p><strong>Translation equivariance</strong>：在很多机器学习场景中，Translation equivariance是一个很强的归纳偏置。由于基于内容的信息交互是排列等变的，因此本就是translation equivariant。而位置的信息交互获得translation equivariant则通过对任意的translation $t$确保位置编码满足$\boldsymbol{E}_{n m}=\boldsymbol{E}_{t(n) t(m)}$来做到。实际中，我们定义一个相对位置编码的张量$\boldsymbol{R} \in \mathbb{R}^{|k| \times|r| \times|u|}$，其中$r$索引对所有的$(n,m)$对可能的相对位置，并将其重新索引为$\boldsymbol{E} \in \mathbb{R}^{|k| \times|n| \times|m| \times|u|}$，如$\boldsymbol{E}_{n m}=\boldsymbol{R}_{r(n, m)}$。</p><p><strong>Lambda 卷积</strong>： 尽管有长程信息交互的好处，局部性在许多任务中仍然是一个强烈的归纳偏置。从计算的角度来看，使用全局上下文可能会产生噪声或过度。因此，将位置交互的范围限制到查询位置$n$周围的一个局部邻域，就像局部自注意和卷积的情况一样，可能是有用的。这可以通过对所需范围之外的上下文位置$m$的位置嵌入进行归零来实现。然而，对于较大的$|m|$值，这种策略仍然代价高昂，因为计算仍然会发生(它们只是被归零)。在上下文被安排在多维网格上时，可以通过常规卷积从局部上下文中生成位置lambdas，将$\boldsymbol{V}$中的$v$维视为额外的空间维度。考虑在一维序列上的大小为$|r|$的局部域上生成位置lambdas。相对位置编码张量$\boldsymbol{R} \in \mathbb{R}^{|r| \times|u| \times|k|}$可以被reshape到$\overline{\boldsymbol{R}} \in \mathbb{R}^{|r| \times 1 \times|u| \times|k|}$，并且被用作二维卷积核来计算需要的位置lambda，算式如下。</p><script type="math/tex; mode=display">\boldsymbol{\lambda}_{b n v k}=\operatorname{conv} 2 \mathrm{d}\left(\boldsymbol{V}_{b n v u}, \overline{\boldsymbol{R}}_{r 1 u k}\right)</script><p>这个操作称为lambda卷积，由于计算被限制在一个局部范围，lambda卷积相对于输入只需要线性时间和内存复杂度的消耗。lambda卷积很容易和其他功能一起使用，如dilation和striding，并且在硬件计算上享受告诉运算。计算效率和局部自注意力形成了鲜明对比，如下表。</p><p><img src="https://i.loli.net/2020/11/04/bmIGXYyWcRNJ23T.png" alt=""></p><h3 id="multiquery-lambdas减少复杂性"><a href="#multiquery-lambdas减少复杂性" class="headerlink" title="multiquery lambdas减少复杂性"></a><strong>multiquery lambdas减少复杂性</strong></h3><p>这部分作者主要对计算复杂度进行了分析，设计了多query lambda，计算复杂度对比如下。</p><p><img src="https://i.loli.net/2020/11/04/41S9hjkmUFdxiXb.png" alt=""></p><p>提出的multiquery lambdas可以通过einsum高效实现。</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{\lambda}_{b k v}^{c}=& \operatorname{einsum}\left(\overline{\boldsymbol{K}}_{b m k u}, \boldsymbol{V}_{b m v u}\right) \\\boldsymbol{\lambda}_{b n k v}^{p} &=\operatorname{einsum}\left(\boldsymbol{E}_{k n m u}, \boldsymbol{V}_{b m v u}\right) \\\boldsymbol{Y}_{b n h v}^{c} &=\operatorname{einsum}\left(\boldsymbol{Q}_{b n h k}, \boldsymbol{\lambda}_{b k v}^{c}\right) \\\boldsymbol{Y}_{b n h v}^{p} &=\operatorname{einsum}\left(\boldsymbol{Q}_{b n h k}, \boldsymbol{\lambda}_{b n k v}^{p}\right) \\\boldsymbol{Y}_{b n h v} &=\boldsymbol{Y}_{b n h v}^{c}+\boldsymbol{Y}_{b n h v}^{p}\end{aligned}</script><p>然后，对比了lambda 层和自注意力在resnet50架构上的imagenet分类任务效果。显然，lambda层参数量是很少的，且准确率很高。</p><p><img src="https://i.loli.net/2020/11/04/fehAXKUOJM6Nas2.png" alt=""></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在大尺度高分辨率计算机视觉任务上进行了充分的实验，和SOTA的EfficientNet相比，可以说无论是速度还是精度都有不小的突破。</p><p><img src="https://i.loli.net/2020/11/04/mWH1QgTY4JGOfdZ.png" alt=""></p><p>其长子检测任务上，LambdaResNet也极具优势。</p><p><img src="https://i.loli.net/2020/11/04/1PTjFqlRaEcmJkK.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作者提出了Lambda Layer代替自注意力机制，获得了较好的改进。并借此设计了LambdaNetworks，其在各个任务上都超越了SOTA且速度提高了很多。如果实践证明，Lambda Layer的效果具有足够的鲁棒性，在以后的研究中应该会被广泛使用。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Anonymous. LambdaNetworks: Modeling long-range Interactions without Attention[A]. 2020.</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LambdaNetworks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FairMOT实时多目标跟踪系统</title>
      <link href="2020/10/24/fairmot-realtime/"/>
      <url>2020/10/24/fairmot-realtime/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>FairMOT是今年很火的一个多目标跟踪算法，前不久也开放了最新版本的论文，并于最近重构了开源代码，我也在实际工程视频上进行了测试，效果是很不错的。不过，官方源码没有提高实时摄像头跟踪的编程接口，我在源码的基础上进行了修改，增加了实时跟踪模块。本文介绍如何进行环境配置和脚本修改，实现摄像头跟踪（<strong>本文均采用Ubuntu16.04进行环境配置，使用Windows在安装DCN等包的时候会有很多问题，不建议使用</strong>）。</p><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><blockquote><p>下述环境配置需要保证用户已经安装了git和conda，否则配置pytorch和cuda会诸多不便。</p></blockquote><p>首先，通过下面的git命令从Github克隆源码到本地并进入该项目。访问<a href="https://pan.baidu.com/s/1H1Zp8wrTKDk20_DSPAeEkg">链接</a>（提取码uouv）下载训练好的模型，在项目根目录下新建<code>models</code>目录（和已有的<code>assets</code>、<code>src</code>等目录同级），将刚刚下载好的模型文件<code>fairmot_dla34.pth</code>放到这个<code>models</code>目录下。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone git@github.com:ifzhang/FairMOT.gitcd FairMOT<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>下面，通过conda创建适用于该项目的虚拟环境（环境隔离），国内用户速度慢可以参考<a href="https://zhouchen.blog.csdn.net/article/details/86086919">我conda的文章</a>配置国内源。创建之后通过<code>activate</code>激活环境（该命令出错将<code>conda</code>换为<code>source</code>）。然后在当前虚拟环境下（<strong>后续关于该项目的操作都需要在该虚拟环境下</strong>）安装pytorch和cuda（这里也建议配置国内源后安装<code>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch</code>）。最后，通过pip命令安装所需d的Python包（国内建议<a href="https://zhouchen.blog.csdn.net/article/details/106420275">配置清华源</a>），注意先安装cython。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">conda create -n fairmot python=3.6conda activate fairmotconda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0pip install cythonpip install -r requirements.txtpip install -U opencv-python==4.1.1.26<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>同时，用于项目使用了DCNv2所以需要安装该包，该包只能通过源码安装，依次执行下述命令即可（安装过程报warning是正常情况，不报error就行）。<br></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https://github.com/CharlesShang/DCNv2cd DCNv2./make.shcd ../<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>至此，所有的环境配置已经完成，由于这里还需要使用到ffmpeg来生成视频文件，所以系统需要安装ffmpeg（Ubuntu采用apt安装即可），教程很多，不多赘述。<p></p><p>想要试试项目是否正常工作，可以使用下面的命令在demo视频上进行跟踪测试（初次允许需要下载dla34模型，这个模型国内下载速度还可以，我就直接通过允许代码下载的）。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">cd srcpython demo.py mot --input-video ../videos/MOT16-03.mp4 --load_model ../models/fairmot_dla34.pth --conf_thres 0.4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>默认文件输出在项目根目录的<code>demos</code>文件夹下，包括每一帧的检测结果以及组合成的视频。</p><p><img src="https://i.loli.net/2020/10/24/bxVoM7ZyjTBmhLI.png" alt=""></p><p><img src="https://i.loli.net/2020/10/24/NF7idqKkQ8CyfG9.png" alt=""></p><h2 id="实时跟踪"><a href="#实时跟踪" class="headerlink" title="实时跟踪"></a>实时跟踪</h2><p>实时跟踪主要在两个方面进行修改，一是数据加载器，二是跟踪器。首先，我们在<code>src</code>目录下新建一个类似于<code>demo.py</code>的脚本文件名为<code>camera.py</code>，写入和<code>demo.py</code>类似的内容，不过，我们把视频路径换位摄像机编号（这是考虑到JDE采用opencv进行视频读取，而opencv视频读取和摄像机视频流读取是一个接口）。具体<code>camera.py</code>内容如下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> _init_paths<span class="token keyword">from</span> opts <span class="token keyword">import</span> opts<span class="token keyword">from</span> tracking_utils<span class="token punctuation">.</span>utils <span class="token keyword">import</span> mkdir_if_missing<span class="token keyword">import</span> datasets<span class="token punctuation">.</span>dataset<span class="token punctuation">.</span>jde <span class="token keyword">as</span> datasets<span class="token keyword">from</span> track <span class="token keyword">import</span> eval_seq<span class="token keyword">def</span> <span class="token function">recogniton</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    result_root <span class="token operator">=</span> opt<span class="token punctuation">.</span>output_root <span class="token keyword">if</span> opt<span class="token punctuation">.</span>output_root <span class="token operator">!=</span> <span class="token string">''</span> <span class="token keyword">else</span> <span class="token string">'.'</span>    mkdir_if_missing<span class="token punctuation">(</span>result_root<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"start tracking"</span><span class="token punctuation">)</span>    dataloader <span class="token operator">=</span> datasets<span class="token punctuation">.</span>LoadVideo<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> opt<span class="token punctuation">.</span>img_size<span class="token punctuation">)</span>    result_filename <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>result_root<span class="token punctuation">,</span> <span class="token string">'results.txt'</span><span class="token punctuation">)</span>    frame_rate <span class="token operator">=</span> dataloader<span class="token punctuation">.</span>frame_rate    frame_dir <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token keyword">if</span> opt<span class="token punctuation">.</span>output_format <span class="token operator">==</span> <span class="token string">'text'</span> <span class="token keyword">else</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>result_root<span class="token punctuation">,</span> <span class="token string">'frame'</span><span class="token punctuation">)</span>    eval_seq<span class="token punctuation">(</span>opt<span class="token punctuation">,</span> dataloader<span class="token punctuation">,</span> <span class="token string">'mot'</span><span class="token punctuation">,</span> result_filename<span class="token punctuation">,</span>             save_dir<span class="token operator">=</span>frame_dir<span class="token punctuation">,</span> show_image<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> frame_rate<span class="token operator">=</span>frame_rate<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'0'</span>    opt <span class="token operator">=</span> opts<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>init<span class="token punctuation">(</span><span class="token punctuation">)</span>    recogniton<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接着，原来JDE关于视频加载是针对真正的视频的，对于摄像头这种无限视频流，修改其帧数为无限大（很大很大的整数值即可），也就是将<code>src/lib/datasets/dataset/jde.py</code>中<code>LoadVideo</code>修改如下。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LoadVideo</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> path<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1088</span><span class="token punctuation">,</span> <span class="token number">608</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>cap <span class="token operator">=</span> cv2<span class="token punctuation">.</span>VideoCapture<span class="token punctuation">(</span>path<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>frame_rate <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">round</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FPS<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>vw <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_WIDTH<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>vh <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_HEIGHT<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">type</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>vn <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token number">32</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>vn <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>get<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>CAP_PROP_FRAME_COUNT<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>width <span class="token operator">=</span> img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>height <span class="token operator">=</span> img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>        self<span class="token punctuation">.</span>w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h <span class="token operator">=</span> <span class="token number">1920</span><span class="token punctuation">,</span> <span class="token number">1080</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Lenth of the video: {:d} frames'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>vn<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_size</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vw<span class="token punctuation">,</span> vh<span class="token punctuation">,</span> dw<span class="token punctuation">,</span> dh<span class="token punctuation">)</span><span class="token punctuation">:</span>        wa<span class="token punctuation">,</span> ha <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span>dw<span class="token punctuation">)</span> <span class="token operator">/</span> vw<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span>dh<span class="token punctuation">)</span> <span class="token operator">/</span> vh        a <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>wa<span class="token punctuation">,</span> ha<span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token builtin">int</span><span class="token punctuation">(</span>vw <span class="token operator">*</span> a<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>vh <span class="token operator">*</span> a<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">return</span> self    <span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>count <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>count <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> StopIteration        <span class="token comment"># Read image</span>        res<span class="token punctuation">,</span> img0 <span class="token operator">=</span> self<span class="token punctuation">.</span>cap<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># BGR</span>        <span class="token keyword">assert</span> img0 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">'Failed to load frame {:d}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>count<span class="token punctuation">)</span>        img0 <span class="token operator">=</span> cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>img0<span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># Padded resize</span>        img<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> letterbox<span class="token punctuation">(</span>img0<span class="token punctuation">,</span> height<span class="token operator">=</span>self<span class="token punctuation">.</span>height<span class="token punctuation">,</span> width<span class="token operator">=</span>self<span class="token punctuation">.</span>width<span class="token punctuation">)</span>        <span class="token comment"># Normalize RGB</span>        img <span class="token operator">=</span> img<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        img <span class="token operator">=</span> np<span class="token punctuation">.</span>ascontiguousarray<span class="token punctuation">(</span>img<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>        img <span class="token operator">/=</span> <span class="token number">255.0</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>count<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img0    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>vn  <span class="token comment"># number of frames</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>至此，读取视频流也通过一个粗暴的方式实现了，然后就是窗口显示了，原来项目中跟踪器只会一帧一帧写入跟踪后的结果图像，然后通过<code>ffmpeg</code>将这些图像组合为视频。不过，原项目已经设计了实时显示跟踪结果窗口的接口了，只需要调用<code>track.py</code>中的<code>eval_seq</code>函数时，参数<code>show_image</code>设置为<code>True</code>即可。不过，也许作者并没有测试过这个模块，这里显示会有些问题，务必将<code>eval_seq</code>中下述代码段进行如下修改。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> show_image<span class="token punctuation">:</span>    cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">'online_im'</span><span class="token punctuation">,</span> online_im<span class="token punctuation">)</span>    cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>调整完成后，输入下面的命令运行跟踪脚本（命令行Ctrl+C停止跟踪，跟踪的每一帧存放在指定的<code>output-root</code>目录下的<code>frame</code>目录中）。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python camera.py mot --load_model ../models/fairmot_dla34.pth --output-root ../results<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://i.loli.net/2020/10/24/Kp52w4qlb7nyVRQ.png" alt=""></p><p>上图是我实际测试得到的运行结果，摄像头分辨率比较低并且我做了一些隐私模糊处理，不过，整个算法的实用性还是非常强的，平均FPS也有18左右（单卡2080Ti）。</p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文对FairMOT源码进行了简单粗暴的修改以实现了一个摄像头视频实时跟踪系统，只是研究FairMOT代码闲暇之余的小demo，具体代码可以在<a href="https://github.com/luanshiyinyang/FairMOT">我的Github</a>找到。</p>]]></content>
      
      
      <categories>
          
          <category> 多目标跟踪 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FairMOT摄像头实时多目标跟踪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DLA 论文解读</title>
      <link href="2020/10/22/dla/"/>
      <url>2020/10/22/dla/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>沿着卷积神经网络在计算机视觉的发展史，可以发现，丰富的表示（representations）是视觉任务最需要的。随着网络的深度增加，单独一层的信息是远远不够的，只有聚合各层信息才能提高网络对 what（识别）和 where（定位）两个问题的推断能力。现有的很多网络设计工作主要致力于设计更深更宽的网络，但是如何更好地组合不同网络层（layer）、不同结构块（block）其实值得更多的关注。尽管跳跃连接（skip connecions）常用来组合多层，不过这样的连接依然是浅层的（因为其只采用了简单的单步运算）。该论文通过更深层的聚合来增强标准网络的能力。DLA 结构能够迭代式层级组合多层特征以提高精度的同时减参。实验证明，DLA 相比现有的分支和合并结构，效果更好。</p><ul><li><p>论文地址</p><p><a href="https://arxiv.org/abs/1707.06484">https://arxiv.org/abs/1707.06484</a></p></li><li><p>论文源码</p><p><a href="https://github.com/ucbdrive/dla">https://github.com/ucbdrive/dla</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>表示学习和迁移学习的发展推动了计算机视觉的发展，可以简单组合的特性催生了很多深度网络。为了满足各种不同的任务，寻找合适的网络结构至关重要。随着网络尺寸的增加，模块化的设计更为重要，所以现在的网络越来越深的同时，更紧密的连接能否带来提升呢？</p><p>更多的非线性、更强的表示能力、更大的感受野一般能够提高网络精度，但是会带来难以优化和计算量大的问题。为了克服这些缺陷，不同的block和modul被集成到一起来平衡和优化这些特点，如使用bottlenecks进行降维、使用residual、gated和concatnative连接特征以及梯度传播算法。这些技术使得网络可以到达100甚至1000层。</p><p>然而，如何连接这些layer和module还需要更多的探索。简单地通过序列化堆叠层来构造网络，如LeNet、AlexNet以及ResNet。经过复杂的分析，更深的网络层能提取到更多语义和全局的特征，但是这并不能表明最后一层就是任务需要的表示。实际上“跳跃连接”已经证明了对于分类、回归以及其他结构化问题的有效性。因此，如何聚合，尤其是深度与宽度上的聚合，对于网络结构的优化是一个非常重要的技术。</p><p><img src="https://i.loli.net/2020/10/22/ELq8voPCiXpZRWk.png" alt=""></p><p>论文研究了如何聚合各层特征来融合语义和空间信息以进行识别与定位任务。通过扩展现有的“浅跳跃连接”（单层内部进行连接），论文提出的聚合结构实现了更深的信息共享。文中主要引入两种DLA结构：iterative deep aggregation (IDA，迭代式深度聚合)和hierarchal deep aggregation (HDA，层级深度聚合)。为了更好的兼容现有以及以后的网络结构，IDA和HDA结构通过独立于backbone的结构化框架实现。IDA主要进行跨分辨率和尺度的融合，而HDA主要用于融合各个module和channel的特征。<strong>从上图也可以看出来，DLA集成了密集连接和特征金字塔的优势，IDA根据基础网络结构，逐级提炼分辨率和聚合尺度（语义信息的融合（发生在通道和深度上），类似残差模块），HDA通过自身的树状连接结构，将各个层级聚合为不同等级的表征（空间信息的融合（发生在分辨率和尺度上），类似于FPN）。本文的策略可以通过混合使用IDA与HDA来共同提升效果。</strong> </p><p>DLA通过实验在现有的ResNet和ResNeXt网络结构上采用DLA架构进行拓展，来进行图像分类、细粒度的图像识别、语义分割和边界检测任务。实验表明，DLA的可以在现有的ResNet、ResNeXt、DenseNet等网络结构的基础上提升模型的性能、减少参数数量以及减少显存消耗。DLA达到了目前分类任务中compact models的最佳精度，在分割等任务也达到超越SOTA。DLA是一种通用而有效的深度网络拓展技术。</p><h2 id="DLA"><a href="#DLA" class="headerlink" title="DLA"></a>DLA</h2><p>文中将“聚合”定义为跨越整个网络的多层组合，文中研究的也是那些深度、分辨率、尺度上能有效聚合的一系列网络。由于网络可以包含许多层和连接，模块化的设计可以通过分组和重复来克服复杂度问题。多个layer组合为一个block，多个block再根据分辨率组合为一个stage，DLA则主要探讨block和stage的组合（stage间网络保持一致分辨率，那么空间融合发生在stage间，语义融合发生在stage内）。</p><p><img src="https://i.loli.net/2020/10/22/6cypY4rg9Hkt12h.png" alt=""></p><h3 id="IDA"><a href="#IDA" class="headerlink" title="IDA"></a>IDA</h3><p>IDA沿着迭代堆叠的backbone进行，依据分辨率对整个网络分stage，越深的stage含有更多的语义信息但空间信息很少。“跳跃连接”由浅至深融合了不同尺度以及分辨率信息，但是这样的“跳跃连接”都是线性且都融合了最浅层的信息，如上图b中，每个stage都只融合上一步的信息。</p><p>因此，论文提出了IDA结构，从最浅最小的尺度开始，迭代式地融合更深更大尺度地信息，这样可以使得浅层网络信息在后续stage中获得更多地处理从而得到精炼，上图c就是IDA基本结构。</p><p>IDA对应的聚合函数$I$对不同层的特征，随着加深的语义信息表示如下（N表示聚合结点，后文会提到）：</p><script type="math/tex; mode=display">I\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\left\{\begin{array}{ll}\mathbf{x}_{1} & \text { if } n=1 \\I\left(N\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right), \ldots, \mathbf{x}_{n}\right) & \text { otherwise }\end{array}\right.</script><h3 id="HDA"><a href="#HDA" class="headerlink" title="HDA"></a>HDA</h3><p>HDA以树的形式合并block和stage来保持和组合特征通道，通过HDA，浅层和深层的网络层可以组合到一起，这样的组合信息可以跨越各个层级从而学得更加丰富。尽管IDA可以高效组合stage，但它依然是序列性的，不足以用来融合网络各个block信息。上图d就是HDA的深度分支结构，这是一个明显的树形结构。</p><p>在基础的HDA结构上，可以改进其深度和效率，将一个聚合结点的输出重置为主干网络用于下一棵子树的输入。结构如上图e所示，这样，之前所有block的信息都被送入后续处理中。同时，为了效率，作者将同一深度的聚合结点（指的是父亲和左孩子，也就是相同特征图尺寸的）合并，如上图f。</p><p>HDA的聚合函数$T_n$计算如下，$n$表示深度，$N$依然是聚合结点。</p><script type="math/tex; mode=display">\begin{array}{r}T_{n}(\mathbf{x})=N\left(R_{n-1}^{n}(\mathbf{x}), R_{n-2}^{n}(\mathbf{x}), \ldots,\right. \left.R_{1}^{n}(\mathbf{x}), L_{1}^{n}(\mathbf{x}), L_{2}^{n}(\mathbf{x})\right),\end{array}</script><p>上面式子的$R$和$L$定义如下，表示左树和右树，下式$B$表示卷积块。</p><script type="math/tex; mode=display">\begin{aligned}L_{2}^{n}(\mathbf{x}) &=B\left(L_{1}^{n}(\mathbf{x})\right), \quad L_{1}^{n}(\mathbf{x})=B\left(R_{1}^{n}(\mathbf{x})\right) \\R_{m}^{n}(\mathbf{x}) &=\left\{\begin{array}{ll}T_{m}(\mathbf{x}) & \text { if } m=n-1 \\T_{m}\left(R_{m+1}^{n}(\mathbf{x})\right) & \text { otherwise }\end{array}\right.\end{aligned}</script><h3 id="结构元素"><a href="#结构元素" class="headerlink" title="结构元素"></a>结构元素</h3><p><strong>聚合结点（Aggregation Nodes ）</strong></p><p>其主要功能是组合压缩输入，它通过学习如何选择和投射重要的信息，以在它们的输出中保持与单个输入相同的维度。论文中，IDA都是二分的（两个输入），HDA则根据树结构深度不同有不定量的参数。</p><p>虽然聚合结点可以采用任意结构，不过为了简单起见，文中采用了conv-BN-激活函数的组合。图像分类中所有聚合结点采用1x1卷积，分割任务中，额外一个IDA用来特征插值，此时采用3x3卷积。</p><p>由于残差连接的有效性，本文聚合结点也采用了残差连接，这能保证梯度不会消失和爆炸。基础聚合函数定义如下，其中$\sigma$是非线性激活函数，$w$和$b$是卷积核参数。</p><script type="math/tex; mode=display">N\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\sigma\left(\text { BatchNorm }\left(\sum_{i} W_{i} \mathbf{x}_{i}+\mathbf{b}\right)\right)</script><p>包含残差连接的聚合函数变为下式。</p><script type="math/tex; mode=display">N\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right)=\sigma\left(\text { Batch } \operatorname{Norm}\left(\sum_{i} W_{i} \mathbf{x}_{i}+\mathbf{b}\right)+\mathbf{x}_{n}\right)</script><p><strong>块和层（Blocks and Stages）</strong></p><p>DLA是一系列适用于各种backbone的结构，它对block和stage内部结构没有要求。论文主要实验了三种残差块，分别如下。</p><ul><li>Basic blocks：将堆叠的卷积层通过一个跳跃连接连接起来；</li><li>Bottleneck blocks：通过1x1卷积对堆叠的卷积层进行降维来进行正则化；</li><li>Split blocks：将不同的通道分组到不同的路径（称为cardinality split）来对特征图进行分散。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者设计了分类网络和密集预测网络，前者基于ResNet等修改设计了如下的DLA网络，下表是设计的一系列网络的配置。</p><p><img src="https://i.loli.net/2020/10/22/kJOICgQwhpxYnKD.png" alt=""></p><p><img src="https://i.loli.net/2020/10/22/xNY5ERprfaWTnMm.png" alt=""></p><p>DLA网络在Imagenet上对比其他的紧凑网络，结果如下，从精度和参数量来看，性能都是卓越的。</p><p><img src="https://i.loli.net/2020/10/22/U4E2mg5ZM8GITuy.png" alt=""></p><p>作者还在检测等任务上也做了实验，这里我就不分析了，感兴趣的可以去阅读原论文。</p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>在很多人还在致力于研究如何设计更深更宽的网络的时候，DLA 想到的问题是如何更好地聚合一个网络中不同层和块的信息，并开创性地提出了 IDA 和 HDA 两种聚合思路。在多类任务上效果都有明显改善，包括分类、分割、细粒度分类等。而且，相比于普通 backbone，DLA 结构的网络参数量更少（并不意味着运算速度快，因为这种跳跃连接结构是很耗内存的），在原始网络上进行 DLA 改进往往能获得更为不错的效果。</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DLA论文解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Plotly基础教程</title>
      <link href="2020/10/09/plotly/"/>
      <url>2020/10/09/plotly/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Plotly 是一个非常强大的开源数据可视化框架，它通过构建基于 HTML 的交互式图表来显示信息，可创建各种形式的精美图表。本文所说的 Plotly 指的是 Plotly.js 的 Python 封装，<a href="https://plotly.com/">plotly</a>本身是个生态非常复杂的绘图工具，它对很多编程语言提供接口。交互式和美观易用应该是 Plotly 最大的优势，而 Matplotlib 的特点则是可定制化程度高，但语法也相对难学，各有优缺点。</p><h2 id="安装及开发工具"><a href="#安装及开发工具" class="headerlink" title="安装及开发工具"></a>安装及开发工具</h2><p>安装通过 PIP 进行即可。</p><p><code>pip install plotly</code></p><p>Plotly Python 其对应的<a href="https://plotly.com/python/">官网</a>，上面有一些教程和官方API接口的查询。</p><p><img src="https://i.loli.net/2020/10/09/BHfGe7nIWZ49oVt.png" alt=""></p><p><strong>上面说了 Plotly 是基于 HTML 显示的，所以这里推荐使用 Jupyter lab（Jupyter notebook 也行）作为开发工具，Jupyter lab 的安装本文不多提及，可以自行查找。尤其注意的是，Plotly 主要维护 Jupyter notebook，所以对 Jupyter lab 支持不是很好，绘图无法显示，最新版 Plotly 需要通过命令<code>conda install nodejs</code>和<code>jupyter labextension install jupyterlab-plotly@4.11.0</code>安装支持插件。</strong></p><h2 id="Plotly-生态"><a href="#Plotly-生态" class="headerlink" title="Plotly 生态"></a>Plotly 生态</h2><ul><li>Plotly 是绘图基础库，它可以深度定制调整绘图，但是 API 复杂学习成本较高。</li><li>Plotly_exprress 则是对 Plotly 的高级封装，上手容易，它对 Plotly 的常用绘图函数进行了封装。缺点是没有 plotly 那样自由度高，个人感觉类似 Seaborn 和 Matplotlib 的关系。<strong>本文不以express为主。</strong></li><li>Dash 用于创建交互式绘图工具，可以方便地用它来探索数据，其绘图基于 Plotly。使用 Dash 需要注册并购买套餐，也就是常说的“在线模式”，一般，我们在 Jupyter 内本地绘图就够用了，这是“离线模式”。</li></ul><h2 id="绘图教程"><a href="#绘图教程" class="headerlink" title="绘图教程"></a>绘图教程</h2><p>下面涉及到的内容均可以在<a href="https://plotly.com/python/">官方文档</a>找到参考，下面的内容也只涉及基础的图形绘制（使用Plotly实现），一些比较基础的图形库知识查看<a href="https://plotly.com/python/plotly-fundamentals/">对应教程</a>。</p><h3 id="基本图表"><a href="#基本图表" class="headerlink" title="基本图表"></a>基本图表</h3><p>在 Plotly 中，预定义了如下的一些基本图表，包括散点图、折线图、柱状图、饼图等，它们的使用方式都是类似的，通过向Figure上添加绘图对象进行绘图，而向绘图对象传递的就是其需要的格式的数据。</p><p><img src="https://i.loli.net/2020/10/09/HDkcCSLxO1Q83eo.png" alt=""></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npN <span class="token operator">=</span> <span class="token number">1000</span>t <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>t<span class="token punctuation">)</span>fig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span>data<span class="token operator">=</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>t<span class="token punctuation">,</span> y<span class="token operator">=</span>y<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面的代码就是简单散点图的绘制，其在Jupyter中的执行结果如下图，不妨来仔细看看这张图，<strong>在绘制的图形右上角有一行菜单栏且这个图形是可交互的</strong>（包括缩放、旋转、裁剪等），右上角的菜单包括图像下载、缩放、裁剪、在dash中编辑等。</p><p><img src="https://i.loli.net/2020/10/09/VrR52bqpQA4yzea.png" alt=""></p><p>也可以通过<code>add_trace</code>来逐个添加绘图对象</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token comment"># Create random data with numpy</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npnp<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>N <span class="token operator">=</span> <span class="token number">100</span>random_x <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span>random_y0 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">5</span>random_y1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">)</span>random_y2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">5</span>fig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># Add traces</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>random_x<span class="token punctuation">,</span> y<span class="token operator">=</span>random_y0<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'markers'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>random_x<span class="token punctuation">,</span> y<span class="token operator">=</span>random_y1<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'lines+markers'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'lines+markers'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>random_x<span class="token punctuation">,</span> y<span class="token operator">=</span>random_y2<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'lines'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'lines'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/10/09/2n6AJes5rNKqtiH.png" alt=""></p><p>其他基本图表类似，传入指定格式的数据即可。</p><h3 id="统计图表"><a href="#统计图表" class="headerlink" title="统计图表"></a>统计图表</h3><p>很多统计学图表也预先定义在了Plotly中，主要包括下图所示的箱型图、直方图、热力图、等高线图等。</p><p><img src="https://i.loli.net/2020/10/09/GpthdgN5PvaOEyz.png" alt=""></p><p>和上面的基本图表类似，绘图方式是固定的，只是绘图对象改变了而已。下面的代码就是直方图绘制</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> gofig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Bar<span class="token punctuation">(</span>    name<span class="token operator">=</span><span class="token string">'Control'</span><span class="token punctuation">,</span>    x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Trial 1'</span><span class="token punctuation">,</span> <span class="token string">'Trial 2'</span><span class="token punctuation">,</span> <span class="token string">'Trial 3'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    error_y<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span> array<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_trace<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Bar<span class="token punctuation">(</span>    name<span class="token operator">=</span><span class="token string">'Experimental'</span><span class="token punctuation">,</span>    x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Trial 1'</span><span class="token punctuation">,</span> <span class="token string">'Trial 2'</span><span class="token punctuation">,</span> <span class="token string">'Trial 3'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    error_y<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'data'</span><span class="token punctuation">,</span> array<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>update_layout<span class="token punctuation">(</span>barmode<span class="token operator">=</span><span class="token string">'group'</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/10/09/rwXYOZnCmokGjaI.png" alt=""></p><h3 id="AI图表"><a href="#AI图表" class="headerlink" title="AI图表"></a>AI图表</h3><p>同时，Plotly也支持绘制一些简单的机器学习图表，不过都是依靠上面的基本图表实现的，如下述的线性回归。</p><p><img src="https://i.loli.net/2020/10/09/UIqydzwFvbRK5Q4.png" alt=""></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> plotly<span class="token punctuation">.</span>express <span class="token keyword">as</span> px<span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegressiondf <span class="token operator">=</span> px<span class="token punctuation">.</span>data<span class="token punctuation">.</span>tips<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> df<span class="token punctuation">.</span>total_bill<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>model <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> df<span class="token punctuation">.</span>tip<span class="token punctuation">)</span>x_range <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>X<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>y_range <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_range<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig <span class="token operator">=</span> px<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>df<span class="token punctuation">,</span> x<span class="token operator">=</span><span class="token string">'total_bill'</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token string">'tip'</span><span class="token punctuation">,</span> opacity<span class="token operator">=</span><span class="token number">0.65</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>add_traces<span class="token punctuation">(</span>go<span class="token punctuation">.</span>Scatter<span class="token punctuation">(</span>x<span class="token operator">=</span>x_range<span class="token punctuation">,</span> y<span class="token operator">=</span>y_range<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Regression Fit'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/10/09/Q12kFUBOcmKgqX5.png" alt=""></p><h3 id="科学绘图"><a href="#科学绘图" class="headerlink" title="科学绘图"></a>科学绘图</h3><p>下面的这些数据科学领域用的挺多的图也做了封装，例如下面的代码就是绘制heatmap的样例。</p><p><img src="https://i.loli.net/2020/10/09/2tSgERp8NkrxJDi.png" alt=""></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> gofig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span>data<span class="token operator">=</span>go<span class="token punctuation">.</span>Heatmap<span class="token punctuation">(</span>                   z<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   x<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Monday'</span><span class="token punctuation">,</span> <span class="token string">'Tuesday'</span><span class="token punctuation">,</span> <span class="token string">'Wednesday'</span><span class="token punctuation">,</span> <span class="token string">'Thursday'</span><span class="token punctuation">,</span> <span class="token string">'Friday'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   y<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'Morning'</span><span class="token punctuation">,</span> <span class="token string">'Afternoon'</span><span class="token punctuation">,</span> <span class="token string">'Evening'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                   hoverongaps <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/10/09/ElyNPz96JOrmnuF.png" alt=""></p><h3 id="三维绘图"><a href="#三维绘图" class="headerlink" title="三维绘图"></a>三维绘图</h3><p>Plotly的三维绘图真的很好看，而且其是可交互的，非常方便，例如下面的3D曲面图。</p><p><img src="https://i.loli.net/2020/10/09/aP16hAGqZrT4C8p.png" alt=""></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> plotly<span class="token punctuation">.</span>graph_objects <span class="token keyword">as</span> go<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token comment"># Read data from a csv</span>z_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'https://raw.githubusercontent.com/plotly/datasets/master/api_docs/mt_bruno_elevation.csv'</span><span class="token punctuation">)</span>fig <span class="token operator">=</span> go<span class="token punctuation">.</span>Figure<span class="token punctuation">(</span>data<span class="token operator">=</span><span class="token punctuation">[</span>go<span class="token punctuation">.</span>Surface<span class="token punctuation">(</span>z<span class="token operator">=</span>z_data<span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>update_layout<span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">'test'</span><span class="token punctuation">,</span> autosize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>                  width<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> height<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>                  margin<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>l<span class="token operator">=</span><span class="token number">65</span><span class="token punctuation">,</span> r<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> b<span class="token operator">=</span><span class="token number">65</span><span class="token punctuation">,</span> t<span class="token operator">=</span><span class="token number">90</span><span class="token punctuation">)</span><span class="token punctuation">)</span>fig<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/10/09/DT2Oz5plMsvkCFJ.png" alt=""></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文简单介绍了Plotly的基本绘图方式，其实只要了解了Plotly的生态，使用它并不难，更多的子图、标注等技巧本文没有涉及，还是建议到<a href="https://plotly.com/python/">官网教程</a>查看，非常易读。</p>]]></content>
      
      
      <categories>
          
          <category> 数据可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Plotly教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BorderDet 论文解读</title>
      <link href="2020/10/04/borderdet/"/>
      <url>2020/10/04/borderdet/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>目前密集目标检测器很受欢迎，其速度很快且精度不低，不过这种这种基于点的特征虽然使用方便，但会缺少关键的边界信息。旷视于 ECCV2020 发表的这篇 BorderDet，其中的核心就是设计了 Border Align 操作来从边界极限点提取边界特征用于加强点的特征。以此为基础设计了 BorderDet 框架，该框架依据 FCOS 的 baseline 插入 Border Align 构成，其在多个数据集上涨点明显。Border Align 是适用于几乎所有基于点的密集目标检测算法的即插即用模块。</p><ul><li><p>论文地址</p><p><a href="https://arxiv.org/abs/2007.11056">https://arxiv.org/abs/2007.11056</a></p></li><li><p>论文源码</p><p><a href="https://github.com/Megvii-BaseDetection/BorderDet">https://github.com/Megvii-BaseDetection/BorderDet</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>目前大多数 point-based 的目标检测算法（如 SSD、RetinaNet、FCOS 等方法）都使用特征图上的 single-point 进行目标的回归和分类，但是，single-point 特征没有足够的信息表示一个目标实例，主要是因为缺乏边界信息。此前有很多方法来补充 single-point 的表示能力，但是这些方法往往带来较大计算量的同时并没有引入太多有用的信息，反而带来一些无用的背景信息。这篇文章设计了新的特征提取操作 BorderAlign 来直接利用边界特征优化 single-point 特征，以 BorderAlign 为拓展配合作为 baseline 的 FCOS，提出新的检测框架 BorderDet，实现 SOTA。</p><p>本文的贡献文中列了不少，但在我看来，只有一个核心：<strong>分析密集目标检测器的特征表示，发现边界信息对 single-point 特征的重要性，并设计了一个高效的边界特征提取器 BorderAlign。</strong> 其他的贡献都是顺理成章的附属产物。</p><h2 id="BorderAlign"><a href="#BorderAlign" class="headerlink" title="BorderAlign"></a>BorderAlign</h2><p><img src="https://i.loli.net/2020/10/04/BAv3C6PYKR17f4c.png" alt=""></p><p>BorderAlign 的提出是基于大量的实验对比的，我这边就按照作者的思路来进行阐述。首先，采用如上图不同的特征增强方式在 FCOS 的基础上评估效果，结果如下表，根据效果最好的二四两行，发现，只使用边界上中心点做增强效果媲美 region-based 的方法。因此，得出结论，<strong>point-based 方法做目标检测确实缺乏完整的目标特征，但从完整的边界框中密集提取特征是没必要且冗余的，高效的边界特征提取策略可以获得更好的特征增强效果。</strong></p><p><img src="https://i.loli.net/2020/10/04/cLVCn1SkRBZXlTK.png" alt=""></p><p>针对上述结论，一种高效显式自适应提取边界特征的方法，BorderAlign 被提出。如下图所示，一个$5C$的 border-sensitive 特征图作为输入，其中$4C$维度对应边界框的四条边，另外$C$维度对应原始 anchor 点的特征。对于一个 anchor 点预测的边界框，对其四个边界在特征图上的特征做池化操作，由于框的位置是小数，所以采用双线性插值取边界特征。</p><p>这里具体的实现如下：假设输入的 5 个通道表示(single point, left border, top border, right border, bottom border)，那么对 anchor 点$(i, j)$对应的 bbox 各边均匀采样$N$个点，$N$默认是 5，如下图所示。采样点的值采用上面所说的双线性插值，然后通过逐通道最大池化得到输出，每个边只会输出值最大的采样点，那么每个 anchor 点最后采用 5 个点的特征作为输出，所以输出也是$5C$维度的。</p><p><img src="https://i.loli.net/2020/10/04/GvkmthPsnaNMyWg.png" alt=""></p><p>输出特征图相对输入特征图，各通道计算式如下，$(x_0, y_0, x_!, y_1)$为 anchor 点预测的 bbox。</p><p><img src="https://i.loli.net/2020/10/04/UqBdCvYag6FJi7R.png" alt=""></p><p>显然，BorderAlign 是一种自适应的通过边界极限点得到边界特征的方法。文章中对其进行了一些可视化工作，下图所示的边上的小圆圈是边界极限点，大圆圈是不同 channel 上预测的边界极限点。</p><p><img src="https://i.loli.net/2020/10/04/JwlLQHDmBPtZvsE.png" alt=""></p><h2 id="BAM-Border-Alignment-Module"><a href="#BAM-Border-Alignment-Module" class="headerlink" title="BAM(Border Alignment Module)"></a>BAM(Border Alignment Module)</h2><p><img src="https://i.loli.net/2020/10/04/d1p4NU2JvhKrEVZ.png" alt=""></p><p>该模块用于修正粗糙的 detection 结果，因而必须保证输入输出是同维张量，而其中的 BorderAlign 需求的是 5 个通道，所以必然要经历<strong>降维、特征增强、升维</strong>的过程，为了验证 border feature 的效果，BAM 采用 1x1 卷积实现维度变换。</p><h2 id="BorderDet"><a href="#BorderDet" class="headerlink" title="BorderDet"></a>BorderDet</h2><p><img src="https://i.loli.net/2020/10/04/8QERAPL2jxT3zYm.png" alt=""></p><p>上图的框架采用 FCOS 作为 baseline，上面是分类分支，下面是回归分支，coarse cls score 和 coarse box reg 表示 FCOS 的输出。在四个卷积层后引出一个分支做 BorderAlign 操作，也就是进入 BAM 模块，该模块需要 bbox 位置信息，所以看到 coarse box reg 送入两个 BAM 中。最终这两个 BAM 预测得到 border cls score 和 border box reg，和检测器原始输出组合变为最终输出。</p><p>最后补充一点，BorderDet 在推理时对两种分类结果进行直接的相乘输出，而对于 bbox 定位则使用 border 定位预测对初步定位的 bbox 进行原论文中公式(2)的反向转换，对所有的结果进行 NMS 输出（IOU 阈值设置为 0.6）。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>论文进行了非常丰富的消融实验以对比 BorderAlign 的效果。</p><h3 id="各分支效果"><a href="#各分支效果" class="headerlink" title="各分支效果"></a>各分支效果</h3><p>对比两个 BAM 模块有无的实际效果，发现两个分支效果提升差不多，都是 1.1 左右。<br><img src="https://i.loli.net/2020/10/04/bGoiQs1RwuaUyFS.png" alt=""></p><h3 id="相比其他特征增强效果"><a href="#相比其他特征增强效果" class="headerlink" title="相比其他特征增强效果"></a>相比其他特征增强效果</h3><p>和其他经典的特征增强手段相比，BorderAlign 在速度（使用 CUDA 实现了 BorderAlign）和精度上都有突破。<br><img src="https://i.loli.net/2020/10/04/PKk3SOw6Np2IoM8.png" alt=""></p><h3 id="集成到检测器涨点效果"><a href="#集成到检测器涨点效果" class="headerlink" title="集成到检测器涨点效果"></a>集成到检测器涨点效果</h3><p>有比较明显的改进。<br><img src="https://i.loli.net/2020/10/04/tZvEcdHCjyYMqBw.png" alt=""></p><h3 id="和主流检测器对比"><a href="#和主流检测器对比" class="headerlink" title="和主流检测器对比"></a>和主流检测器对比</h3><p>可以看到，即使不使用多尺度策略，BorderDet 和当前 SOTA 相比效果也是不遑多让的。<br><img src="https://i.loli.net/2020/10/04/DmlrKSEqVpw4nhI.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>边界信息对于 OD 问题十分重要，BorderDet 的核心思想 BorderAlign 高效地将边界特征融入到目标预测中，而且能够 PnP 融入到各种 point-based 目标检测算法中以带来较大的性能提升。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Qiu H, Ma Y, Li Z, et al. BorderDet: Border Feature for Dense Object Detection[J]. arXiv preprint arXiv:2007.11056, 2020.</p><p>[2]<a href="https://zhuanlan.zhihu.com/p/163044323">https://zhuanlan.zhihu.com/p/163044323</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BorderDet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matplotlib绘制动图</title>
      <link href="2020/09/21/dynamic-picture/"/>
      <url>2020/09/21/dynamic-picture/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Matplotlib是非常著名的Python绘图库，支持非常复杂的底层定制化操作。本文通过Matplotlib中的动画绘制工具来讲解如何绘制动态图，首先讲解通过交互模式如何显示动态图，继而讲解通过两个动画类来实现动图地保存（GIF格式）。</p><h2 id="显示动态图"><a href="#显示动态图" class="headerlink" title="显示动态图"></a>显示动态图</h2><p>首先，需要明确，Matplotlib绘图有两种显示模式，分别为<strong>阻塞模式</strong>和<strong>交互模式</strong>，他们具体的说明如下。</p><ol><li>阻塞模式，该模式下绘制地图地显示必须使用<code>plt.show()</code>进行展示（默认会弹出一个窗口），代码会运行到该行阻塞不继续执行，直到关闭这个展示（默认是关闭弹出的显示窗口，Pycharm等集成开发环境会自动捕获图片然后跳过阻塞）。</li><li>交互模式，该模式下任何绘图相关的操作如<code>plt.plot()</code>会立即显示绘制的图形然后迅速关闭，继续代码的运行，不发生阻塞。</li></ol><p>默认情况下，Matplotlib使用阻塞模式，要想打开交互模式需要通过下面的几个函数来做操作，下面直接列出要用的核心函数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">plt<span class="token punctuation">.</span>ion<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 打开交互模式</span>plt<span class="token punctuation">.</span>ioff<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 关闭交互模式</span>plt<span class="token punctuation">.</span>clf<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清除当前的Figure对象</span>plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 暂停GUI功能多少秒</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>然后就是要清楚，所谓的动图或者视频是怎么做到的，其实它们本质上就是很多静态图以较快的速度连续播放从而给人一种动感，利用Matplotlib绘制动图的原理也是一样的，遵循<code>画布绘图</code>-&gt;<code>清理画布</code>-&gt;<code>画布绘图</code>的循环就行了，不过这里注意，由于交互模式下绘图都是一闪而过，因此<strong>通过<code>plt.pause(n)</code>暂停GUI显示n秒才能得到连续有显示的图像</strong>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">def</span> <span class="token function">io_test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 生成画布</span>    plt<span class="token punctuation">.</span>ion<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 打开交互模式</span>    <span class="token keyword">for</span> index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        fig<span class="token punctuation">.</span>clf<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清空当前Figure对象</span>        fig<span class="token punctuation">.</span>suptitle<span class="token punctuation">(</span><span class="token string">"3d io pic"</span><span class="token punctuation">)</span>        <span class="token comment"># 生成数据</span>        point_count <span class="token operator">=</span> <span class="token number">100</span>  <span class="token comment"># 随机生成100个点</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        z <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        color <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span>        scale <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>point_count<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span>        ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">,</span> projection<span class="token operator">=</span><span class="token string">"3d"</span><span class="token punctuation">)</span>        <span class="token comment"># 绘图</span>        ax<span class="token punctuation">.</span>scatter3D<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> s<span class="token operator">=</span>scale<span class="token punctuation">,</span> c<span class="token operator">=</span>color<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">"o"</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"X"</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Y"</span><span class="token punctuation">)</span>        ax<span class="token punctuation">.</span>set_zlabel<span class="token punctuation">(</span><span class="token string">"Z"</span><span class="token punctuation">)</span>        <span class="token comment"># 暂停</span>        plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>    <span class="token comment"># 关闭交互模式</span>    plt<span class="token punctuation">.</span>ioff<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    io_test<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上述代码演示了三维空间如何动态显示100个随机点的变化，使用录制软件得到的动图如下，其本质就是不停显示不同的图像而已。</p><p><img src="https://i.loli.net/2020/09/21/lyQHWBXOGv6KSZ3.gif" alt=""></p><h2 id="动图保存"><a href="#动图保存" class="headerlink" title="动图保存"></a>动图保存</h2><p>很多时候我们的需求并不是在窗口中动态显示图像，还需要保存到本地GIF图像，显然使用录制工具是一个比较低效的用法，Matplotlib的<code>animation</code>模块提供了两个动画绘制接口，分别是<strong>FuncAnimation</strong>和<strong>ArtistAnimation</strong>，它们都是继承自<code>TimedAnimation</code>的类，因而也具有<code>Animation</code>对象的通用方法，如<code>Animation.save()</code>和<code>Animation.to_html5_video()</code>两个方法实例化一个<code>Animation</code>对象后均可调用，前者表示将动画保存为一个图像，后者表示将动画表示为一个HTML视频。</p><ul><li>FuncAnimation: 通过反复调用同一更新函数来制作动画。</li><li>ArtistAnimation: 通过调用一个固定的Artist对象来制作动画，例如给定的图片序列或者Matplotlib的绘图对象。</li></ul><p>下面给出上述两个类的构造函数所需参数，它们的主要参数也是类似的，都是一个Figure对象作为画布，然后一个对象作为更新的实现方式（前者需要一个反复绘图的更新函数，后者则为一个图像列表或者绘图对象列表）。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>FuncAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> func<span class="token punctuation">,</span> frames<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> init_func<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> fargs<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> save_count<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> cache_frame_data<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>ArtistAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> artists<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>相比较而言，我更喜欢使用<code>FuncAnimation</code>，它的使用要求简洁且定制化程度较高。但是如果想将很多图片合并为一个动图，那么<code>ArtistAnimation</code>是最合适的选择。</p><p>下面的代码演示了如何保存一个动态变化渲染的柱状图，<code>ArtistAnimation</code>传入了一个图像序列，序列中每个元素为绘制的柱状图。然后通过使用<code>Animation</code>的<code>save</code>方法保存了动态图，**需要注意的是，这里有个动画写入器（writer）可以选择，默认不是pillow，我个人觉得pillow安装简单一些。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>animation <span class="token keyword">as</span> animationfig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token punctuation">)</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> tmp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>    y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>    temp <span class="token operator">=</span> ax<span class="token punctuation">.</span>bar<span class="token punctuation">(</span>x<span class="token punctuation">,</span> height<span class="token operator">=</span>y<span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">)</span>    tmp<span class="token punctuation">.</span>append<span class="token punctuation">(</span>temp<span class="token punctuation">)</span>ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>ArtistAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> tmp<span class="token punctuation">,</span> interval<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> repeat_delay<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>ani<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">"bar.gif"</span><span class="token punctuation">,</span> writer<span class="token operator">=</span><span class="token string">'pillow'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面代码的执行结果如下图。</p><p><img src="https://i.loli.net/2020/09/21/LHoTe14VU2tqlX9.gif" alt=""></p><p>接着，演示使用范围更广的<code>FuncAnimation</code>如何使用。下面的代码中，动态展示了梯度下降在三维图上的优化过程，其中最为核心的代码如下。用于构造<code>Animation</code>对象的除了画布就是一个更新函数，在这个更新函数内部多次绘制散点图从而形成动态效果， <code>frames</code>是帧数，如果设置了这个帧数，那么<code>update</code>函数第一个参数必须有一个<code>num</code>占位，这个<code>num</code>由<code>Animation</code>对象维护，每次内部执行<code>update</code>会自动递增，后面的参数列表<code>fargs</code>只需要传入除了<code>num</code>后面的参数即可。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> ax<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> z<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span>    ax<span class="token punctuation">.</span>scatter3D<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> axani <span class="token operator">=</span> animation<span class="token punctuation">.</span>FuncAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> update<span class="token punctuation">,</span> frames<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> fargs<span class="token operator">=</span><span class="token punctuation">(</span>x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list<span class="token punctuation">,</span> ax3d<span class="token punctuation">)</span><span class="token punctuation">,</span> interval<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> blit<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的代码演示效果如下图，完整的代码附在文末补充说明中。</p><p><img src="https://i.loli.net/2020/09/21/Kp6qMkuLIt3rFwo.gif" alt=""></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文介绍了如何使用Matplotlib绘制动态图，主要通过交互模式和<code>animation</code>模块进行，如果觉得有所帮助，欢迎点赞。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>animation <span class="token keyword">as</span> animation<span class="token keyword">def</span> <span class="token function">GD</span><span class="token punctuation">(</span>x0<span class="token punctuation">,</span> y0<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    f <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">-</span> y <span class="token operator">**</span> <span class="token number">2</span>    g_x <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token number">2</span> <span class="token operator">*</span> x    x<span class="token punctuation">,</span> y <span class="token operator">=</span> x0<span class="token punctuation">,</span> y0    x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>        x_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        z_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>f<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1.01</span><span class="token punctuation">)</span>        grad_x<span class="token punctuation">,</span> grad_y <span class="token operator">=</span> g_x<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> g_x<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        x <span class="token operator">-=</span> lr <span class="token operator">*</span> grad_x        y <span class="token operator">-=</span> lr <span class="token operator">*</span> grad_y        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch{}: grad={} {}, x={}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> grad_x<span class="token punctuation">,</span> grad_y<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>grad_x<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span> <span class="token keyword">and</span> <span class="token builtin">abs</span><span class="token punctuation">(</span>grad_y<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">:</span>            <span class="token keyword">break</span>    <span class="token keyword">return</span> x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list<span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>num<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> ax<span class="token punctuation">)</span><span class="token punctuation">:</span>    x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span><span class="token punctuation">,</span> z<span class="token punctuation">[</span><span class="token punctuation">:</span>num<span class="token punctuation">]</span>    ax<span class="token punctuation">.</span>scatter3D<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> ax<span class="token keyword">def</span> <span class="token function">draw_gd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>    x<span class="token punctuation">,</span> y <span class="token operator">=</span> np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    z <span class="token operator">=</span> x <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">-</span> y <span class="token operator">**</span> <span class="token number">2</span>    ax3d <span class="token operator">=</span> plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span>projection<span class="token operator">=</span><span class="token string">'3d'</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"X"</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Y"</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>set_zlabel<span class="token punctuation">(</span><span class="token string">"Z"</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>tick_params<span class="token punctuation">(</span>labelsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>    ax3d<span class="token punctuation">.</span>plot_surface<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> cstride<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> rstride<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"jet"</span><span class="token punctuation">)</span>    x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list <span class="token operator">=</span> GD<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span>    x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x_list<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>y_list<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>z_list<span class="token punctuation">)</span>    ani <span class="token operator">=</span> animation<span class="token punctuation">.</span>FuncAnimation<span class="token punctuation">(</span>fig<span class="token punctuation">,</span> update<span class="token punctuation">,</span> frames<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> fargs<span class="token operator">=</span><span class="token punctuation">(</span>x_list<span class="token punctuation">,</span> y_list<span class="token punctuation">,</span> z_list<span class="token punctuation">,</span> ax3d<span class="token punctuation">)</span><span class="token punctuation">,</span> interval<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> blit<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    ani<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token string">'test.gif'</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    draw_gd<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matplotlib动态图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DY-ReLU论文解读</title>
      <link href="2020/09/15/dy-relu/"/>
      <url>2020/09/15/dy-relu/</url>
      
        <content type="html"><![CDATA[<h1 id="Dynamic-ReLU"><a href="#Dynamic-ReLU" class="headerlink" title="Dynamic ReLU"></a>Dynamic ReLU</h1><blockquote><p>其实一直在做论文阅读心得方面的工作，只是一直没有分享出来，这篇文章可以说是这个前沿论文解读系列的第一篇文章，希望能坚持下来。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>论文提出了动态线性修正单元（Dynamic Relu，下文简称 DY-ReLU），它能够依据输入动态调整对应分段函数，与 ReLU 及其静态变种相比，仅仅需要增加一些可以忽略不计的参数就可以带来大幅的性能提升，它可以无缝嵌入已有的主流模型中，在轻量级模型（如 MobileNetV2）上效果更加明显。</p><ul><li><p>论文地址</p><p><a href="http://arxiv.org/abs/2003.10027">http://arxiv.org/abs/2003.10027</a></p></li><li><p>论文源码</p><p><a href="https://github.com/Islanna/DynamicReLU">https://github.com/Islanna/DynamicReLU</a></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>ReLU 在深度学习的发展中地位举足轻重，它简单而且高效，极大地提高了深度网络的性能，被很多 CV 任务的经典网络使用。不过 ReLU 及其变种（无参数的 leaky ReLU 和有参数的 PReLU）都是静态的，也就是说他们最终的参数都是固定的。<strong>那么自然会引发一个问题，能否根据输入的数据动态调整 ReLU 的参数呢？</strong></p><p><img src="https://i.loli.net/2020/09/15/NHuT5iLaxc4Drbo.png" alt=""></p><p>针对上述问题，论文提出了 DY-ReLU，它是一个分段函数$f_{\boldsymbol{\theta}(\boldsymbol{x})}(\boldsymbol{x})$，其参数由超函数$\boldsymbol{\theta {(x)}}$根据$x$计算得到。如上图所示，输入$x$在进入激活函数前分成两个流分别输入$\boldsymbol{\theta {(x)}}$和$f_{\boldsymbol{\theta}(\boldsymbol{x})}(\boldsymbol{x})$，前者用于获得激活函数的参数，后者用于获得激活函数的输出值。超函数$\boldsymbol{\theta {(x)}}$能够编码输入$x$的各个维度（对卷积网络而言，这里指的就是通道，所以原文采用 c 来标记）的全局上下文信息来自适应激活函数$f_{\boldsymbol{\theta}(\boldsymbol{x})}(\boldsymbol{x})$。</p><p>该设计能够在引入极少量的参数的情况下大大增强网络的表示能力，本文对于空间和通道上不同的共享机制设计了三种 DY-ReLU，分别是 DY-ReLU-A、DY-ReLU-B 以及 DY-ReLU-C。</p><h2 id="Dynamic-ReLU-1"><a href="#Dynamic-ReLU-1" class="headerlink" title="Dynamic ReLU"></a>Dynamic ReLU</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><img src="https://i.loli.net/2020/09/15/MKfwy57uRIHxFs3.png" alt=""></p><p>原始的 ReLU 为$\boldsymbol{y}=\max {\boldsymbol{x}, 0}$，这是一个非常简单的分段函数。对于输入向量$x$的第$c$个通道的输入$x_c$，对应的激活函数可以记为$y_{c}=\max \left{x_{c}, 0\right}$。进而，ReLU 可以统一表示为带参分段线性函数$y_{c}=\max _{k}\left{a_{c}^{k} x_{c}+b_{c}^{k}\right}$，基于此提出下式动态 ReLU 来针对$\boldsymbol{x}=\left{x_{c}\right}$自适应$a_c^k$和$b_c^k$。</p><script type="math/tex; mode=display">y_{c}=f_{\boldsymbol{\theta}(\boldsymbol{x})}\left(x_{c}\right)=\max _{1 \leq k \leq K}\left\{a_{c}^{k}(\boldsymbol{x}) x_{c}+b_{c}^{k}(\boldsymbol{x})\right\}</script><p>系数$\left(a_{c}^{k}, b_{c}^{k}\right)$由超函数$\boldsymbol{\theta (x)}$计算得到，具体如下，其中$K$为函数的数目，$C$为通道数目。且参数$\left(a_{c}^{k}, b_{c}^{k}\right)$不仅仅与$x_c$有关，还和$x_{j \neq c}$有关。</p><p>$\left[a_{1}^{1}, \ldots, a_{C}^{1}, \ldots, a_{1}^{K}, \ldots, a_{C}^{K}, b_{1}^{1}, \ldots, b_{C}^{1}, \ldots, b_{1}^{K}, \ldots, b_{C}^{K}\right]^{T}=\boldsymbol{\theta}(\boldsymbol{x})$</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>DY-ReLU 的核心超函数$\boldsymbol{\theta {(x)}}$的实现采用 SE 模块（SENet 提出的）实现，对于维度为$C \times H \times W$的张量输入，首先通过一个全局池化层压缩空间信息，然后经过两个中间夹着一个 ReLU 的全连接层，最后一个标准化层用于标准化输出的范围在$(-1,1)$之间（采用 sigmoid<br>函数）。该模块最终输出$2KC$个元素，分别是$a_{1: C}^{1: K}$和$b_{1: C}^{1: K}$的残差，记为$\Delta a_{1: C}^{1: K}$和$\Delta b_{1: C}^{1: K}$，最后的输出为初始值和残差的加权和，计算式如下。</p><script type="math/tex; mode=display">a_{c}^{k}(\boldsymbol{x})=\alpha^{k}+\lambda_{a} \Delta a_{c}^{k}(\boldsymbol{x}), b_{c}^{k}(\boldsymbol{x})=\beta^{k}+\lambda_{b} \Delta b_{c}^{k}(\boldsymbol{x})</script><p>其中，$\alpha^k$和$\beta^k$分别为$a_c^k$和$b_c^k$的初始值，$\lambda_a$和$\lambda_b$为残差范围控制标量，也就是加的权。$\alpha^k$和$\beta^k$以及$\lambda_a$、$\lambda_b$都是超参数。若$K=2$，有$\alpha^{1}=1, \alpha^{2}=\beta^{1}=\beta^{2}=0$，这就是原始 ReLU。默认的$\lambda_a$和$\lambda_b$分别为 1.0 和 0.5。</p><p><img src="https://i.loli.net/2020/09/15/Z9FVGdh1jJiavCy.png" alt=""></p><p>对于学习到不同的参数，DY-ReLU 会有不同的形式，它可以等价于 ReLU、Leaky ReLU 和 PReLU，也可以等价于 SE 模块或者 Maxout 算子，至于具体的形式依据输入而改变，是一种非常灵活的动态激活函数。</p><h3 id="变种设计"><a href="#变种设计" class="headerlink" title="变种设计"></a>变种设计</h3><p>主要提出三种不同的 DY-ReLU 设计，分别是 DY-ReLU-A、DY-ReLU-B 以及 DY-ReLU-C。DY-ReLU-A 空间和通道均共享，只会输出$2K$个参数，计算简单，表达能力较弱；DY-ReLU-B 仅空间上共享，输出$2KC$个参数；DY-ReLU-C 空间和通道均不共享，参数量极大。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>经过对比实验得出 DY-ReLU-B 更适合图像分类，DY-ReLU-C 更适合关键点检测任务，在几个典型网络上改用论文提出的 DY-ReLU，效果如下图，不难发现，在轻量级网络上突破较大。</p><p><img src="https://i.loli.net/2020/09/15/LaHep6XgyjnOFkZ.png" alt=""></p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>下面是 DY-ReLU-B 的 Pytorch 实现。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">class</span> <span class="token class-name">DyReLU</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> conv_type<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>DyReLU<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels        self<span class="token punctuation">.</span>k <span class="token operator">=</span> k        self<span class="token punctuation">.</span>conv_type <span class="token operator">=</span> conv_type        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>conv_type <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'1d'</span><span class="token punctuation">,</span> <span class="token string">'2d'</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels <span class="token operator">//</span> reduction<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channels <span class="token operator">//</span> reduction<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>k<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'lambdas'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token operator">*</span>k <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token operator">*</span>k<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'init_v'</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>k <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_relu_coefs</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv_type <span class="token operator">==</span> <span class="token string">'2d'</span><span class="token punctuation">:</span>            theta <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>theta<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>theta<span class="token punctuation">)</span>        theta <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>theta<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>        <span class="token keyword">return</span> theta    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> NotImplementedError<span class="token keyword">class</span> <span class="token class-name">DyReLUB</span><span class="token punctuation">(</span>DyReLU<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> conv_type<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>DyReLUB<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> reduction<span class="token punctuation">,</span> k<span class="token punctuation">,</span> conv_type<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channels <span class="token operator">//</span> reduction<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>k<span class="token operator">*</span>channels<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>channels        theta <span class="token operator">=</span> self<span class="token punctuation">.</span>get_relu_coefs<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        relu_coefs <span class="token operator">=</span> theta<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>self<span class="token punctuation">.</span>k<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>lambdas <span class="token operator">+</span> self<span class="token punctuation">.</span>init_v        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv_type <span class="token operator">==</span> <span class="token string">'1d'</span><span class="token punctuation">:</span>            <span class="token comment"># BxCxL -&gt; LxBxCx1</span>            x_perm <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            output <span class="token operator">=</span> x_perm <span class="token operator">*</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k<span class="token punctuation">:</span><span class="token punctuation">]</span>            <span class="token comment"># LxBxCx2 -&gt; BxCxL</span>            result <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>conv_type <span class="token operator">==</span> <span class="token string">'2d'</span><span class="token punctuation">:</span>            <span class="token comment"># BxCxHxW -&gt; HxWxBxCx1</span>            x_perm <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>            output <span class="token operator">=</span> x_perm <span class="token operator">*</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>k<span class="token punctuation">]</span> <span class="token operator">+</span> relu_coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k<span class="token punctuation">:</span><span class="token punctuation">]</span>            <span class="token comment"># HxWxBxCx2 -&gt; BxCxHxW</span>            result <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个结构和上文我所说的 SE 模块是大体对应的，目前支持一维和二维卷积，要想使用只需要像下面这样替换激活层即可（DY-ReLU 需要指定输入通道数目和卷积类型）。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">from</span> dyrelu <span class="token keyword">import</span> DyReluB<span class="token keyword">class</span> <span class="token class-name">Model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Model<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> DyReLUB<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> conv_type<span class="token operator">=</span><span class="token string">'2d'</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>有空的话我会在 MobileNet 和 ResNet 上具体实验，看看实际效果是否如论文所述。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文提出了 DY-ReLU，能够根据输入动态地调整激活函数，与 ReLU 及其变种对比，仅需额外的少量计算即可带来大幅的性能提升，能无缝嵌入到当前的主流模型中，是一个涨点利器。本质上，DY-ReLU 就是各种 ReLU 的数学归纳和拓展，这对后来激活函数的研究有指导意义。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Chen Y, Dai X, Liu M, et al. Dynamic ReLU[J]. arXiv:2003.10027 [cs], 2020.</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dynamic ReLU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人脸表情识别</title>
      <link href="2020/08/25/facial-expression-recognition/"/>
      <url>2020/08/25/facial-expression-recognition/</url>
      
        <content type="html"><![CDATA[<h1 id="人脸表情识别"><a href="#人脸表情识别" class="headerlink" title="人脸表情识别"></a>人脸表情识别</h1><blockquote><p>2020.8.22，重构了整个仓库代码，改用Tensorflow2中的keras api实现整个系统。考虑到很多反映jupyter notebook写的train使用起来不太方便，这里改成了py脚本实现。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>使用卷积神经网络构建整个系统，在尝试了Gabor、LBP等传统人脸特征提取方式基础上，深度模型效果显著。在FER2013、JAFFE和CK+三个表情识别数据集上进行模型评估。</p><h2 id="环境部署"><a href="#环境部署" class="headerlink" title="环境部署"></a>环境部署</h2><p>基于Python3和Keras2（TensorFlow后端），具体依赖安装如下(推荐使用conda虚拟环境)。<br></p><pre class="line-numbers language-shell" data-language="shell"><div class="caption"><span>script</span></div><code class="language-shell">git clone https://github.com/luanshiyinyang/FacialExpressionRecognition.gitcd FacialExpressionRecognitionconda create -n FER python=3.6source activate FERconda install cudatoolkit=10.1conda install cudnn=7.6.5pip install -r requirements.txt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>如果你是Linux用户，直接执行根目录下的<code>env.sh</code>即可一键配置环境，执行命令为<code>bash env.sh</code>。<p></p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>数据集和预训练模型均已经上传到百度网盘，<a href="https://pan.baidu.com/s/1LFu52XTMBdsTSQjMIPYWnw">链接</a>给出，提取密码为2pmd。下载后将<code>model.zip</code>移动到根目录下的<code>models</code>文件夹下并解压得到一个<code>*.h5</code>的模型参数文件，将<code>data.zip</code>移动到根目录下的<code>dataset</code>文件夹下并解压得到包含多个数据集压缩文件，均解压即可得到包含图像的数据集（<strong>其中rar后缀的为原始jaffe数据集，这里建议使用我处理好的</strong>）。</p><h2 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a><strong>传统方法</strong></h3><ul><li>数据预处理<ul><li>图片降噪</li><li>人脸检测（HAAR分类器检测（opencv））</li></ul></li><li>特征工程<ul><li>人脸特征提取<ul><li>LBP</li><li>Gabor</li></ul></li></ul></li><li>分类器<ul><li>SVM<h3 id="深度方法"><a href="#深度方法" class="headerlink" title="深度方法"></a><strong>深度方法</strong></h3></li></ul></li><li>人脸检测<ul><li>HAAR分类器</li><li>MTCNN（效果更好）</li></ul></li><li>卷积神经网络<ul><li>用于特征提取+分类</li></ul></li></ul><h2 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h2><p>使用经典的卷积神经网络，模型的构建主要参考2018年CVPR几篇论文以及谷歌的Going Deeper设计如下网络结构，输入层后加入(1,1)卷积层增加非线性表示且模型层次较浅，参数较少（大量参数集中在全连接层）。<br><img src="https://i.loli.net/2020/08/25/TtcFkPSm3vgZbME.png" alt=""></p><p><img src="https://i.loli.net/2020/08/25/faKs6yzbLciUvxm.png" alt=""></p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>主要在FER2013、JAFFE、CK+上进行训练，JAFFE给出的是半身图因此做了人脸检测。最后在FER2013上Pub Test和Pri Test均达到67%左右准确率（该数据集爬虫采集存在标签错误、水印、动画图片等问题），JAFFE和CK+5折交叉验证均达到99%左右准确率（这两个数据集为实验室采集，较为准确标准）。</p><p>执行下面的命令将在指定的数据集（fer2013或jaffe或ck+）上按照指定的batch_size训练指定的轮次。训练会生成对应的可视化训练过程，下图为在三个数据集上训练过程的共同绘图。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python src/train.py --dataset fer2013 --epochs 300 --batch_size 32<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://i.loli.net/2020/08/25/urIbfVFaGiz5EdW.png" alt=""></p><h2 id="模型应用"><a href="#模型应用" class="headerlink" title="模型应用"></a>模型应用</h2><p>与传统方法相比，卷积神经网络表现更好，使用该模型构建识别系统，提供<strong>GUI界面和摄像头实时检测</strong>（摄像必须保证补光足够）。预测时对一张图片进行水平翻转、偏转15度、平移等增广得到多个概率分布，将这些概率分布加权求和得到最后的概率分布，此时概率最大的作为标签（也就是使用了推理数据增强）。</p><h3 id="GUI界面"><a href="#GUI界面" class="headerlink" title="GUI界面"></a><strong>GUI界面</strong></h3><p>注意，<strong>GUI界面预测只显示最可能是人脸的那个脸表情，但是对所有检测到的人脸都会框定预测结果并在图片上标记，标记后的图片在output目录下。</strong></p><p>执行下面的命令即可打开GUI程序，该程序依赖PyQT设计，在一个测试图片（来源于网络）上进行测试效果如下图。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python src/gui.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://i.loli.net/2020/08/25/f9VxPwk1dGJ8NKC.png" alt=""></p><p>上图的GUI反馈的同时，会对图片上每个人脸进行检测并表情识别，处理后如下图。</p><p><img src="https://i.loli.net/2020/08/25/BmxSpOt1X4RVEUe.png" alt=""></p><h3 id="实时检测"><a href="#实时检测" class="headerlink" title="实时检测"></a><strong>实时检测</strong></h3><p>实时检测基于Opencv进行设计，旨在用摄像头对实时视频流进行预测，同时考虑到有些人的反馈，当没有摄像头想通过视频进行测试则修改命令行参数即可。</p><p>使用下面的命令会打开摄像头进行实时检测（ESC键退出），若要指定视频进行进行检测，则使用下面的第二个命令。<br></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python src/recognition_camera.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python src/recognition_camera.py --source 1 --video_path 视频绝对路径或者相对于该项目的根目录的相对路径<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>下图是动态演示的在Youtube上<a href="https://www.youtube.com/watch?v=r5Z741PC9_c">某个视频</a>上的识别结果。<p></p><p><img src="https://i.loli.net/2020/08/25/sCyxpwNifDJLEd9.gif" alt=""></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>具体项目代码、数据集、模型已经开源于<a href="https://github.com/luanshiyinyang/FacialExpressionRecognition.git">我的Github</a>，欢迎Star或者Fork。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人脸表情识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv5自定义训练</title>
      <link href="2020/08/20/yolo5-train/"/>
      <url>2020/08/20/yolo5-train/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLOv5自定义数据集训练"><a href="#YOLOv5自定义数据集训练" class="headerlink" title="YOLOv5自定义数据集训练"></a>YOLOv5自定义数据集训练</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文介绍如何在自己的VOC格式数据集上训练YOLO5目标检测模型。</p><h2 id="VOC数据集格式"><a href="#VOC数据集格式" class="headerlink" title="VOC数据集格式"></a>VOC数据集格式</h2><p>首先，先来了解一下<a href="http://host.robots.ox.ac.uk/pascal/VOC/">Pascal VOC数据集</a>的格式，该数据集油5个部分组成，文件组织结构如下，目前主要的是VOC2007和VOC2012.</p><pre class="line-numbers language-none"><code class="language-none">- VOC    - JPEGImages        - 1.jpg        - 2.jpg        - ...    - Annotations        - 1.xml        - 2.xml        - ...    - ImageSets        - Main            - train.txt            - val.txt            - test.txt            - trainval.txt        - ...    - SegmentationClass    - SegmentationObject<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>第一个文件夹<strong>JPEGImages</strong>为所有的图像，也就是说，训练集、验证集和测试集需要自己划分；<strong>Annotations</strong>为JPEGImages文件夹中每个图片对应的标注，xml格式文件，文件名与对应图像相同；<strong>ImageSets</strong>主要的子文件夹为Main，其中有四个文本文件，为训练集、验证集、测试集和训练验证集的图片文件名；<strong>SegmentationClass</strong>和<strong>SegmentationObject</strong>文件夹存放分割的结果图，前者为语义分割，后者为实例分割。</p><p>上述xml标注文件，格式如下。对其具体标注解释。<br></p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>annotation</span><span class="token punctuation">&gt;</span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>folder</span><span class="token punctuation">&gt;</span></span>down<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>folder</span><span class="token punctuation">&gt;</span></span> # 图片所处文件夹  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>filename</span><span class="token punctuation">&gt;</span></span>1.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>filename</span><span class="token punctuation">&gt;</span></span> # 图片文件名及后缀  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>path</span><span class="token punctuation">&gt;</span></span>./savePicture/train_29635.jpg<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>path</span><span class="token punctuation">&gt;</span></span> # 存放路径  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>source</span><span class="token punctuation">&gt;</span></span>  #图源信息    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>database</span><span class="token punctuation">&gt;</span></span>Unknown<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>database</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>source</span><span class="token punctuation">&gt;</span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>size</span><span class="token punctuation">&gt;</span></span> # 图片尺寸和通道    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>width</span><span class="token punctuation">&gt;</span></span>640<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>width</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>height</span><span class="token punctuation">&gt;</span></span>480<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>height</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>depth</span><span class="token punctuation">&gt;</span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>depth</span><span class="token punctuation">&gt;</span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>size</span><span class="token punctuation">&gt;</span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>segmented</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>segmented</span><span class="token punctuation">&gt;</span></span>  #是否有分割label，0无1有  # 图像中包含的所有目标，一个目标一个object标签  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>car<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>  # 目标类别    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">&gt;</span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">&gt;</span></span>  # 目标的姿态    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">&gt;</span></span>  # 目标是否被部分遮挡（&gt;15%）    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">&gt;</span></span>  # 是否为难以辨识的目标， 需要结合背景才能判断出类别的物体    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">&gt;</span></span>  # 目标边界框信息      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">&gt;</span></span>2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">&gt;</span></span>156<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">&gt;</span></span>111<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">&gt;</span></span>259<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">&gt;</span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">&gt;</span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>multi_signs<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>editType</span> <span class="token punctuation">/&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pose</span><span class="token punctuation">&gt;</span></span>Unspecified<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pose</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>truncated</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>truncated</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>difficult</span><span class="token punctuation">&gt;</span></span>0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>difficult</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>bndbox</span><span class="token punctuation">&gt;</span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmin</span><span class="token punctuation">&gt;</span></span>81<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmin</span><span class="token punctuation">&gt;</span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymin</span><span class="token punctuation">&gt;</span></span>98<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymin</span><span class="token punctuation">&gt;</span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xmax</span><span class="token punctuation">&gt;</span></span>154<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xmax</span><span class="token punctuation">&gt;</span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ymax</span><span class="token punctuation">&gt;</span></span>243<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>ymax</span><span class="token punctuation">&gt;</span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>bndbox</span><span class="token punctuation">&gt;</span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>annotation</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><strong>也就是说，遇到这种文件格式的数据（主要特点为图像全放在一个文件夹，标注格式如上等），将其作为VOC格式的数据集，将自己的数据集重构为VOC格式以便开源项目的处理。</strong><p></p><h2 id="自定义训练"><a href="#自定义训练" class="headerlink" title="自定义训练"></a>自定义训练</h2><h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a><strong>下载源码</strong></h3><p>通过<code>git clone git@github.com:ultralytics/yolov5.git</code>将YOLOv5源码下载到本地，本文后面的内容也可以参考<a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">官方的自定义数据集训练教程</a>，不同于我的教程，该教程全面包含了VOC格式和COCO格式数据集的处理方法。</p><p>此时创建虚拟环境，并通过<code>pip install -r requirements.txt</code>安装依赖包，我这里测试过，最新的项目是兼容Pytorch 1.6的，1.6之前的Pytorch会有一些问题。</p><h3 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a><strong>数据集处理</strong></h3><p>一般，符合VOC格式的数据集至少包含图像和标注两个文件夹，结构如下。我这里假定测试集是独立的，该数据集实际上为训练集，只需要划分出训练集和验证集即可。<strong>这里建议将文件夹重命名如下，否则后续可能出现数据集加载失败的情况。</strong></p><pre class="line-numbers language-none"><code class="language-none">- 根目录    - images    - Annotations<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>下面，编写脚本划分数据集，<code>split_train_val.py</code>脚本内容如下（参考Github上的开源脚本），只需要执行<code>python split_train_val.py --xml_path dataset_root/Annotations/ --txt_path dataset_root/anno_txt/</code>就得到了划分结果的文件列表，如训练集对应的<code>train.txt</code>如下图，里面与训练图片所有的文件名。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> random<span class="token keyword">import</span> argparseparser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--xml_path'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'input xml label path'</span><span class="token punctuation">)</span>parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--txt_path'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'output txt label path'</span><span class="token punctuation">)</span>opt <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>trainval_percent <span class="token operator">=</span> <span class="token number">1.0</span>train_percent <span class="token operator">=</span> <span class="token number">0.8</span>xmlfilepath <span class="token operator">=</span> opt<span class="token punctuation">.</span>xml_pathtxtsavepath <span class="token operator">=</span> opt<span class="token punctuation">.</span>txt_pathtotal_xml <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>xmlfilepath<span class="token punctuation">)</span><span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>txtsavepath<span class="token punctuation">)</span><span class="token punctuation">:</span>    os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>txtsavepath<span class="token punctuation">)</span>num <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>total_xml<span class="token punctuation">)</span>list_index <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num<span class="token punctuation">)</span>tv <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>num <span class="token operator">*</span> trainval_percent<span class="token punctuation">)</span>tr <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>tv <span class="token operator">*</span> train_percent<span class="token punctuation">)</span>trainval <span class="token operator">=</span> random<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>list_index<span class="token punctuation">,</span> tv<span class="token punctuation">)</span>train <span class="token operator">=</span> random<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>trainval<span class="token punctuation">,</span> tr<span class="token punctuation">)</span>file_trainval <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/trainval.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>file_test <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/test.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>file_train <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/train.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>file_val <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>txtsavepath <span class="token operator">+</span> <span class="token string">'/val.txt'</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> list_index<span class="token punctuation">:</span>    name <span class="token operator">=</span> total_xml<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'\n'</span>    <span class="token keyword">if</span> i <span class="token keyword">in</span> trainval<span class="token punctuation">:</span>        file_trainval<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>        <span class="token keyword">if</span> i <span class="token keyword">in</span> train<span class="token punctuation">:</span>            file_train<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            file_val<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        file_test<span class="token punctuation">.</span>write<span class="token punctuation">(</span>name<span class="token punctuation">)</span>file_trainval<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>file_train<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>file_val<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>file_test<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://i.loli.net/2020/08/20/HY5yUEdALgmVthw.png" alt=""></p><p>接下来，我们要做的就是每个xml标注提取bbox信息为txt格式（这个称为yolo_txt格式），每个图像对应一个txt文件，文件每一行为一个目标的信息，包括<code>类别 xmin xmax ymin ymax</code>。使用的脚本<code>voc_label.py</code>内容如下（<strong>注意，类别要替换为当前数据集的类别列表</strong>），<strong>在数据集根目录（此时包含Annotations、anno_txt以及images三个文件夹的目录）下执行该脚本</strong>，如<code>python ../../utils/voc_label.py</code>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># -*- coding: utf-8 -*-</span><span class="token keyword">import</span> xml<span class="token punctuation">.</span>etree<span class="token punctuation">.</span>ElementTree <span class="token keyword">as</span> ET<span class="token keyword">import</span> os<span class="token keyword">from</span> os <span class="token keyword">import</span> getcwdsets <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'val'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span>classes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'window_shielding'</span><span class="token punctuation">,</span> <span class="token string">'multi_signs'</span><span class="token punctuation">,</span> <span class="token string">'non_traffic_signs'</span><span class="token punctuation">]</span>abs_path <span class="token operator">=</span> os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">convert</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> box<span class="token punctuation">)</span><span class="token punctuation">:</span>    dw <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> <span class="token punctuation">(</span>size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    dh <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token operator">/</span> <span class="token punctuation">(</span>size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    x <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span> <span class="token operator">-</span> <span class="token number">1</span>    y <span class="token operator">=</span> <span class="token punctuation">(</span>box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2.0</span> <span class="token operator">-</span> <span class="token number">1</span>    w <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    h <span class="token operator">=</span> box<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> box<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>    x <span class="token operator">=</span> x <span class="token operator">*</span> dw    w <span class="token operator">=</span> w <span class="token operator">*</span> dw    y <span class="token operator">=</span> y <span class="token operator">*</span> dh    h <span class="token operator">=</span> h <span class="token operator">*</span> dh    <span class="token keyword">return</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> w<span class="token punctuation">,</span> h<span class="token keyword">def</span> <span class="token function">convert_annotation</span><span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">:</span>    in_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'Annotations/%s.xml'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">)</span>    out_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'labels/%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    tree <span class="token operator">=</span> ET<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>in_file<span class="token punctuation">)</span>    root <span class="token operator">=</span> tree<span class="token punctuation">.</span>getroot<span class="token punctuation">(</span><span class="token punctuation">)</span>    size <span class="token operator">=</span> root<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'size'</span><span class="token punctuation">)</span>    w <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>size<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'width'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>    h <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>size<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'height'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span>    <span class="token keyword">for</span> obj <span class="token keyword">in</span> root<span class="token punctuation">.</span><span class="token builtin">iter</span><span class="token punctuation">(</span><span class="token string">'object'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        difficult <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'difficult'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        cls <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text        <span class="token keyword">if</span> cls <span class="token keyword">not</span> <span class="token keyword">in</span> classes <span class="token keyword">or</span> <span class="token builtin">int</span><span class="token punctuation">(</span>difficult<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token keyword">continue</span>        cls_id <span class="token operator">=</span> classes<span class="token punctuation">.</span>index<span class="token punctuation">(</span>cls<span class="token punctuation">)</span>        xmlbox <span class="token operator">=</span> obj<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'bndbox'</span><span class="token punctuation">)</span>        b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'xmin'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'xmax'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'ymin'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span>             <span class="token builtin">float</span><span class="token punctuation">(</span>xmlbox<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'ymax'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>        b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4 <span class="token operator">=</span> b        <span class="token comment"># 标注越界修正</span>        <span class="token keyword">if</span> b2 <span class="token operator">&gt;</span> w<span class="token punctuation">:</span>            b2 <span class="token operator">=</span> w        <span class="token keyword">if</span> b4 <span class="token operator">&gt;</span> h<span class="token punctuation">:</span>            b4 <span class="token operator">=</span> h        b <span class="token operator">=</span> <span class="token punctuation">(</span>b1<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> b4<span class="token punctuation">)</span>        bb <span class="token operator">=</span> convert<span class="token punctuation">(</span><span class="token punctuation">(</span>w<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token punctuation">)</span>        out_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>cls_id<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token keyword">for</span> a <span class="token keyword">in</span> bb<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span>wd <span class="token operator">=</span> getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> image_set <span class="token keyword">in</span> sets<span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">'labels/'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span><span class="token string">'labels/'</span><span class="token punctuation">)</span>    image_ids <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'anno_txt/%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_set<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    list_file <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'%s.txt'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_set<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> image_id <span class="token keyword">in</span> image_ids<span class="token punctuation">:</span>        list_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>abs_path <span class="token operator">+</span> <span class="token string">'/images/%s.jpg\n'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>image_id<span class="token punctuation">)</span><span class="token punctuation">)</span>        convert_annotation<span class="token punctuation">(</span>image_id<span class="token punctuation">)</span>    list_file<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这时候，我们的目标检测数据集就构建完成了，其内容如下，其中labels中为不同图像的标注文件，<code>train.txt</code>等几个根目录下的txt文件为划分后图像所在位置的绝对路径，如<code>train.txt</code>就含有所有训练集图像的绝对路径。</p><p><img src="https://i.loli.net/2020/08/20/NPwkyM3o4T2cn9m.png" alt=""></p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>下面需要两个配置文件用于模型的训练，一个用于数据集的配置，一个用于模型的配置。</p><p>首先是数据集的配置，在根目录下的data目录下新建一个yaml文件，内容如下，首先是训练集和验证集的划分文件，这个文件在上面一节最后生成得到了，然后是目标的类别数目和具体类别列表，这个列表务必和上一节最后<code>voc_label.py</code>中的一致。</p><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">train</span><span class="token punctuation">:</span> dataset/train/train.txt<span class="token key atrule">val</span><span class="token punctuation">:</span> dataset/train/val.txt <span class="token comment"># number of classes</span><span class="token key atrule">nc</span><span class="token punctuation">:</span> <span class="token number">3</span> <span class="token comment"># class names</span><span class="token key atrule">names</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'window_shielding'</span><span class="token punctuation">,</span> <span class="token string">'multi_signs'</span><span class="token punctuation">,</span> <span class="token string">'non_traffic_sign'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后，编辑模型的配置文件，此时需要先在项目根目录下的weights目录下执行其中的download_weights.sh这个shell脚本来下载四种模型的权重。然后，选择一个模型，编辑项目根目录下models目录中选择的模型的配置文件，将第一个参数nc改为自己的数据集类别数即可，例如我使用yolov5x模型，则修改yolov5x.yaml文件。<strong>这里weights的下载可能因为网络而难以进行，我也将其上传到了百度网盘，<a href="链接：https://pan.baidu.com/s/1UQX6URxaJP0ZqALvWpDWkA">地址</a>给出，提取码为vjlx。</strong></p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>此时，可以使用下面的命令进行模型的训练，训练日志默认保存在<code>./runs/</code>下，包括模型参数、Tensorboard记录等。此时TensorBoard以已经默认打开，浏览器访问效果如下图（由于数据量很小，很快过拟合）。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python train.py --img <span class="token number">640</span> --batch <span class="token number">8</span> --epoch <span class="token number">300</span> --data ./data/ads.yaml --cfg ./models/yolov5x.yaml --weights weights/yolov5x.pt --device <span class="token string">'0'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://i.loli.net/2020/08/20/GrLI9OTtZD3fJFH.png" alt=""></p><h3 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h3><p>接着，就是在有标注的测试集或者验证集上进行模型效果的评估，在目标检测中最常使用的指标为mAP。通过下面的命令进行模型测试，由于这是个比赛，测试集没有标注，这里使用验证集作为测试用数据，下述命令只需要指定数据集配置文件和训练结果模型即可。<br></p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python test.py  --data ./data/ads.yaml --weights ./runs/exp0/weights/best.pt --augment<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>不进行测试时数据增强和进行测试时数据增强（TTA）在验证集上的表现分别如下。<br><pre class="line-numbers language-none"><code class="language-none">Class  Images  Targets   P       R      mAP@.5    mAP@.5:.95all    400      970    0.376    0.441     0.35       0.235<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br><pre class="line-numbers language-none"><code class="language-none">Class  Images  Targets    P      R      mAP@.5    mAP@.5:.95all     400     970     0.272   0.532   0.366        0.24<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p></p><h3 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h3><p>最后，模型在没有标注的数据上进行推理，使用下面的命令（该命令中<code>save-txt</code>选项用于生成结果的txt标注文件，不指定则只会生成结果图像）。其中，weights使用最满意的实验即可，source则提供一个包含所有测试图片的文件夹即可。</p><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">python detect.py --weights runs/exp0/weights/best.pt --source ./dataset/test/ --device <span class="token number">0</span> --save-txt<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这样，对每个测试图片会在默认的<code>inference/output</code>文件夹中生成一个同名的txt文件，按照我的需求修改了<code>detect.py</code>文件后，每个txt会生成一行一个目标的信息，信息包括<code>类别序号 置信度 xcenter ycenter w h</code>，后面四个为bbox位置，均未归一化。如下图。</p><p><img src="https://i.loli.net/2020/08/20/l86zj2dw9xHnTFO.png" alt=""></p><p>我这里因为是一个比赛，再将这个txt处理为了json文件。<strong>不论是这里的处理代码还是上面对<code>detect.py</code>修改的代码，都可以在文末给出的Github仓库找到。</strong></p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>本文介绍了如何使用YOLOv5在自己的数据集上进行训练，按部就班地进行了讲解。该项目在YOLOv5地源码基础上修改完成，代码开源于我的Github，欢迎star或者fork。</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> yolov5自定义数据集训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>anchor-free目标检测</title>
      <link href="2020/08/20/anchor-free-detection/"/>
      <url>2020/08/20/anchor-free-detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Anchor-free目标检测"><a href="#Anchor-free目标检测" class="headerlink" title="Anchor-free目标检测"></a>Anchor-free目标检测</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>沿着two-step到one-step，anchor-based到anchor-free的发展路线，如今，目标检测（Object Detection，OD）已经进入anchor-free的时代。这些anchor-free方法因为适用于多目标跟踪等领域，也促使了MOT的飞速发展。本文将沿着anchor-free目标检测发展地路线，简单介绍一下主要地一些方法思路，包括目前关注度较大地<strong>FCOS</strong>和<strong>CenterNet</strong>。</p><h2 id="何为anchor"><a href="#何为anchor" class="headerlink" title="何为anchor"></a>何为anchor</h2><p>在了解anchor-free的方法前，我们得知道什么是anchor，在过去，目标检测通常被建模为对候选框的分类和回归，不过，按照候选区域的产生方式不同，分为二阶段（two-step）检测和单阶段（one-step）检测，前者的候选框通过RPN（区域推荐网络）网络产生proposal，后者通过滑窗产生anchor。</p><p><img src="https://i.loli.net/2020/08/20/keLHXrlPFBnqjAf.png" alt=""></p><p>本文所提到的anchor-free方法则通过完全不同的思路解决目标检测问题，这些思路都没有采用预定义的候选框的概念。这两年，从CornerNet开始，anchor-free的目标检测框架层出不穷，宣告着目标检测迈入anchor-free时代。</p><h2 id="anchor-free发展"><a href="#anchor-free发展" class="headerlink" title="anchor-free发展"></a>anchor-free发展</h2><p>其实anchor-free不是一个很新的概念，最早可以追溯到YOLO算法，这应该是最早的anchor-free模型，而最近的anchor-free方法主要分为<strong>基于密集预测</strong>和<strong>基于关键点估计</strong>两种。</p><h3 id="早期研究"><a href="#早期研究" class="headerlink" title="早期研究"></a>早期研究</h3><p>先是聊一聊目标检测比较古老的研究，分别是Densebox和YOLO，前者发表于2015年9月，后者则开源于2016年。</p><p><strong>Densebox</strong></p><p>首先来聊一聊Densebox，这是地平线的算法工程师黄李超于2015年设计的一个端到端检测框架，对此有专门的<a href="https://zhuanlan.zhihu.com/p/24350950">文章</a>介绍。Densebox是深度学习用于目标检测的开山之作之一，当时已经有不错效果的R-CNN不够直接且高效，因而Densebox作者从OverFeat方法上得到启发：在图像上进行卷积等同于使用滑窗分类，为何不能使用全卷积对整个图像进行目标检测呢？</p><p><img src="https://i.loli.net/2020/08/20/fMqbOVpkK1rFSsE.png" alt=""></p><p>在这个基础上，设计了一套端到端的多任务全卷积模型，如上图所示。该模型可以直接回归出目标出现的置信度和相对位置，同时为了处理遮挡和小目标，引入上采用层融合浅层网络特征，得到更大的尺寸的输出特征图。下图是网络的输入和输出，对每个像素会得到一个5维向量，表示分类置信度和bbox到该pixel的四个距离。</p><p><img src="https://i.loli.net/2020/08/20/sZfUd1VOzECyGXv.png" alt=""></p><p>Densebox的主要贡献有两个：证明了单FCN（全卷积网络）可以实现检测遮挡和不同尺度的目标；在FCN结构中添加少量层引入landmark localization，将landmark heatmap和score map融合能够进一步提高检测性能。</p><p><strong>遗憾的是，当时目标检测的学者们沿着RCNN铺好的路亦步亦趋，现在想想，如果当时，就有足够多的关注给与Densebox，今天的目标检测是否会是全新的局面呢？</strong></p><p><strong>YOLO</strong></p><p>2016年开源的YOLOv1算法，是目前工业界比较关注的算法之一，它开创性地将目标检测中的候选框生成和目标识别通过一步完成，因而论文名为“You only look once”，YOLO模型可以直接从整个图像上得到边界框和对应的置信度。比较详细的理解可以参考<a href="https://zhouchen.blog.csdn.net/article/details/105178437">我之前YOLO算法的文章</a>。</p><p><img src="https://i.loli.net/2020/08/20/whpbPxLcqWmsytH.png" alt=""></p><p>YOLO的最大创新就是速度快，一方面将候选框生成的步骤去除，另一方面，通过多个网格负责目标的检测，大大加快运行速度。</p><blockquote><p>Densebox和YOLO很类似，都可以理解为单阶段检测，不过前者为密集预测，针对每个像素进行预测；后者针对划分得到的网格进行预测。同时，作为anchor-free的两篇开山之作，它们为后来的anchor-free检测提供了很多的思路。</p></blockquote><h3 id="基于密集预测"><a href="#基于密集预测" class="headerlink" title="基于密集预测"></a>基于密集预测</h3><p>沿着上一节YOLO和Densebox的思路，2019年出现了很多以此为基础的目标检测方法，包括FCOS、FSAF以及FoveaBox等等方法。</p><p><strong>FCOS</strong></p><p>这是这两年受到广泛的关注的目标检测算法，一方面它确实是anchor-free系列打破anchor-based精度神话的关键之作，另一方面，业界对这种单阶段高效算法有着巨大的需求。</p><p><img src="https://i.loli.net/2020/08/20/UVEgZsIvuQtBPTm.png" alt=""></p><p>上图是FCOS的pipeline设计图，核心的就是一个特征金字塔和三分支头网络，通过backbone之后对feature map上每一个点进行回归预测，和以往目标检测任务不同的是，<strong>除了分类和回归分支，加入了center-ness以剔除低质量预测，它和分类分支的乘积为最终的置信度。</strong></p><p>FCOS创新点如下:</p><ol><li>突破基于Faster-RCNN修补的思路，开创性地不使用anchor而是直接对每个像素进行预测，并在效果是远超Faster-RCNN。这主要是考虑到anchor地存在会带来大量地超参数，如anchor number等，而且这些anchor要计算和GT地IOU，也是很消耗算力的。</li><li>由于是像素级别的密集预测，因此可以使用分割任务的一些trick并且通过修改目标分支可用于实例分割和关键点检测等任务。</li><li>由于是全卷积网络，拥有很多FCN任务的优势，也可以借用其思想。</li></ol><p><strong>FSAF</strong></p><p><img src="https://i.loli.net/2020/08/20/j6QLvatzU9DmlBn.png" alt=""></p><p>这是一个针对FPN的优化思路，提出FSAF模块，让网络自己学习anchor适配。​在RetinaNet的基础上，FSAF模块引入了2个额外的卷积层，这两个卷积层各自负责anchor-free分支的分类和回归预测。此外，提出了在线特征选择策略，​实例输入到特征金字塔的所有层，然后求得所有anchor-free分支focal loss和IoU loss的和，选择loss最小的特征层来学习实例。训练时，特征根据安排的实例进行更新。推理时，不需要进行特征更新，因为最合适的特征金字塔层必然输出高置信分数。</p><blockquote><p>虽然都是基于密集预测，但相比于YOLO和Densebox，FCOS和FSAF使用FPN进行多尺度预测，此前的方法只有单尺度预测；不过，相比于YOLO这个单分支模型，其他方法都是通过两个子网络来进行分类和回归。</p></blockquote><h3 id="基于关键点估计"><a href="#基于关键点估计" class="headerlink" title="基于关键点估计"></a>基于关键点估计</h3><p>不同于密集预测的思路，以关键点估计为手段，目标检测出现了一条全新的主线，它彻底抛开了区域分类回归思想，主要出现了CornerNet、ExtremeNet以及集大成者的CenterNet，由于有两篇目标检测的文章网络名都是CenterNet，这里特指的是关注度比较高的Objects as points这篇文章。</p><p><strong>CornerNet</strong></p><p>这篇文章是后来很多基于关键点估计处理目标检测的算法基础，它开创性地用一对角点来检测目标。对一幅图像，预测两组heatmap，一组为top-left角点，另一组为bottom-right角点，每组heatmap有类别个通道。下图为框架图。</p><p><img src="https://i.loli.net/2020/08/20/1SjiLUVbJGzXr9o.png" alt=""></p><p><strong>ExtremeNet</strong></p><p>不同于CornerNet使用角点检测目标，ExtremeNet通过极值点和中心点来检测目标，这应该是最大地区别，其他一些关键点估计方面地细节，这里不多提。</p><p><img src="https://i.loli.net/2020/08/20/QuToZCtJUprdxwX.png" alt=""></p><p><strong>CenterNet</strong></p><p>下面来看看关键点估计用于目标检测地集大成者，CenterNet。抛开了传统的边框目标表示方法，将目标检测视为对一个点进行的关键点估计问题。相比较于基于bbox的方法，该模型端到端可微，其简单高效且实时性高。在主流地OD数据集上超越了大部分SOTA方法，且论文称在速度上超越了YOLO3。</p><p><img src="https://i.loli.net/2020/08/20/CIp6humX5QG9dPN.png" alt=""></p><p>通过中心点来表示目标，然后在中心点位置回归出目标的其他属性，这样，目标检测问题变成了一个关键点估计问题。只需要将图像传入全卷积网络，得到热力图，热力图的峰值点就是中心点。这里可以把中心点看做形状未知的锚点。但是该锚点只在位置上，没有尺寸框，没有阈值进行前后景分类；每个目标只会有一个正的锚点，因此不会用到NMS；而且，CenterNet与传统目标检测相比，下采样倍数较低，不需要多重特征图。</p><h2 id="发展思路"><a href="#发展思路" class="headerlink" title="发展思路"></a>发展思路</h2><h3 id="成功原因"><a href="#成功原因" class="headerlink" title="成功原因"></a>成功原因</h3><p>anchor-free能在精度上追赶上anchor-based方法，最大地原因应该归属上面绝大多数方法避不开地FPN（特征金字塔网络），因为在每个位置只预测一个框地前提下，FPN对尺度信息进行了很好地弥补，而Focal loss则对区域地回归有一定辅助效果。</p><h3 id="anchor-free局限性"><a href="#anchor-free局限性" class="headerlink" title="anchor-free局限性"></a>anchor-free局限性</h3><p>当然，anchor-free地目标检测方法也有很大地局限性，这些方法虽然声称精度追上了较好地二阶段方法，但存在一些训练上地细节以及部分不公平地比较。不过，总体来说，速度上地突破还是吸引了很多工业界的关注的。</p><h3 id="GT的设计"><a href="#GT的设计" class="headerlink" title="GT的设计"></a>GT的设计</h3><p>上面的很多方法其实出发点都是bbox这个矩形框冗余信息太多，目标信息少，大部分是背景。它们大多都改变了GT的定义，如CornerNet将其定义为角点，ExtremeNet将其定义为极值点，FCOS虽然还是矩形框但也使用了center-ness进行抑制，FSAF则将GT定义为中心区域。对于GT目标的改进优化促使了目标检测的发展。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><strong>Densebox</strong>: Huang L, Yang Y, Deng Y, et al. Densebox: Unifying landmark localization with end to end object detection[J]. arXiv preprint arXiv:1509.04874, 2015.<br><br><strong>YOLO</strong>: Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[A]. Proceedings of the IEEE conference on computer vision and pattern recognition[C]. 2016: 779–788.<br><br><strong>FCOS</strong>: Tian Z, Shen C, Chen H, et al. FCOS: Fully Convolutional One-Stage Object Detection[J]. arXiv:1904.01355 [cs], 2019.<br><br><strong>FSAF</strong>: Zhu C, He Y, Savvides M. Feature Selective Anchor-Free Module for Single-Shot Object Detection[J]. arXiv:1903.00621 [cs], 2019.<br><br><strong>CornerNet</strong>: Law H, Deng J. CornerNet: Detecting Objects as Paired Keypoints[J]. arXiv:1808.01244 [cs], 2019.<br><br><strong>ExtremeNet</strong>: Zhou X, Zhuo J, Krähenbühl P. Bottom-up Object Detection by Grouping Extreme and Center Points[J]. arXiv:1901.08043 [cs], 2019.<br><br><strong>CenterNet</strong>: Zhou X, Wang D, Krähenbühl P. Objects as points[A]. arXiv preprint arXiv:1904.07850[C]. 2019.</p>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> anchor-free目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="2020/08/19/hello-world/"/>
      <url>2020/08/19/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
